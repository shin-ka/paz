{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction PAZ is a hierarchical perception library in Python. Selected examples: PAZ is used in the following examples (links to real-time demos and training scripts): Probabilistic 2D keypoints 6D head-pose estimation Object detection Emotion classifier 2D keypoint estimation Mask-RCNN (in-progress) 3D keypoint discovery Haar Cascade detector 6D pose estimation Implicit orientation Attention (STNs) All models can be re-trained with your own data (except for Mask-RCNN, we are working on it here ). Hierarchical APIs PAZ can be used with three diferent API levels which are there to be helpful for the user's specific application. High-level Easy out-of-the-box prediction. For example, for detecting objects we can call the following pipeline: from paz.pipelines import SSD512COCO detect = SSD512COCO () # apply directly to an image (numpy-array) inferences = detect ( image ) There are multiple high-level functions a.k.a. pipelines already implemented in PAZ here . Those functions are build using our mid-level API described now below. Mid-level While the high-level API is useful for quick applications, it might not be flexible enough for your specific purporse. Therefore, in PAZ we can build high-level functions using our a mid-level API. Mid-level: Sequential If your function is sequential you can construct a sequential function using SequentialProcessor . In the example below we create a data-augmentation pipeline: from paz.abstract import SequentialProcessor from paz import processors as pr augment = SequentialProcessor () augment . add ( pr . RandomContrast ()) augment . add ( pr . RandomBrightness ()) augment . add ( pr . RandomSaturation ()) augment . add ( pr . RandomHue ()) # you can now use this now as a normal function image = augment ( image ) You can also add any function not only those found in processors . For example we can pass a numpy function to our original data-augmentation pipeline: augment . add ( np . mean ) There are multiple functions a.k.a. Processors already implemented in PAZ here . Using these processors we can build more complex pipelines e.g. data augmentation for object detection : pr.AugmentDetection Mid-level: Explicit Non-sequential pipelines can be also build by abstracting Processor . In the example below we build a emotion classifier from scratch using our high-level and mid-level functions. from paz.applications import HaarCascadeFrontalFace , MiniXceptionFER import paz.processors as pr class EmotionDetector ( pr . Processor ): def __init__ ( self ): super ( EmotionDetector , self ) . __init__ () self . detect = HaarCascadeFrontalFace ( draw = False ) self . crop = pr . CropBoxes2D () self . classify = MiniXceptionFER () self . draw = pr . DrawBoxes2D ( self . classify . class_names ) def call ( self , image ): boxes2D = self . detect ( image )[ 'boxes2D' ] cropped_images = self . crop ( image , boxes2D ) for cropped_image , box2D in zip ( cropped_images , boxes2D ): box2D . class_name = self . classify ( cropped_image )[ 'class_name' ] return self . draw ( image , boxes2D ) detect = EmotionDetector () # you can now apply it to an image (numpy array) predictions = detect ( image ) Processors allow us to easily compose, compress and extract away parameters of functions. However, most processors are build using our low-level API (backend) shown next. Low-level Mid-level processors are mostly built from small backend functions found in: boxes , cameras , images , keypoints and quaternions . These functions can found in paz.backend : from paz.backend import boxes , camera , image , keypoints , quaternion For example, you can use them in your scripts to load or show images: from paz.backend.image import load_image , show_image image = load_image ( 'my_image.png' ) show_image ( image ) Additional functionality PAZ has built-in messages e.g. Pose6D for an easier data exchange with other libraries or frameworks such as ROS . There are custom callbacks e.g. MAP evaluation for object detectors while training PAZ comes with data loaders for the multiple datasets: OpenImages , VOC , YCB-Video , FAT , FERPlus , FER2013 . We have an automatic batch creation and dispatching wrappers for an easy connection between you pipelines and tensorflow generators. Please look at the examples and the processor pr.SequenceWrapper for more information.","title":"Getting-started"},{"location":"#introduction","text":"PAZ is a hierarchical perception library in Python.","title":"Introduction"},{"location":"#selected-examples","text":"PAZ is used in the following examples (links to real-time demos and training scripts): Probabilistic 2D keypoints 6D head-pose estimation Object detection Emotion classifier 2D keypoint estimation Mask-RCNN (in-progress) 3D keypoint discovery Haar Cascade detector 6D pose estimation Implicit orientation Attention (STNs) All models can be re-trained with your own data (except for Mask-RCNN, we are working on it here ).","title":"Selected examples:"},{"location":"#hierarchical-apis","text":"PAZ can be used with three diferent API levels which are there to be helpful for the user's specific application.","title":"Hierarchical APIs"},{"location":"#high-level","text":"Easy out-of-the-box prediction. For example, for detecting objects we can call the following pipeline: from paz.pipelines import SSD512COCO detect = SSD512COCO () # apply directly to an image (numpy-array) inferences = detect ( image ) There are multiple high-level functions a.k.a. pipelines already implemented in PAZ here . Those functions are build using our mid-level API described now below.","title":"High-level"},{"location":"#mid-level","text":"While the high-level API is useful for quick applications, it might not be flexible enough for your specific purporse. Therefore, in PAZ we can build high-level functions using our a mid-level API.","title":"Mid-level"},{"location":"#mid-level-sequential","text":"If your function is sequential you can construct a sequential function using SequentialProcessor . In the example below we create a data-augmentation pipeline: from paz.abstract import SequentialProcessor from paz import processors as pr augment = SequentialProcessor () augment . add ( pr . RandomContrast ()) augment . add ( pr . RandomBrightness ()) augment . add ( pr . RandomSaturation ()) augment . add ( pr . RandomHue ()) # you can now use this now as a normal function image = augment ( image ) You can also add any function not only those found in processors . For example we can pass a numpy function to our original data-augmentation pipeline: augment . add ( np . mean ) There are multiple functions a.k.a. Processors already implemented in PAZ here . Using these processors we can build more complex pipelines e.g. data augmentation for object detection : pr.AugmentDetection","title":"Mid-level: Sequential"},{"location":"#mid-level-explicit","text":"Non-sequential pipelines can be also build by abstracting Processor . In the example below we build a emotion classifier from scratch using our high-level and mid-level functions. from paz.applications import HaarCascadeFrontalFace , MiniXceptionFER import paz.processors as pr class EmotionDetector ( pr . Processor ): def __init__ ( self ): super ( EmotionDetector , self ) . __init__ () self . detect = HaarCascadeFrontalFace ( draw = False ) self . crop = pr . CropBoxes2D () self . classify = MiniXceptionFER () self . draw = pr . DrawBoxes2D ( self . classify . class_names ) def call ( self , image ): boxes2D = self . detect ( image )[ 'boxes2D' ] cropped_images = self . crop ( image , boxes2D ) for cropped_image , box2D in zip ( cropped_images , boxes2D ): box2D . class_name = self . classify ( cropped_image )[ 'class_name' ] return self . draw ( image , boxes2D ) detect = EmotionDetector () # you can now apply it to an image (numpy array) predictions = detect ( image ) Processors allow us to easily compose, compress and extract away parameters of functions. However, most processors are build using our low-level API (backend) shown next.","title":"Mid-level: Explicit"},{"location":"#low-level","text":"Mid-level processors are mostly built from small backend functions found in: boxes , cameras , images , keypoints and quaternions . These functions can found in paz.backend : from paz.backend import boxes , camera , image , keypoints , quaternion For example, you can use them in your scripts to load or show images: from paz.backend.image import load_image , show_image image = load_image ( 'my_image.png' ) show_image ( image )","title":"Low-level"},{"location":"#additional-functionality","text":"PAZ has built-in messages e.g. Pose6D for an easier data exchange with other libraries or frameworks such as ROS . There are custom callbacks e.g. MAP evaluation for object detectors while training PAZ comes with data loaders for the multiple datasets: OpenImages , VOC , YCB-Video , FAT , FERPlus , FER2013 . We have an automatic batch creation and dispatching wrappers for an easy connection between you pipelines and tensorflow generators. Please look at the examples and the processor pr.SequenceWrapper for more information.","title":"Additional functionality"},{"location":"contributing/","text":"Pull requests Always use full english names for variable names Only the following exceptions are allowed: \"number\" should be \"num\" \"argument\" should be \"arg\" \"width\" can be \"W\" if the context is clear. \"height\" can be \"H\" if the context is clear. Functions should be small, approximately ~6 lines: Functions should only do one thing. Certain aspects of the code base don't reflect this but we are working on changing this. Use PEP8 syntax conventions: This can be easily achieved when you install a linter e.g. flake8 If new functionality is added please include unit-tests for it. Please make sure that all unit-tests are passing before your make your PR. Commits should try to have the following structure: Commits are titles: Start with a capital letter Don't end the commit with a period Commits should be written to answer: If applied, this commit will A good commit would then look like: \"Remove deprecated backend function\" Find more information about how to write good commits here . Provide documentation of new features: Use the documentation syntax of the repository If new functionality is added please add your function to paz/docs/structure.py After looking into the points here discussed, please submit your PR such that we can start a discussion about it and check that all tests are passing.","title":"Contributing"},{"location":"contributing/#pull-requests","text":"Always use full english names for variable names Only the following exceptions are allowed: \"number\" should be \"num\" \"argument\" should be \"arg\" \"width\" can be \"W\" if the context is clear. \"height\" can be \"H\" if the context is clear. Functions should be small, approximately ~6 lines: Functions should only do one thing. Certain aspects of the code base don't reflect this but we are working on changing this. Use PEP8 syntax conventions: This can be easily achieved when you install a linter e.g. flake8 If new functionality is added please include unit-tests for it. Please make sure that all unit-tests are passing before your make your PR. Commits should try to have the following structure: Commits are titles: Start with a capital letter Don't end the commit with a period Commits should be written to answer: If applied, this commit will A good commit would then look like: \"Remove deprecated backend function\" Find more information about how to write good commits here . Provide documentation of new features: Use the documentation syntax of the repository If new functionality is added please add your function to paz/docs/structure.py After looking into the points here discussed, please submit your PR such that we can start a discussion about it and check that all tests are passing.","title":"Pull requests"},{"location":"datasets/","text":"[source] VOC paz . datasets . voc . VOC ( path = None , split = 'train' , class_names = 'all' , name = 'VOC2007' , with_difficult_samples = True , evaluate = False ) Dataset loader for the falling things dataset (FAT). Arguments data_path : Data path to VOC2007 annotations split : String determining the data split to load. e.g. train , val or test class_names : all or list. If list it should contain as elements strings indicating each class name. name : String or list indicating with dataset or datasets to load. e.g. VOC2007 or [''VOC2007'', VOC2012] . with_difficult_samples : Boolean. If True flagged difficult boxes will be added to the returned data. evaluate : Boolean. If True returned data will be loaded without normalization for a direct evaluation. Return data : List of dictionaries with keys corresponding to the image paths and values numpy arrays of shape [num_objects, 4 + 1] where the + 1 contains the class_arg and num_objects refers to the amount of boxes in the image. [source] FAT paz . datasets . fat . FAT ( path , split = 'train' , class_names = 'all' ) Dataset loader for the falling things dataset (FAT). Arguments path : String indicating full path to dataset e.g. /home/user/fat/ split : String determining the data split to load. e.g. train , val or test class_names : all or list. If list it should contain as elements strings indicating each class name. References Deep Object Pose Estimation (DOPE) [source] FER paz . datasets . fer . FER ( path , split = 'train' , class_names = 'all' , image_size = ( 48 , 48 )) Class for loading FER2013 emotion classification dataset. Arguments path : String. Full path to fer2013.csv file. split : String. Valid option contain 'train', 'val' or 'test'. class_names : String or list: If 'all' then it loads all default class names. image_size : List of length two. Indicates the shape in which the image will be resized. References - FER2013 Dataset and Challenge [source] FERPlus paz . datasets . ferplus . FERPlus ( path , split = 'train' , class_names = 'all' , image_size = ( 48 , 48 )) Class for loading FER2013 emotion classification dataset. with FERPlus labels. Arguments path : String. Path to directory that has inside the files: fer2013.csv and fer2013new.csv split : String. Valid option contain 'train', 'val' or 'test'. class_names : String or list: If 'all' then it loads all default class names. image_size : List of length two. Indicates the shape in which the image will be resized. References FerPlus FER2013 [source] OpenImages paz . datasets . open_images . OpenImages ( path , split = 'train' , class_names = 'all' ) Dataset loader for the OpenImagesV4 dataset. Arguments path : String indicating full path to dataset e.g. /home/user/open_images/ split : String determining the data split to load. e.g. train , val or test class_names : all or list. If list it should contain as elements the strings of the class names. [source] CityScapes paz . datasets . cityscapes . CityScapes ( image_path , label_path , split , class_names = 'all' ) CityScapes data manager for loading the paths of the RGB and segmentation masks. Arguments image_path : String. Path to RGB images e.g. '/home/user/leftImg8bit/' label_path : String. Path to label masks e.g. '/home/user/gtFine/' split : String. Valid option contain 'train', 'val' or 'test'. class_names : String or list: If 'all' then it loads all default class names. References - The Cityscapes Dataset for Semantic Urban Scene Understanding","title":"Datasets"},{"location":"datasets/#voc","text":"paz . datasets . voc . VOC ( path = None , split = 'train' , class_names = 'all' , name = 'VOC2007' , with_difficult_samples = True , evaluate = False ) Dataset loader for the falling things dataset (FAT). Arguments data_path : Data path to VOC2007 annotations split : String determining the data split to load. e.g. train , val or test class_names : all or list. If list it should contain as elements strings indicating each class name. name : String or list indicating with dataset or datasets to load. e.g. VOC2007 or [''VOC2007'', VOC2012] . with_difficult_samples : Boolean. If True flagged difficult boxes will be added to the returned data. evaluate : Boolean. If True returned data will be loaded without normalization for a direct evaluation. Return data : List of dictionaries with keys corresponding to the image paths and values numpy arrays of shape [num_objects, 4 + 1] where the + 1 contains the class_arg and num_objects refers to the amount of boxes in the image. [source]","title":"VOC"},{"location":"datasets/#fat","text":"paz . datasets . fat . FAT ( path , split = 'train' , class_names = 'all' ) Dataset loader for the falling things dataset (FAT). Arguments path : String indicating full path to dataset e.g. /home/user/fat/ split : String determining the data split to load. e.g. train , val or test class_names : all or list. If list it should contain as elements strings indicating each class name. References Deep Object Pose Estimation (DOPE) [source]","title":"FAT"},{"location":"datasets/#fer","text":"paz . datasets . fer . FER ( path , split = 'train' , class_names = 'all' , image_size = ( 48 , 48 )) Class for loading FER2013 emotion classification dataset. Arguments path : String. Full path to fer2013.csv file. split : String. Valid option contain 'train', 'val' or 'test'. class_names : String or list: If 'all' then it loads all default class names. image_size : List of length two. Indicates the shape in which the image will be resized. References - FER2013 Dataset and Challenge [source]","title":"FER"},{"location":"datasets/#ferplus","text":"paz . datasets . ferplus . FERPlus ( path , split = 'train' , class_names = 'all' , image_size = ( 48 , 48 )) Class for loading FER2013 emotion classification dataset. with FERPlus labels. Arguments path : String. Path to directory that has inside the files: fer2013.csv and fer2013new.csv split : String. Valid option contain 'train', 'val' or 'test'. class_names : String or list: If 'all' then it loads all default class names. image_size : List of length two. Indicates the shape in which the image will be resized. References FerPlus FER2013 [source]","title":"FERPlus"},{"location":"datasets/#openimages","text":"paz . datasets . open_images . OpenImages ( path , split = 'train' , class_names = 'all' ) Dataset loader for the OpenImagesV4 dataset. Arguments path : String indicating full path to dataset e.g. /home/user/open_images/ split : String determining the data split to load. e.g. train , val or test class_names : all or list. If list it should contain as elements the strings of the class names. [source]","title":"OpenImages"},{"location":"datasets/#cityscapes","text":"paz . datasets . cityscapes . CityScapes ( image_path , label_path , split , class_names = 'all' ) CityScapes data manager for loading the paths of the RGB and segmentation masks. Arguments image_path : String. Path to RGB images e.g. '/home/user/leftImg8bit/' label_path : String. Path to label masks e.g. '/home/user/gtFine/' split : String. Valid option contain 'train', 'val' or 'test'. class_names : String or list: If 'all' then it loads all default class names. References - The Cityscapes Dataset for Semantic Urban Scene Understanding","title":"CityScapes"},{"location":"installation/","text":"Installation PAZ has only three dependencies: Tensorflow2.0 , OpenCV and NumPy . To install PAZ with pypi run: pip install pypaz --user","title":"Installation"},{"location":"installation/#installation","text":"PAZ has only three dependencies: Tensorflow2.0 , OpenCV and NumPy . To install PAZ with pypi run: pip install pypaz --user","title":"Installation"},{"location":"abstract/loader/","text":"[source] Loader class paz . abstract . loader . Loader ( path , split , class_names , name ) Abstract class for loading a dataset. Arguments path : String. Path to data. split : String. Dataset split e.g. traing, val, test. class_names : List of strings. Label names of the classes. name : String. Dataset name. Properties name : Str. path : Str. split : Str or Flag. class_names : List of strings. num_classes : Int. Methods load_data() Loader methods [source] load_data load_data () Abstract method for loading dataset. Returns dictionary containing absolute image paths as keys, and ground truth vectors as values.","title":"Loader"},{"location":"abstract/loader/#loader-class","text":"paz . abstract . loader . Loader ( path , split , class_names , name ) Abstract class for loading a dataset. Arguments path : String. Path to data. split : String. Dataset split e.g. traing, val, test. class_names : List of strings. Label names of the classes. name : String. Dataset name. Properties name : Str. path : Str. split : Str or Flag. class_names : List of strings. num_classes : Int. Methods load_data()","title":"Loader class"},{"location":"abstract/loader/#loader-methods","text":"[source]","title":"Loader methods"},{"location":"abstract/loader/#load_data","text":"load_data () Abstract method for loading dataset. Returns dictionary containing absolute image paths as keys, and ground truth vectors as values.","title":"load_data"},{"location":"abstract/messages/","text":"[source] Box2D class paz . abstract . messages . Box2D ( coordinates , score , class_name = None ) Bounding box 2D coordinates with class label and score. Properties coordinates : List of float/integers indicating the [x_min, y_min, x_max, y_max] coordinates. score : Float. Indicates the score of label associated to the box. class_name : String indicating the class label name of the object. Methods contains() Box2D methods [source] contains contains ( point ) Checks if point is inside bounding box. Arguments point : Numpy array of size 2. Returns Boolean. 'True' if 'point' is inside bounding box. 'False' otherwise. [source] Pose6D paz . abstract . messages . Pose6D ( quaternion , translation , class_name = None ) Pose estimation results with 6D coordinates. Properties quaternion : List of 4 floats indicating (w, x, y, z) components. translation : List of 3 floats indicating (x, y, z) translation components. class_name : String or None indicating the class label name of the object. Class Methods from_rotation_vector : Instantiates a Pose6D object using a rotation and a translation vector.","title":"Messages"},{"location":"abstract/messages/#box2d-class","text":"paz . abstract . messages . Box2D ( coordinates , score , class_name = None ) Bounding box 2D coordinates with class label and score. Properties coordinates : List of float/integers indicating the [x_min, y_min, x_max, y_max] coordinates. score : Float. Indicates the score of label associated to the box. class_name : String indicating the class label name of the object. Methods contains()","title":"Box2D class"},{"location":"abstract/messages/#box2d-methods","text":"[source]","title":"Box2D methods"},{"location":"abstract/messages/#contains","text":"contains ( point ) Checks if point is inside bounding box. Arguments point : Numpy array of size 2. Returns Boolean. 'True' if 'point' is inside bounding box. 'False' otherwise. [source]","title":"contains"},{"location":"abstract/messages/#pose6d","text":"paz . abstract . messages . Pose6D ( quaternion , translation , class_name = None ) Pose estimation results with 6D coordinates. Properties quaternion : List of 4 floats indicating (w, x, y, z) components. translation : List of 3 floats indicating (x, y, z) translation components. class_name : String or None indicating the class label name of the object. Class Methods from_rotation_vector : Instantiates a Pose6D object using a rotation and a translation vector.","title":"Pose6D"},{"location":"abstract/processor/","text":"[source] Processor class paz . abstract . processor . Processor ( name = None ) Abstract class for creating a processor unit. Arguments name : String indicating name of the processing unit. Methods call() Example class NormalizeImage ( Processor ): def __init__ ( self ): super ( NormalizeImage , self ) . __init__ () def call ( self , image ): return image / 255.0 Why this name? Originally PAZ was only meant for pre-processing pipelines that included data-augmentation, normalization, etc. However, I found out that we could use the same API for post-processing; therefore, I thought at the time that Processor would be adequate to describe the capacity of both pre-processing and post-processing. Names that I also thought could have worked were: Function , Functor but I didn't want to use those since I thought they could also cause confusion. Similarly, in Keras this abstraction is interpreted as a Layer but here I don't think that abstraction is adequate. A layer of computation maybe? So after having this thoughts swirling around I decided to go with Processor and try to be explicit about my mental jugglery hoping the name doesn't cause much mental overhead. Processor methods [source] call call ( X ) Custom user's logic should be implemented here. [source] SequentialProcessor class paz . abstract . processor . SequentialProcessor ( processors = None , name = None ) Abstract class for creating a sequential pipeline of processors. Arguments processors : List of instantiated child classes of Processor classes. name : String indicating name of the processing unit. Methods add() remove() pop() insert() get_processor() Example AugmentImage = SequentialProcessor () AugmentImage . add ( pr . RandomContrast ()) AugmentImage . add ( pr . RandomBrightness ()) augment_image = AugmentImage () transformed_image = augment_image ( image ) SequentialProcessor methods [source] add add ( processor ) Adds a process to the sequence of processes to be applied to input. Arguments processor : An instantiated child class of of Processor . [source] remove remove ( name ) Removes processor from sequence Arguments name : String indicating the process name [source] pop pop ( index =- 1 ) Pops processor in given index from sequence Arguments index : Int. [source] insert insert ( index , processor ) Inserts processor to self.processors queue at index Argument index : Int. processor : An instantiated child class of of Processor . [source] get_processor get_processor ( name ) Gets processor from sequencer Arguments name : String indicating the process name","title":"Processor"},{"location":"abstract/processor/#processor-class","text":"paz . abstract . processor . Processor ( name = None ) Abstract class for creating a processor unit. Arguments name : String indicating name of the processing unit. Methods call() Example class NormalizeImage ( Processor ): def __init__ ( self ): super ( NormalizeImage , self ) . __init__ () def call ( self , image ): return image / 255.0 Why this name? Originally PAZ was only meant for pre-processing pipelines that included data-augmentation, normalization, etc. However, I found out that we could use the same API for post-processing; therefore, I thought at the time that Processor would be adequate to describe the capacity of both pre-processing and post-processing. Names that I also thought could have worked were: Function , Functor but I didn't want to use those since I thought they could also cause confusion. Similarly, in Keras this abstraction is interpreted as a Layer but here I don't think that abstraction is adequate. A layer of computation maybe? So after having this thoughts swirling around I decided to go with Processor and try to be explicit about my mental jugglery hoping the name doesn't cause much mental overhead.","title":"Processor class"},{"location":"abstract/processor/#processor-methods","text":"[source]","title":"Processor methods"},{"location":"abstract/processor/#call","text":"call ( X ) Custom user's logic should be implemented here. [source]","title":"call"},{"location":"abstract/processor/#sequentialprocessor-class","text":"paz . abstract . processor . SequentialProcessor ( processors = None , name = None ) Abstract class for creating a sequential pipeline of processors. Arguments processors : List of instantiated child classes of Processor classes. name : String indicating name of the processing unit. Methods add() remove() pop() insert() get_processor() Example AugmentImage = SequentialProcessor () AugmentImage . add ( pr . RandomContrast ()) AugmentImage . add ( pr . RandomBrightness ()) augment_image = AugmentImage () transformed_image = augment_image ( image )","title":"SequentialProcessor class"},{"location":"abstract/processor/#sequentialprocessor-methods","text":"[source]","title":"SequentialProcessor methods"},{"location":"abstract/processor/#add","text":"add ( processor ) Adds a process to the sequence of processes to be applied to input. Arguments processor : An instantiated child class of of Processor . [source]","title":"add"},{"location":"abstract/processor/#remove","text":"remove ( name ) Removes processor from sequence Arguments name : String indicating the process name [source]","title":"remove"},{"location":"abstract/processor/#pop","text":"pop ( index =- 1 ) Pops processor in given index from sequence Arguments index : Int. [source]","title":"pop"},{"location":"abstract/processor/#insert","text":"insert ( index , processor ) Inserts processor to self.processors queue at index Argument index : Int. processor : An instantiated child class of of Processor . [source]","title":"insert"},{"location":"abstract/processor/#get_processor","text":"get_processor ( name ) Gets processor from sequencer Arguments name : String indicating the process name","title":"get_processor"},{"location":"abstract/sequence/","text":"[source] ProcessingSequence paz . abstract . sequence . ProcessingSequence ( processor , batch_size , data , as_list = False ) Sequence generator used for processing samples given in data . Arguments processor : Function, used for processing elements of data . batch_size : Int. data : List. Each element of the list is processed by processor . as_list : Bool, if True inputs and labels are dispatched as lists. If false inputs and labels are dispatched as dictionaries. [source] GeneratingSequence paz . abstract . sequence . GeneratingSequence ( processor , batch_size , num_steps , as_list = False ) Sequence generator used for generating samples. Arguments processor : Function used for generating and processing samples . batch_size : Int. num_steps : Int. Number of steps for each epoch. as_list : Bool, if True inputs and labels are dispatched as lists. If false inputs and labels are dispatched as dictionaries.","title":"Sequence"},{"location":"abstract/sequence/#processingsequence","text":"paz . abstract . sequence . ProcessingSequence ( processor , batch_size , data , as_list = False ) Sequence generator used for processing samples given in data . Arguments processor : Function, used for processing elements of data . batch_size : Int. data : List. Each element of the list is processed by processor . as_list : Bool, if True inputs and labels are dispatched as lists. If false inputs and labels are dispatched as dictionaries. [source]","title":"ProcessingSequence"},{"location":"abstract/sequence/#generatingsequence","text":"paz . abstract . sequence . GeneratingSequence ( processor , batch_size , num_steps , as_list = False ) Sequence generator used for generating samples. Arguments processor : Function used for generating and processing samples . batch_size : Int. num_steps : Int. Number of steps for each epoch. as_list : Bool, if True inputs and labels are dispatched as lists. If false inputs and labels are dispatched as dictionaries.","title":"GeneratingSequence"},{"location":"backend/boxes/","text":"Backend functionality for 2D bounding boxes [source] apply_non_max_suppression paz . backend . boxes . apply_non_max_suppression ( boxes , scores , iou_thresh = 0.45 , top_k = 200 ) Apply non maximum suppression. Arguments boxes : Numpy array, box coordinates of shape (num_boxes, 4) where each columns corresponds to x_min, y_min, x_max, y_max. scores : Numpy array, of scores given for each box in boxes . iou_thresh : float, intersection over union threshold for removing boxes. top_k : int, number of maximum objects per class. Returns selected_indices : Numpy array, selected indices of kept boxes. num_selected_boxes : int, number of selected boxes. [source] offset paz . backend . boxes . offset ( coordinates , offset_scales ) Apply offsets to box coordinates Arguments coordinates : List of floats containing coordinates in point form. offset_scales : List of floats having x and y scales respectively. Returns coordinates : List of floats containing coordinates in point form. i.e. [x_min, y_min, x_max, y_max]. [source] clip paz . backend . boxes . clip ( coordinates , image_shape ) Clip box to valid image coordinates Arguments coordinates : List of floats containing coordinates in point form i.e. [x_min, y_min, x_max, y_max]. image_shape : List of two integers indicating height and width of image respectively. Returns List of clipped coordinates. [source] compute_iou paz . backend . boxes . compute_iou ( box , boxes ) Calculates the intersection over union between 'box' and all 'boxes'. Both box and boxes are in corner coordinates. Arguments box : Numpy array with length at least of 4. boxes : Numpy array with shape (num_boxes, 4) . Returns Numpy array of shape (num_boxes, 1) . [source] compute_ious paz . backend . boxes . compute_ious ( boxes_A , boxes_B ) Calculates the intersection over union between boxes_A and boxes_B . For each box present in the rows of boxes_A it calculates the intersection over union with respect to all boxes in boxes_B . The variables boxes_A and boxes_B contain the corner coordinates of the left-top corner (x_min, y_min) and the right-bottom (x_max, y_max) corner. Arguments boxes_A : Numpy array with shape (num_boxes_A, 4) . boxes_B : Numpy array with shape (num_boxes_B, 4) . Returns Numpy array of shape (num_boxes_A, num_boxes_B) . [source] decode paz . backend . boxes . decode ( predictions , priors , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Decode default boxes into the ground truth boxes Arguments loc : Numpy array of shape (num_priors, 4) . priors : Numpy array of shape (num_priors, 4) . variances : List of two floats. Variances of prior boxes. Returns decoded boxes: Numpy array of shape (num_priors, 4) . [source] denormalize_box paz . backend . boxes . denormalize_box ( box , image_shape ) Scales corner box coordinates from normalized values to image dimensions. Arguments box : Numpy array containing corner box coordinates. image_shape : List of integers with (height, width). Returns returns : box corner coordinates in image dimensions [source] encode paz . backend . boxes . encode ( matched , priors , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Encode the variances from the priorbox layers into the ground truth boxes we have matched (based on jaccard overlap) with the prior boxes. Arguments matched : Numpy array of shape (num_priors, 4) with boxes in point-form. priors : Numpy array of shape (num_priors, 4) with boxes in center-form. variances : (list[float]) Variances of priorboxes Returns encoded boxes: Numpy array of shape (num_priors, 4) . [source] flip_left_right paz . backend . boxes . flip_left_right ( boxes , width ) Flips box coordinates from left-to-right and vice-versa. Arguments boxes : Numpy array of shape [num_boxes, 4] . Returns Numpy array of shape [num_boxes, 4] . [source] make_box_square paz . backend . boxes . make_box_square ( box ) Makes box coordinates square with sides equal to the longest original side. Arguments box : Numpy array with shape (4) with point corner coordinates. Returns returns : List of box coordinates ints. [source] match paz . backend . boxes . match ( boxes , prior_boxes , positive_iou = 0.5 , negative_iou = 0.0 ) Matches each prior box with a ground truth box (box from boxes ). It then selects which matched box will be considered positive e.g. iou > .5 and returns for each prior box a ground truth box that is either positive (with a class argument different than 0) or negative. Arguments boxes : Numpy array of shape (num_ground_truh_boxes, 4 + 1) , where the first the first four coordinates correspond to box coordinates and the last coordinates is the class argument. This boxes should be the ground truth boxes. prior_boxes : Numpy array of shape (num_prior_boxes, 4) . where the four coordinates are in center form coordinates. positive_iou : Float between [0, 1]. Intersection over union used to determine which box is considered a positive box. negative_iou : Float between [0, 1]. Intersection over union used to determine which box is considered a negative box. Returns numpy array of shape (num_prior_boxes, 4 + 1) . where the first the first four coordinates correspond to point form box coordinates and the last coordinates is the class argument. [source] nms_per_class paz . backend . boxes . nms_per_class ( box_data , nms_thresh = 0.45 , conf_thresh = 0.01 , top_k = 200 ) Applies non-maximum-suppression per class. Arguments box_data : Numpy array of shape (num_prior_boxes, 4 + num_classes) . nsm_thresh : Float. Non-maximum suppression threshold. conf_thresh : Float. Filter scores with a lower confidence value before performing non-maximum supression. top_k : Integer. Maximum number of boxes per class outputted by nms. Returns Numpy array of shape (num_classes, top_k, 5) . [source] to_image_coordinates paz . backend . boxes . to_image_coordinates ( boxes , image ) Transforms normalized box coordinates into image coordinates. Arguments image : Numpy array. boxes : Numpy array of shape [num_boxes, N] where N >= 4. Returns Numpy array of shape [num_boxes, N] . [source] to_center_form paz . backend . boxes . to_center_form ( boxes ) Transform from corner coordinates to center coordinates. Arguments boxes : Numpy array with shape (num_boxes, 4) . Returns Numpy array with shape (num_boxes, 4) . [source] to_one_hot paz . backend . boxes . to_one_hot ( class_indices , num_classes ) Transform from class index to one-hot encoded vector. Arguments class_indices : Numpy array. One dimensional array specifying the index argument of the class for each sample. num_classes : Integer. Total number of classes. Returns Numpy array with shape (num_samples, num_classes) . [source] to_normalized_coordinates paz . backend . boxes . to_normalized_coordinates ( boxes , image ) Transforms coordinates in image dimensions to normalized coordinates. Arguments image : Numpy array. boxes : Numpy array of shape [num_boxes, N] where N >= 4. Returns Numpy array of shape [num_boxes, N] . [source] to_corner_form paz . backend . boxes . to_corner_form ( boxes ) Transform from center coordinates to corner coordinates. Arguments boxes : Numpy array with shape (num_boxes, 4) . Returns Numpy array with shape (num_boxes, 4) . [source] extract_bounding_box_corners paz . backend . boxes . extract_bounding_box_corners ( points3D ) Extracts the (x_min, y_min, z_min) and the (x_max, y_max, z_max) coordinates from an array of points3D Arguments points3D : Array (num_points, 3) Returns Left-down-bottom corner (x_min, y_min, z_min) and right-up-top (x_max, y_max, z_max) corner.","title":"Boxes"},{"location":"backend/boxes/#apply_non_max_suppression","text":"paz . backend . boxes . apply_non_max_suppression ( boxes , scores , iou_thresh = 0.45 , top_k = 200 ) Apply non maximum suppression. Arguments boxes : Numpy array, box coordinates of shape (num_boxes, 4) where each columns corresponds to x_min, y_min, x_max, y_max. scores : Numpy array, of scores given for each box in boxes . iou_thresh : float, intersection over union threshold for removing boxes. top_k : int, number of maximum objects per class. Returns selected_indices : Numpy array, selected indices of kept boxes. num_selected_boxes : int, number of selected boxes. [source]","title":"apply_non_max_suppression"},{"location":"backend/boxes/#offset","text":"paz . backend . boxes . offset ( coordinates , offset_scales ) Apply offsets to box coordinates Arguments coordinates : List of floats containing coordinates in point form. offset_scales : List of floats having x and y scales respectively. Returns coordinates : List of floats containing coordinates in point form. i.e. [x_min, y_min, x_max, y_max]. [source]","title":"offset"},{"location":"backend/boxes/#clip","text":"paz . backend . boxes . clip ( coordinates , image_shape ) Clip box to valid image coordinates Arguments coordinates : List of floats containing coordinates in point form i.e. [x_min, y_min, x_max, y_max]. image_shape : List of two integers indicating height and width of image respectively. Returns List of clipped coordinates. [source]","title":"clip"},{"location":"backend/boxes/#compute_iou","text":"paz . backend . boxes . compute_iou ( box , boxes ) Calculates the intersection over union between 'box' and all 'boxes'. Both box and boxes are in corner coordinates. Arguments box : Numpy array with length at least of 4. boxes : Numpy array with shape (num_boxes, 4) . Returns Numpy array of shape (num_boxes, 1) . [source]","title":"compute_iou"},{"location":"backend/boxes/#compute_ious","text":"paz . backend . boxes . compute_ious ( boxes_A , boxes_B ) Calculates the intersection over union between boxes_A and boxes_B . For each box present in the rows of boxes_A it calculates the intersection over union with respect to all boxes in boxes_B . The variables boxes_A and boxes_B contain the corner coordinates of the left-top corner (x_min, y_min) and the right-bottom (x_max, y_max) corner. Arguments boxes_A : Numpy array with shape (num_boxes_A, 4) . boxes_B : Numpy array with shape (num_boxes_B, 4) . Returns Numpy array of shape (num_boxes_A, num_boxes_B) . [source]","title":"compute_ious"},{"location":"backend/boxes/#decode","text":"paz . backend . boxes . decode ( predictions , priors , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Decode default boxes into the ground truth boxes Arguments loc : Numpy array of shape (num_priors, 4) . priors : Numpy array of shape (num_priors, 4) . variances : List of two floats. Variances of prior boxes. Returns decoded boxes: Numpy array of shape (num_priors, 4) . [source]","title":"decode"},{"location":"backend/boxes/#denormalize_box","text":"paz . backend . boxes . denormalize_box ( box , image_shape ) Scales corner box coordinates from normalized values to image dimensions. Arguments box : Numpy array containing corner box coordinates. image_shape : List of integers with (height, width). Returns returns : box corner coordinates in image dimensions [source]","title":"denormalize_box"},{"location":"backend/boxes/#encode","text":"paz . backend . boxes . encode ( matched , priors , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Encode the variances from the priorbox layers into the ground truth boxes we have matched (based on jaccard overlap) with the prior boxes. Arguments matched : Numpy array of shape (num_priors, 4) with boxes in point-form. priors : Numpy array of shape (num_priors, 4) with boxes in center-form. variances : (list[float]) Variances of priorboxes Returns encoded boxes: Numpy array of shape (num_priors, 4) . [source]","title":"encode"},{"location":"backend/boxes/#flip_left_right","text":"paz . backend . boxes . flip_left_right ( boxes , width ) Flips box coordinates from left-to-right and vice-versa. Arguments boxes : Numpy array of shape [num_boxes, 4] . Returns Numpy array of shape [num_boxes, 4] . [source]","title":"flip_left_right"},{"location":"backend/boxes/#make_box_square","text":"paz . backend . boxes . make_box_square ( box ) Makes box coordinates square with sides equal to the longest original side. Arguments box : Numpy array with shape (4) with point corner coordinates. Returns returns : List of box coordinates ints. [source]","title":"make_box_square"},{"location":"backend/boxes/#match","text":"paz . backend . boxes . match ( boxes , prior_boxes , positive_iou = 0.5 , negative_iou = 0.0 ) Matches each prior box with a ground truth box (box from boxes ). It then selects which matched box will be considered positive e.g. iou > .5 and returns for each prior box a ground truth box that is either positive (with a class argument different than 0) or negative. Arguments boxes : Numpy array of shape (num_ground_truh_boxes, 4 + 1) , where the first the first four coordinates correspond to box coordinates and the last coordinates is the class argument. This boxes should be the ground truth boxes. prior_boxes : Numpy array of shape (num_prior_boxes, 4) . where the four coordinates are in center form coordinates. positive_iou : Float between [0, 1]. Intersection over union used to determine which box is considered a positive box. negative_iou : Float between [0, 1]. Intersection over union used to determine which box is considered a negative box. Returns numpy array of shape (num_prior_boxes, 4 + 1) . where the first the first four coordinates correspond to point form box coordinates and the last coordinates is the class argument. [source]","title":"match"},{"location":"backend/boxes/#nms_per_class","text":"paz . backend . boxes . nms_per_class ( box_data , nms_thresh = 0.45 , conf_thresh = 0.01 , top_k = 200 ) Applies non-maximum-suppression per class. Arguments box_data : Numpy array of shape (num_prior_boxes, 4 + num_classes) . nsm_thresh : Float. Non-maximum suppression threshold. conf_thresh : Float. Filter scores with a lower confidence value before performing non-maximum supression. top_k : Integer. Maximum number of boxes per class outputted by nms. Returns Numpy array of shape (num_classes, top_k, 5) . [source]","title":"nms_per_class"},{"location":"backend/boxes/#to_image_coordinates","text":"paz . backend . boxes . to_image_coordinates ( boxes , image ) Transforms normalized box coordinates into image coordinates. Arguments image : Numpy array. boxes : Numpy array of shape [num_boxes, N] where N >= 4. Returns Numpy array of shape [num_boxes, N] . [source]","title":"to_image_coordinates"},{"location":"backend/boxes/#to_center_form","text":"paz . backend . boxes . to_center_form ( boxes ) Transform from corner coordinates to center coordinates. Arguments boxes : Numpy array with shape (num_boxes, 4) . Returns Numpy array with shape (num_boxes, 4) . [source]","title":"to_center_form"},{"location":"backend/boxes/#to_one_hot","text":"paz . backend . boxes . to_one_hot ( class_indices , num_classes ) Transform from class index to one-hot encoded vector. Arguments class_indices : Numpy array. One dimensional array specifying the index argument of the class for each sample. num_classes : Integer. Total number of classes. Returns Numpy array with shape (num_samples, num_classes) . [source]","title":"to_one_hot"},{"location":"backend/boxes/#to_normalized_coordinates","text":"paz . backend . boxes . to_normalized_coordinates ( boxes , image ) Transforms coordinates in image dimensions to normalized coordinates. Arguments image : Numpy array. boxes : Numpy array of shape [num_boxes, N] where N >= 4. Returns Numpy array of shape [num_boxes, N] . [source]","title":"to_normalized_coordinates"},{"location":"backend/boxes/#to_corner_form","text":"paz . backend . boxes . to_corner_form ( boxes ) Transform from center coordinates to corner coordinates. Arguments boxes : Numpy array with shape (num_boxes, 4) . Returns Numpy array with shape (num_boxes, 4) . [source]","title":"to_corner_form"},{"location":"backend/boxes/#extract_bounding_box_corners","text":"paz . backend . boxes . extract_bounding_box_corners ( points3D ) Extracts the (x_min, y_min, z_min) and the (x_max, y_max, z_max) coordinates from an array of points3D Arguments points3D : Array (num_points, 3) Returns Left-down-bottom corner (x_min, y_min, z_min) and right-up-top (x_max, y_max, z_max) corner.","title":"extract_bounding_box_corners"},{"location":"backend/camera/","text":"Backend functionality for cameras [source] Camera class paz . backend . camera . Camera ( device_id = 0 , name = 'Camera' , intrinsics = None , distortion = None ) Camera abstract class. By default this camera uses the openCV functionality. It can be inherited to overwrite methods in case another camera API exists. Camera methods [source] is_open is_open () Checks if camera is open. Returns Boolean [source] start start () Starts capturing device Returns Camera object. [source] stop stop () Stops capturing device. [source] VideoPlayer class paz . backend . camera . VideoPlayer ( image_size , pipeline , camera , topic = 'image' ) Performs visualization inferences in a real-time video. Properties image_size : List of two integers. Output size of the displayed image. pipeline : Function. Should take RGB image as input and it should output a dictionary with key 'image' containing a visualization of the inferences. Built-in pipelines can be found in paz/processing/pipelines . Methods run() record() VideoPlayer methods [source] step step () Runs the pipeline process once Returns Inferences from pipeline . [source] run run () Opens camera and starts continuous inference using pipeline , until the user presses q inside the opened window. [source] record record ( name = 'video.avi' , fps = 20 , fourCC = 'XVID' ) Opens camera and records continuous inference using pipeline . Arguments name : String. Video name. Must include the postfix .avi. fps : Int. Frames per second. fourCC : String. Indicates the four character code of the video. e.g. XVID, MJPG, X264.","title":"Camera"},{"location":"backend/camera/#camera-class","text":"paz . backend . camera . Camera ( device_id = 0 , name = 'Camera' , intrinsics = None , distortion = None ) Camera abstract class. By default this camera uses the openCV functionality. It can be inherited to overwrite methods in case another camera API exists.","title":"Camera class"},{"location":"backend/camera/#camera-methods","text":"[source]","title":"Camera methods"},{"location":"backend/camera/#is_open","text":"is_open () Checks if camera is open. Returns Boolean [source]","title":"is_open"},{"location":"backend/camera/#start","text":"start () Starts capturing device Returns Camera object. [source]","title":"start"},{"location":"backend/camera/#stop","text":"stop () Stops capturing device. [source]","title":"stop"},{"location":"backend/camera/#videoplayer-class","text":"paz . backend . camera . VideoPlayer ( image_size , pipeline , camera , topic = 'image' ) Performs visualization inferences in a real-time video. Properties image_size : List of two integers. Output size of the displayed image. pipeline : Function. Should take RGB image as input and it should output a dictionary with key 'image' containing a visualization of the inferences. Built-in pipelines can be found in paz/processing/pipelines . Methods run() record()","title":"VideoPlayer class"},{"location":"backend/camera/#videoplayer-methods","text":"[source]","title":"VideoPlayer methods"},{"location":"backend/camera/#step","text":"step () Runs the pipeline process once Returns Inferences from pipeline . [source]","title":"step"},{"location":"backend/camera/#run","text":"run () Opens camera and starts continuous inference using pipeline , until the user presses q inside the opened window. [source]","title":"run"},{"location":"backend/camera/#record","text":"record ( name = 'video.avi' , fps = 20 , fourCC = 'XVID' ) Opens camera and records continuous inference using pipeline . Arguments name : String. Video name. Must include the postfix .avi. fps : Int. Frames per second. fourCC : String. Indicates the four character code of the video. e.g. XVID, MJPG, X264.","title":"record"},{"location":"backend/draw/","text":"[source] draw_circle paz . backend . image . draw . draw_circle ( image , point , color = ( 0 , 255 , 0 ), radius = 5 ) Draws a circle in image. Arguments image : Numpy array of shape [H, W, 3] . point : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. radius : Integer indicating the radius of the point to be drawn. Returns Numpy array with shape [H, W, 3] . Image with circle. [source] draw_cube paz . backend . image . draw . draw_cube ( image , points , color = ( 0 , 255 , 0 ), thickness = 2 , radius = 5 ) Draws a cube in image. Arguments image : Numpy array of shape (H, W, 3). points : List of length 8 having each element a list of length two indicating (U, V) openCV coordinates. color : List of length three indicating RGB color of point. thickness : Integer indicating the thickness of the line to be drawn. radius : Integer indicating the radius of corner points to be drawn. Returns Numpy array with shape (H, W, 3). Image with cube. [source] draw_dot paz . backend . image . draw . draw_dot ( image , point , color = ( 0 , 255 , 0 ), radius = 5 , filled =- 1 ) Draws a dot (small rectangle) in image. Arguments image : Numpy array of shape [H, W, 3] . point : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. radius : Integer indicating the radius of the point to be drawn. filled : Boolean. If True rectangle is filled with color . Returns Numpy array with shape [H, W, 3] . Image with dot. [source] draw_filled_polygon paz . backend . image . draw . draw_filled_polygon ( image , vertices , color ) Draws filled polygon Arguments image : Numpy array. vertices : List of elements each having a list of length two indicating (y, x) openCV coordinates. color : Numpy array specifying RGB color of the polygon. Returns Numpy array with shape [H, W, 3] . Image with polygon. [source] draw_line paz . backend . image . draw . draw_line ( image , point_A , point_B , color = ( 0 , 255 , 0 ), thickness = 5 ) Draws a line in image from point_A to point_B . Arguments image : Numpy array of shape [H, W, 3] . point_A : List of length two indicating (y, x) openCV coordinates. point_B : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. thickness : Integer indicating the thickness of the line to be drawn. Returns Numpy array with shape [H, W, 3] . Image with line. [source] draw_random_polygon paz . backend . image . draw . draw_random_polygon ( image , max_radius_scale = 0.5 ) Draw random polygon image. Arguments image : Numpy array with shape [H, W, 3] . max_radius_scale : Float between [0, 1]. Returns Numpy array with shape [H, W, 3] . Image with polygon. [source] draw_rectangle paz . backend . image . draw . draw_rectangle ( image , corner_A , corner_B , color , thickness ) Draws a filled rectangle from corner_A to corner_B . Arguments image : Numpy array of shape [H, W, 3] . corner_A : List of length two indicating (y, x) openCV coordinates. corner_B : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. thickness : Integer/openCV Flag. Thickness of rectangle line. or for filled use cv2.FILLED flag. Returns Numpy array with shape [H, W, 3] . Image with rectangle. [source] lincolor paz . backend . image . draw . lincolor ( num_colors , saturation = 1 , value = 1 , normalized = False ) Creates a list of RGB colors linearly sampled from HSV space with randomised Saturation and Value. Arguments num_colors : Int. saturation : Float or None . If float indicates saturation. If None it samples a random value. value : Float or None . If float indicates value. If None it samples a random value. normalized : Bool. If True, RGB colors are returned between [0, 1] if False, RGB colors are between [0, 255]. Returns List, for which each element contains a list with RGB color [source] put_text paz . backend . image . draw . put_text ( image , text , point , scale , color , thickness ) Draws text in image. Arguments image : Numpy array. text : String. Text to be drawn. point : Tuple of coordinates indicating the top corner of the text. scale : Float. Scale of text. color : Tuple of integers. RGB color coordinates. thickness : Integer. Thickness of the lines used for drawing text. Returns Numpy array with shape [H, W, 3] . Image with text. [source] make_mosaic paz . backend . image . draw . make_mosaic ( images , shape , border = 0 ) Creates an image mosaic. Arguments images : Numpy array of shape (num_images, height, width, num_channels) shape : List of two integers indicating the mosaic shape. Shape must satisfy: shape[0] * shape[1] == len(images). border : Integer indicating the border per image. Returns A numpy array containing all images. [source] draw_points2D paz . backend . image . draw . draw_points2D ( image , points2D , colors ) Draws a pixel for all points2D in UV space using only numpy. Arguments image : Array (H, W). keypoints : Array (num_points, U, V). Keypoints in image space colors : Array (num_points, 3). Colors in RGB space. Returns Array with drawn points. [source] draw_keypoints_link paz . backend . image . draw . draw_keypoints_link ( image , keypoints , link_args , link_orders , link_colors , check_scores = False , link_width = 2 ) Draw link between the keypoints. Arguments images : Numpy array. keypoints : Keypoint(k0, k1, ...) locations in the image. Numpy array. link_args : Keypoint labels. Dictionary. {'k0':0, 'k1':1, ...} link_orders : List of tuple. [('k0', 'k1'),('kl', 'k2'), ...] link_colors : Color of each link. List of list check_scores : Condition to draw links. Boolean. Returns A numpy array containing drawn link between the keypoints. [source] draw_keypoints paz . backend . image . draw . draw_keypoints ( image , keypoints , keypoint_colors , check_scores = False , keypoint_radius = 6 ) Draw a circle at keypoints. Arguments images : Numpy array. keypoints : Keypoint locations in the image. Numpy array. keypoint_colors : Color of each keypoint. List of list check_scores : Condition to draw keypoint. Boolean. Returns A numpy array containing circle at each keypoints.","title":"Draw"},{"location":"backend/draw/#draw_circle","text":"paz . backend . image . draw . draw_circle ( image , point , color = ( 0 , 255 , 0 ), radius = 5 ) Draws a circle in image. Arguments image : Numpy array of shape [H, W, 3] . point : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. radius : Integer indicating the radius of the point to be drawn. Returns Numpy array with shape [H, W, 3] . Image with circle. [source]","title":"draw_circle"},{"location":"backend/draw/#draw_cube","text":"paz . backend . image . draw . draw_cube ( image , points , color = ( 0 , 255 , 0 ), thickness = 2 , radius = 5 ) Draws a cube in image. Arguments image : Numpy array of shape (H, W, 3). points : List of length 8 having each element a list of length two indicating (U, V) openCV coordinates. color : List of length three indicating RGB color of point. thickness : Integer indicating the thickness of the line to be drawn. radius : Integer indicating the radius of corner points to be drawn. Returns Numpy array with shape (H, W, 3). Image with cube. [source]","title":"draw_cube"},{"location":"backend/draw/#draw_dot","text":"paz . backend . image . draw . draw_dot ( image , point , color = ( 0 , 255 , 0 ), radius = 5 , filled =- 1 ) Draws a dot (small rectangle) in image. Arguments image : Numpy array of shape [H, W, 3] . point : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. radius : Integer indicating the radius of the point to be drawn. filled : Boolean. If True rectangle is filled with color . Returns Numpy array with shape [H, W, 3] . Image with dot. [source]","title":"draw_dot"},{"location":"backend/draw/#draw_filled_polygon","text":"paz . backend . image . draw . draw_filled_polygon ( image , vertices , color ) Draws filled polygon Arguments image : Numpy array. vertices : List of elements each having a list of length two indicating (y, x) openCV coordinates. color : Numpy array specifying RGB color of the polygon. Returns Numpy array with shape [H, W, 3] . Image with polygon. [source]","title":"draw_filled_polygon"},{"location":"backend/draw/#draw_line","text":"paz . backend . image . draw . draw_line ( image , point_A , point_B , color = ( 0 , 255 , 0 ), thickness = 5 ) Draws a line in image from point_A to point_B . Arguments image : Numpy array of shape [H, W, 3] . point_A : List of length two indicating (y, x) openCV coordinates. point_B : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. thickness : Integer indicating the thickness of the line to be drawn. Returns Numpy array with shape [H, W, 3] . Image with line. [source]","title":"draw_line"},{"location":"backend/draw/#draw_random_polygon","text":"paz . backend . image . draw . draw_random_polygon ( image , max_radius_scale = 0.5 ) Draw random polygon image. Arguments image : Numpy array with shape [H, W, 3] . max_radius_scale : Float between [0, 1]. Returns Numpy array with shape [H, W, 3] . Image with polygon. [source]","title":"draw_random_polygon"},{"location":"backend/draw/#draw_rectangle","text":"paz . backend . image . draw . draw_rectangle ( image , corner_A , corner_B , color , thickness ) Draws a filled rectangle from corner_A to corner_B . Arguments image : Numpy array of shape [H, W, 3] . corner_A : List of length two indicating (y, x) openCV coordinates. corner_B : List of length two indicating (y, x) openCV coordinates. color : List of length three indicating RGB color of point. thickness : Integer/openCV Flag. Thickness of rectangle line. or for filled use cv2.FILLED flag. Returns Numpy array with shape [H, W, 3] . Image with rectangle. [source]","title":"draw_rectangle"},{"location":"backend/draw/#lincolor","text":"paz . backend . image . draw . lincolor ( num_colors , saturation = 1 , value = 1 , normalized = False ) Creates a list of RGB colors linearly sampled from HSV space with randomised Saturation and Value. Arguments num_colors : Int. saturation : Float or None . If float indicates saturation. If None it samples a random value. value : Float or None . If float indicates value. If None it samples a random value. normalized : Bool. If True, RGB colors are returned between [0, 1] if False, RGB colors are between [0, 255]. Returns List, for which each element contains a list with RGB color [source]","title":"lincolor"},{"location":"backend/draw/#put_text","text":"paz . backend . image . draw . put_text ( image , text , point , scale , color , thickness ) Draws text in image. Arguments image : Numpy array. text : String. Text to be drawn. point : Tuple of coordinates indicating the top corner of the text. scale : Float. Scale of text. color : Tuple of integers. RGB color coordinates. thickness : Integer. Thickness of the lines used for drawing text. Returns Numpy array with shape [H, W, 3] . Image with text. [source]","title":"put_text"},{"location":"backend/draw/#make_mosaic","text":"paz . backend . image . draw . make_mosaic ( images , shape , border = 0 ) Creates an image mosaic. Arguments images : Numpy array of shape (num_images, height, width, num_channels) shape : List of two integers indicating the mosaic shape. Shape must satisfy: shape[0] * shape[1] == len(images). border : Integer indicating the border per image. Returns A numpy array containing all images. [source]","title":"make_mosaic"},{"location":"backend/draw/#draw_points2d","text":"paz . backend . image . draw . draw_points2D ( image , points2D , colors ) Draws a pixel for all points2D in UV space using only numpy. Arguments image : Array (H, W). keypoints : Array (num_points, U, V). Keypoints in image space colors : Array (num_points, 3). Colors in RGB space. Returns Array with drawn points. [source]","title":"draw_points2D"},{"location":"backend/draw/#draw_keypoints_link","text":"paz . backend . image . draw . draw_keypoints_link ( image , keypoints , link_args , link_orders , link_colors , check_scores = False , link_width = 2 ) Draw link between the keypoints. Arguments images : Numpy array. keypoints : Keypoint(k0, k1, ...) locations in the image. Numpy array. link_args : Keypoint labels. Dictionary. {'k0':0, 'k1':1, ...} link_orders : List of tuple. [('k0', 'k1'),('kl', 'k2'), ...] link_colors : Color of each link. List of list check_scores : Condition to draw links. Boolean. Returns A numpy array containing drawn link between the keypoints. [source]","title":"draw_keypoints_link"},{"location":"backend/draw/#draw_keypoints","text":"paz . backend . image . draw . draw_keypoints ( image , keypoints , keypoint_colors , check_scores = False , keypoint_radius = 6 ) Draw a circle at keypoints. Arguments images : Numpy array. keypoints : Keypoint locations in the image. Numpy array. keypoint_colors : Color of each keypoint. List of list check_scores : Condition to draw keypoint. Boolean. Returns A numpy array containing circle at each keypoints.","title":"draw_keypoints"},{"location":"backend/groups/","text":"[source] rotation_vector_to_quaternion paz . backend . groups . quaternion . rotation_vector_to_quaternion ( rotation_vector ) Transforms rotation vector into quaternion. Arguments rotation_vector : Numpy array of shape [3] . Returns Numpy array representing a quaternion having a shape [4] . [source] homogenous_quaternion_to_rotation_matrix paz . backend . groups . quaternion . homogenous_quaternion_to_rotation_matrix ( quaternion ) Transforms quaternion to rotation matrix. Arguments quaternion : Array containing quaternion value [q1, q2, q3, w0]. Returns Rotation matrix [3, 3]. Note If quaternion is not a unit quaternion the rotation matrix is not unitary but still orthogonal i.e. the outputted rotation matrix is a scalar multiple of a rotation matrix. [source] quaternion_to_rotation_matrix paz . backend . groups . quaternion . quaternion_to_rotation_matrix ( quaternion ) Transforms quaternion to rotation matrix. Arguments quaternion : Array containing quaternion value [q1, q2, q3, w0]. Returns Rotation matrix [3, 3]. Note \"If the quaternion \"is not a unit quaternion then the homogeneous form is still a scalar multiple of a rotation matrix, while the inhomogeneous form is in general no longer an orthogonal matrix. This is why in numerical work the homogeneous form is to be preferred if distortion is to be avoided.\" wikipedia [source] to_affine_matrix paz . backend . groups . SE3 . to_affine_matrix ( rotation_matrix , translation ) Builds affine matrix from rotation matrix and translation vector. Arguments rotation_matrix : Array (3, 3). Representing a rotation matrix. translation : Array (3). Translation vector. Returns Array (4, 4) representing an affine matrix. [source] rotation_vector_to_rotation_matrix paz . backend . groups . SO3 . rotation_vector_to_rotation_matrix ( rotation_vector ) Transforms rotation vector (axis-angle) form to rotation matrix. Arguments rotation_vector : Array (3). Rotation vector in axis-angle form. Returns Array (3, 3) rotation matrix. [source] build_rotation_matrix_x paz . backend . groups . SO3 . build_rotation_matrix_x ( angle ) Builds rotation matrix in X axis. Arguments angle : Float. Angle in radians. Return Array (3, 3) rotation matrix in Z axis. [source] build_rotation_matrix_y paz . backend . groups . SO3 . build_rotation_matrix_y ( angle ) Builds rotation matrix in Y axis. Arguments angle : Float. Angle in radians. Return Array (3, 3) rotation matrix in Z axis. [source] build_rotation_matrix_z paz . backend . groups . SO3 . build_rotation_matrix_z ( angle ) Builds rotation matrix in Z axis. Arguments angle : Float. Angle in radians. Return Array (3, 3) rotation matrix in Z axis. [source] compute_norm_SO3 paz . backend . groups . SO3 . compute_norm_SO3 ( rotation_mesh , rotation ) Computes norm between SO3 elements. Arguments rotation_mesh : Array (3, 3), rotation matrix. rotation : Array (3, 3), rotation matrix. Returns Scalar representing the distance between both rotation matrices. [source] calculate_canonical_rotation paz . backend . groups . SO3 . calculate_canonical_rotation ( rotation_mesh , rotations ) Returns the rotation matrix closest to rotation mesh. Arguments rotation_mesh : Array (3, 3), rotation matrix. rotations : List of array of (3, 3), rotation matrices. Returns Element of list closest to rotation mesh.","title":"Groups"},{"location":"backend/groups/#rotation_vector_to_quaternion","text":"paz . backend . groups . quaternion . rotation_vector_to_quaternion ( rotation_vector ) Transforms rotation vector into quaternion. Arguments rotation_vector : Numpy array of shape [3] . Returns Numpy array representing a quaternion having a shape [4] . [source]","title":"rotation_vector_to_quaternion"},{"location":"backend/groups/#homogenous_quaternion_to_rotation_matrix","text":"paz . backend . groups . quaternion . homogenous_quaternion_to_rotation_matrix ( quaternion ) Transforms quaternion to rotation matrix. Arguments quaternion : Array containing quaternion value [q1, q2, q3, w0]. Returns Rotation matrix [3, 3]. Note If quaternion is not a unit quaternion the rotation matrix is not unitary but still orthogonal i.e. the outputted rotation matrix is a scalar multiple of a rotation matrix. [source]","title":"homogenous_quaternion_to_rotation_matrix"},{"location":"backend/groups/#quaternion_to_rotation_matrix","text":"paz . backend . groups . quaternion . quaternion_to_rotation_matrix ( quaternion ) Transforms quaternion to rotation matrix. Arguments quaternion : Array containing quaternion value [q1, q2, q3, w0]. Returns Rotation matrix [3, 3]. Note \"If the quaternion \"is not a unit quaternion then the homogeneous form is still a scalar multiple of a rotation matrix, while the inhomogeneous form is in general no longer an orthogonal matrix. This is why in numerical work the homogeneous form is to be preferred if distortion is to be avoided.\" wikipedia [source]","title":"quaternion_to_rotation_matrix"},{"location":"backend/groups/#to_affine_matrix","text":"paz . backend . groups . SE3 . to_affine_matrix ( rotation_matrix , translation ) Builds affine matrix from rotation matrix and translation vector. Arguments rotation_matrix : Array (3, 3). Representing a rotation matrix. translation : Array (3). Translation vector. Returns Array (4, 4) representing an affine matrix. [source]","title":"to_affine_matrix"},{"location":"backend/groups/#rotation_vector_to_rotation_matrix","text":"paz . backend . groups . SO3 . rotation_vector_to_rotation_matrix ( rotation_vector ) Transforms rotation vector (axis-angle) form to rotation matrix. Arguments rotation_vector : Array (3). Rotation vector in axis-angle form. Returns Array (3, 3) rotation matrix. [source]","title":"rotation_vector_to_rotation_matrix"},{"location":"backend/groups/#build_rotation_matrix_x","text":"paz . backend . groups . SO3 . build_rotation_matrix_x ( angle ) Builds rotation matrix in X axis. Arguments angle : Float. Angle in radians. Return Array (3, 3) rotation matrix in Z axis. [source]","title":"build_rotation_matrix_x"},{"location":"backend/groups/#build_rotation_matrix_y","text":"paz . backend . groups . SO3 . build_rotation_matrix_y ( angle ) Builds rotation matrix in Y axis. Arguments angle : Float. Angle in radians. Return Array (3, 3) rotation matrix in Z axis. [source]","title":"build_rotation_matrix_y"},{"location":"backend/groups/#build_rotation_matrix_z","text":"paz . backend . groups . SO3 . build_rotation_matrix_z ( angle ) Builds rotation matrix in Z axis. Arguments angle : Float. Angle in radians. Return Array (3, 3) rotation matrix in Z axis. [source]","title":"build_rotation_matrix_z"},{"location":"backend/groups/#compute_norm_so3","text":"paz . backend . groups . SO3 . compute_norm_SO3 ( rotation_mesh , rotation ) Computes norm between SO3 elements. Arguments rotation_mesh : Array (3, 3), rotation matrix. rotation : Array (3, 3), rotation matrix. Returns Scalar representing the distance between both rotation matrices. [source]","title":"compute_norm_SO3"},{"location":"backend/groups/#calculate_canonical_rotation","text":"paz . backend . groups . SO3 . calculate_canonical_rotation ( rotation_mesh , rotations ) Returns the rotation matrix closest to rotation mesh. Arguments rotation_mesh : Array (3, 3), rotation matrix. rotations : List of array of (3, 3), rotation matrices. Returns Element of list closest to rotation mesh.","title":"calculate_canonical_rotation"},{"location":"backend/heatmaps/","text":"[source] get_keypoints_heatmap paz . backend . heatmaps . get_keypoints_heatmap ( heatmaps , num_keypoints , indices = None , axis = 1 ) Extract the heatmaps that only contains the keypoints. Arguments heatmaps : Numpy array of shape (1, 2*num_keypoints, H, W) num_keypoints : Int. indices : List. Indices of the heatmaps to extract. axis : Int. Returns keypoints : Numpy array of shape (1, num_keypoints, H, W) [source] get_tags_heatmap paz . backend . heatmaps . get_tags_heatmap ( heatmaps , num_keypoints , indices = None , axis = 1 ) Extract the heatmaps that only contains the tags. Arguments heatmaps : Numpy array of shape (1, 2*num_keypoints, H, W) num_keypoints : Int. indices : List. Indices of the heatmaps to extract. axis : Int. Returns tags : Numpy array of shape (1, num_keypoints, H, W) [source] get_keypoints_locations paz . backend . heatmaps . get_keypoints_locations ( indices , image_width ) Calculate the location of keypoints in an image. Arguments indices : Numpy array. Indices of the keypoints in the heatmap. Image width: Int. Returns coordinate : Numpy array. locations of keypoints [source] get_top_k_keypoints_numpy paz . backend . heatmaps . get_top_k_keypoints_numpy ( heatmaps , k ) Numpy implementation of get_top_k_keypoints from heatmaps. Arguments heatmaps : Keypoints heatmaps. Numpy array of shape (1, num_keypoints, H, W) k : Int. Maximum number of instances to return. Returns values : Numpy array. Value of heatmaps at top k keypoints indices : Numpy array. Indices of top k keypoints. [source] get_valid_detections paz . backend . heatmaps . get_valid_detections ( detection , detection_thresh ) Accept the keypoints whose score is greater than the detection threshold. Arguments detection : Numpy array. Contains the location, value, and tags of the keypoints detection_thresh : Float. Detection threshold for the keypoint","title":"Heatmaps"},{"location":"backend/heatmaps/#get_keypoints_heatmap","text":"paz . backend . heatmaps . get_keypoints_heatmap ( heatmaps , num_keypoints , indices = None , axis = 1 ) Extract the heatmaps that only contains the keypoints. Arguments heatmaps : Numpy array of shape (1, 2*num_keypoints, H, W) num_keypoints : Int. indices : List. Indices of the heatmaps to extract. axis : Int. Returns keypoints : Numpy array of shape (1, num_keypoints, H, W) [source]","title":"get_keypoints_heatmap"},{"location":"backend/heatmaps/#get_tags_heatmap","text":"paz . backend . heatmaps . get_tags_heatmap ( heatmaps , num_keypoints , indices = None , axis = 1 ) Extract the heatmaps that only contains the tags. Arguments heatmaps : Numpy array of shape (1, 2*num_keypoints, H, W) num_keypoints : Int. indices : List. Indices of the heatmaps to extract. axis : Int. Returns tags : Numpy array of shape (1, num_keypoints, H, W) [source]","title":"get_tags_heatmap"},{"location":"backend/heatmaps/#get_keypoints_locations","text":"paz . backend . heatmaps . get_keypoints_locations ( indices , image_width ) Calculate the location of keypoints in an image. Arguments indices : Numpy array. Indices of the keypoints in the heatmap. Image width: Int. Returns coordinate : Numpy array. locations of keypoints [source]","title":"get_keypoints_locations"},{"location":"backend/heatmaps/#get_top_k_keypoints_numpy","text":"paz . backend . heatmaps . get_top_k_keypoints_numpy ( heatmaps , k ) Numpy implementation of get_top_k_keypoints from heatmaps. Arguments heatmaps : Keypoints heatmaps. Numpy array of shape (1, num_keypoints, H, W) k : Int. Maximum number of instances to return. Returns values : Numpy array. Value of heatmaps at top k keypoints indices : Numpy array. Indices of top k keypoints. [source]","title":"get_top_k_keypoints_numpy"},{"location":"backend/heatmaps/#get_valid_detections","text":"paz . backend . heatmaps . get_valid_detections ( detection , detection_thresh ) Accept the keypoints whose score is greater than the detection threshold. Arguments detection : Numpy array. Contains the location, value, and tags of the keypoints detection_thresh : Float. Detection threshold for the keypoint","title":"get_valid_detections"},{"location":"backend/image/","text":"[source] resize_image paz . backend . image . opencv_image . resize_image ( image , size , method = 1 ) Resize image. Arguments image : Numpy array. size : List of two ints. method : Flag indicating interpolation method i.e. paz.backend.image.CUBIC Returns Numpy array. [source] convert_color_space paz . backend . image . opencv_image . convert_color_space ( image , flag ) Convert image to a different color space. Arguments image : Numpy array. flag : PAZ or openCV flag. e.g. paz.backend.image.RGB2BGR. Returns Numpy array. [source] load_image paz . backend . image . opencv_image . load_image ( filepath , num_channels = 3 ) Load image from a ''filepath''. Arguments filepath : String indicating full path to the image. num_channels : Int. Returns Numpy array. [source] show_image paz . backend . image . opencv_image . show_image ( image , name = 'image' , wait = True ) Shows RGB image in an external window. Arguments image : Numpy array name : String indicating the window name. wait : Boolean. If ''True'' window stays open until user presses a key. If ''False'' windows closes immediately. [source] warp_affine paz . backend . image . opencv_image . warp_affine ( image , matrix , fill_color = [ 0 , 0 , 0 ], size = None ) Transforms image using an affine matrix transformation. Arguments image : Numpy array. matrix : Numpy array of shape (2,3) indicating affine transformation. fill_color : List/tuple representing BGR use for filling empty space. [source] write_image paz . backend . image . opencv_image . write_image ( filepath , image ) Writes an image inside filepath . If filepath doesn't exist it makes a directory. If image has three channels the image is converted into BGR and then written. This is done such that this function compatible with load_image . Arguments filepath : String with image path. It should include postfix e.g. .png image : Numpy array. [source] gaussian_image_blur paz . backend . image . opencv_image . gaussian_image_blur ( image , kernel_size = ( 5 , 5 )) Applies Gaussian blur to an image. Arguments image : Numpy array of shape ''(H, W, 4)''. kernel_size : List of two ints e.g. ''(5, 5)''. Returns Numpy array [source] median_image_blur paz . backend . image . opencv_image . median_image_blur ( image , apperture = 5 ) Applies median blur to an image. Arguments image : Numpy array of shape ''(H, W, 3)''. apperture. Int. Returns Numpy array. [source] get_rotation_matrix paz . backend . image . opencv_image . get_rotation_matrix ( center , degrees , scale = 1.0 ) Returns a 2D rotation matrix. Arguments center : List of two integer values. degrees : Float indicating the angle in degrees. Returns Numpy array [source] cast_image paz . backend . image . image . cast_image ( image , dtype ) Casts an image into a different type Arguments image : Numpy array. dtype : String or np.dtype. Returns Numpy array. [source] random_saturation paz . backend . image . image . random_saturation ( image , lower = 0.3 , upper = 1.5 ) Applies random saturation to an RGB image. Arguments image : Numpy array representing an image RGB format. lower : Float. upper : Float. [source] random_brightness paz . backend . image . image . random_brightness ( image , delta = 32 ) Applies random brightness to an RGB image. Arguments image : Numpy array representing an image RGB format. delta : Int. [source] random_contrast paz . backend . image . image . random_contrast ( image , lower = 0.5 , upper = 1.5 ) Applies random contrast to an RGB image. Arguments image : Numpy array representing an image RGB format. lower : Float. upper : Float. [source] random_hue paz . backend . image . image . random_hue ( image , delta = 18 ) Applies random hue to an RGB image. Arguments image : Numpy array representing an image RGB format. delta : Int. [source] flip_left_right paz . backend . image . image . flip_left_right ( image ) Flips an image left and right. Arguments image : Numpy array. [source] random_flip_left_right paz . backend . image . image . random_flip_left_right ( image ) Applies random left or right flip. Arguments image : Numpy array. [source] crop_image paz . backend . image . image . crop_image ( image , crop_box ) Resize image. Arguments image : Numpy array. crop_box : List of four ints. Returns Numpy array. [source] image_to_normalized_device_coordinates paz . backend . image . image . image_to_normalized_device_coordinates ( image ) Map image value from [0, 255] -> [-1, 1]. [source] normalized_device_coordinates_to_image paz . backend . image . image . normalized_device_coordinates_to_image ( image ) Map normalized value from [-1, 1] -> [0, 255]. [source] random_shape_crop paz . backend . image . image . random_shape_crop ( image , shape ) Randomly crops an image of the given shape . Arguments image : Numpy array. shape : List of two ints ''(H, W)''. Returns Numpy array of cropped image. [source] make_random_plain_image paz . backend . image . image . make_random_plain_image ( shape ) Makes random plain image by sampling three random values. Arguments shape : Image shape e.g. ''(H, W, 3)''. Returns Numpy array of shape ''(H, W, 3)''. [source] blend_alpha_channel paz . backend . image . image . blend_alpha_channel ( image , background ) Blends image with background using an alpha channel. Arguments image : Numpy array with alpha channel. Shape must be ''(H, W, 4)'' background : Numpy array of shape ''(H, W, 3)''. [source] concatenate_alpha_mask paz . backend . image . image . concatenate_alpha_mask ( image , alpha_mask ) Concatenates alpha mask to image. Arguments image : Numpy array of shape ''(H, W, 3)''. alpha_mask : Numpy array array of shape ''(H, W)''. Returns Numpy array of shape ''(H, W, 4)''. [source] split_and_normalize_alpha_channel paz . backend . image . image . split_and_normalize_alpha_channel ( image ) Splits alpha channel from an RGBA image and normalizes alpha channel. Arguments image : Numpy array of shape ''(H, W, 4)''. Returns List of two numpy arrays containing respectively the image and the alpha channel. [source] random_image_blur paz . backend . image . image . random_image_blur ( image ) Applies random choice blur. Arguments image : Numpy array of shape ''(H, W, 3)''. Returns Numpy array. [source] translate_image paz . backend . image . image . translate_image ( image , translation , fill_color ) Translate image. Arguments image : Numpy array. translation : A list of length two indicating the x,y translation values fill_color : List of three floats representing a color. Returns Numpy array [source] sample_scaled_translation paz . backend . image . image . sample_scaled_translation ( delta_scale , image_shape ) Samples a scaled translation from a uniform distribution. Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. image_shape : List containing the height and width of the image. [source] replace_lower_than_threshold paz . backend . image . image . replace_lower_than_threshold ( source , threshold = 0.001 , replacement = 0.0 ) Replace values from source that are lower than the given threshold. This function doesn't create a new array but does replacement in place. Arguments source : Array. threshold : Float. Values lower than this value will be replaced. replacement : Float. Value taken by elements lower than threshold. Returns Array of same shape as source. [source] normalize_min_max paz . backend . image . image . normalize_min_max ( x , x_min , x_max ) Normalized data using it's maximum and minimum values Arguments x : array x_min : minimum value of x x_max : maximum value of x Returns min-max normalized data [source] sample_scaled_translation paz . backend . image . image . sample_scaled_translation ( delta_scale , image_shape ) Samples a scaled translation from a uniform distribution. Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. image_shape : List containing the height and width of the image. [source] get_rotation_matrix paz . backend . image . opencv_image . get_rotation_matrix ( center , degrees , scale = 1.0 ) Returns a 2D rotation matrix. Arguments center : List of two integer values. degrees : Float indicating the angle in degrees. Returns Numpy array [source] calculate_image_center paz . backend . image . image . calculate_image_center ( image ) Return image center. Arguments image : Numpy array. Returns image center. [source] get_affine_transform paz . backend . image . opencv_image . get_affine_transform ( source_points , destination_points ) Return the transformation matrix. Arguments source_points : Numpy array. destination_points : Numpy array. Returns Transformation matrix.","title":"Image"},{"location":"backend/image/#resize_image","text":"paz . backend . image . opencv_image . resize_image ( image , size , method = 1 ) Resize image. Arguments image : Numpy array. size : List of two ints. method : Flag indicating interpolation method i.e. paz.backend.image.CUBIC Returns Numpy array. [source]","title":"resize_image"},{"location":"backend/image/#convert_color_space","text":"paz . backend . image . opencv_image . convert_color_space ( image , flag ) Convert image to a different color space. Arguments image : Numpy array. flag : PAZ or openCV flag. e.g. paz.backend.image.RGB2BGR. Returns Numpy array. [source]","title":"convert_color_space"},{"location":"backend/image/#load_image","text":"paz . backend . image . opencv_image . load_image ( filepath , num_channels = 3 ) Load image from a ''filepath''. Arguments filepath : String indicating full path to the image. num_channels : Int. Returns Numpy array. [source]","title":"load_image"},{"location":"backend/image/#show_image","text":"paz . backend . image . opencv_image . show_image ( image , name = 'image' , wait = True ) Shows RGB image in an external window. Arguments image : Numpy array name : String indicating the window name. wait : Boolean. If ''True'' window stays open until user presses a key. If ''False'' windows closes immediately. [source]","title":"show_image"},{"location":"backend/image/#warp_affine","text":"paz . backend . image . opencv_image . warp_affine ( image , matrix , fill_color = [ 0 , 0 , 0 ], size = None ) Transforms image using an affine matrix transformation. Arguments image : Numpy array. matrix : Numpy array of shape (2,3) indicating affine transformation. fill_color : List/tuple representing BGR use for filling empty space. [source]","title":"warp_affine"},{"location":"backend/image/#write_image","text":"paz . backend . image . opencv_image . write_image ( filepath , image ) Writes an image inside filepath . If filepath doesn't exist it makes a directory. If image has three channels the image is converted into BGR and then written. This is done such that this function compatible with load_image . Arguments filepath : String with image path. It should include postfix e.g. .png image : Numpy array. [source]","title":"write_image"},{"location":"backend/image/#gaussian_image_blur","text":"paz . backend . image . opencv_image . gaussian_image_blur ( image , kernel_size = ( 5 , 5 )) Applies Gaussian blur to an image. Arguments image : Numpy array of shape ''(H, W, 4)''. kernel_size : List of two ints e.g. ''(5, 5)''. Returns Numpy array [source]","title":"gaussian_image_blur"},{"location":"backend/image/#median_image_blur","text":"paz . backend . image . opencv_image . median_image_blur ( image , apperture = 5 ) Applies median blur to an image. Arguments image : Numpy array of shape ''(H, W, 3)''. apperture. Int. Returns Numpy array. [source]","title":"median_image_blur"},{"location":"backend/image/#get_rotation_matrix","text":"paz . backend . image . opencv_image . get_rotation_matrix ( center , degrees , scale = 1.0 ) Returns a 2D rotation matrix. Arguments center : List of two integer values. degrees : Float indicating the angle in degrees. Returns Numpy array [source]","title":"get_rotation_matrix"},{"location":"backend/image/#cast_image","text":"paz . backend . image . image . cast_image ( image , dtype ) Casts an image into a different type Arguments image : Numpy array. dtype : String or np.dtype. Returns Numpy array. [source]","title":"cast_image"},{"location":"backend/image/#random_saturation","text":"paz . backend . image . image . random_saturation ( image , lower = 0.3 , upper = 1.5 ) Applies random saturation to an RGB image. Arguments image : Numpy array representing an image RGB format. lower : Float. upper : Float. [source]","title":"random_saturation"},{"location":"backend/image/#random_brightness","text":"paz . backend . image . image . random_brightness ( image , delta = 32 ) Applies random brightness to an RGB image. Arguments image : Numpy array representing an image RGB format. delta : Int. [source]","title":"random_brightness"},{"location":"backend/image/#random_contrast","text":"paz . backend . image . image . random_contrast ( image , lower = 0.5 , upper = 1.5 ) Applies random contrast to an RGB image. Arguments image : Numpy array representing an image RGB format. lower : Float. upper : Float. [source]","title":"random_contrast"},{"location":"backend/image/#random_hue","text":"paz . backend . image . image . random_hue ( image , delta = 18 ) Applies random hue to an RGB image. Arguments image : Numpy array representing an image RGB format. delta : Int. [source]","title":"random_hue"},{"location":"backend/image/#flip_left_right","text":"paz . backend . image . image . flip_left_right ( image ) Flips an image left and right. Arguments image : Numpy array. [source]","title":"flip_left_right"},{"location":"backend/image/#random_flip_left_right","text":"paz . backend . image . image . random_flip_left_right ( image ) Applies random left or right flip. Arguments image : Numpy array. [source]","title":"random_flip_left_right"},{"location":"backend/image/#crop_image","text":"paz . backend . image . image . crop_image ( image , crop_box ) Resize image. Arguments image : Numpy array. crop_box : List of four ints. Returns Numpy array. [source]","title":"crop_image"},{"location":"backend/image/#image_to_normalized_device_coordinates","text":"paz . backend . image . image . image_to_normalized_device_coordinates ( image ) Map image value from [0, 255] -> [-1, 1]. [source]","title":"image_to_normalized_device_coordinates"},{"location":"backend/image/#normalized_device_coordinates_to_image","text":"paz . backend . image . image . normalized_device_coordinates_to_image ( image ) Map normalized value from [-1, 1] -> [0, 255]. [source]","title":"normalized_device_coordinates_to_image"},{"location":"backend/image/#random_shape_crop","text":"paz . backend . image . image . random_shape_crop ( image , shape ) Randomly crops an image of the given shape . Arguments image : Numpy array. shape : List of two ints ''(H, W)''. Returns Numpy array of cropped image. [source]","title":"random_shape_crop"},{"location":"backend/image/#make_random_plain_image","text":"paz . backend . image . image . make_random_plain_image ( shape ) Makes random plain image by sampling three random values. Arguments shape : Image shape e.g. ''(H, W, 3)''. Returns Numpy array of shape ''(H, W, 3)''. [source]","title":"make_random_plain_image"},{"location":"backend/image/#blend_alpha_channel","text":"paz . backend . image . image . blend_alpha_channel ( image , background ) Blends image with background using an alpha channel. Arguments image : Numpy array with alpha channel. Shape must be ''(H, W, 4)'' background : Numpy array of shape ''(H, W, 3)''. [source]","title":"blend_alpha_channel"},{"location":"backend/image/#concatenate_alpha_mask","text":"paz . backend . image . image . concatenate_alpha_mask ( image , alpha_mask ) Concatenates alpha mask to image. Arguments image : Numpy array of shape ''(H, W, 3)''. alpha_mask : Numpy array array of shape ''(H, W)''. Returns Numpy array of shape ''(H, W, 4)''. [source]","title":"concatenate_alpha_mask"},{"location":"backend/image/#split_and_normalize_alpha_channel","text":"paz . backend . image . image . split_and_normalize_alpha_channel ( image ) Splits alpha channel from an RGBA image and normalizes alpha channel. Arguments image : Numpy array of shape ''(H, W, 4)''. Returns List of two numpy arrays containing respectively the image and the alpha channel. [source]","title":"split_and_normalize_alpha_channel"},{"location":"backend/image/#random_image_blur","text":"paz . backend . image . image . random_image_blur ( image ) Applies random choice blur. Arguments image : Numpy array of shape ''(H, W, 3)''. Returns Numpy array. [source]","title":"random_image_blur"},{"location":"backend/image/#translate_image","text":"paz . backend . image . image . translate_image ( image , translation , fill_color ) Translate image. Arguments image : Numpy array. translation : A list of length two indicating the x,y translation values fill_color : List of three floats representing a color. Returns Numpy array [source]","title":"translate_image"},{"location":"backend/image/#sample_scaled_translation","text":"paz . backend . image . image . sample_scaled_translation ( delta_scale , image_shape ) Samples a scaled translation from a uniform distribution. Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. image_shape : List containing the height and width of the image. [source]","title":"sample_scaled_translation"},{"location":"backend/image/#replace_lower_than_threshold","text":"paz . backend . image . image . replace_lower_than_threshold ( source , threshold = 0.001 , replacement = 0.0 ) Replace values from source that are lower than the given threshold. This function doesn't create a new array but does replacement in place. Arguments source : Array. threshold : Float. Values lower than this value will be replaced. replacement : Float. Value taken by elements lower than threshold. Returns Array of same shape as source. [source]","title":"replace_lower_than_threshold"},{"location":"backend/image/#normalize_min_max","text":"paz . backend . image . image . normalize_min_max ( x , x_min , x_max ) Normalized data using it's maximum and minimum values Arguments x : array x_min : minimum value of x x_max : maximum value of x Returns min-max normalized data [source]","title":"normalize_min_max"},{"location":"backend/image/#sample_scaled_translation_1","text":"paz . backend . image . image . sample_scaled_translation ( delta_scale , image_shape ) Samples a scaled translation from a uniform distribution. Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. image_shape : List containing the height and width of the image. [source]","title":"sample_scaled_translation"},{"location":"backend/image/#get_rotation_matrix_1","text":"paz . backend . image . opencv_image . get_rotation_matrix ( center , degrees , scale = 1.0 ) Returns a 2D rotation matrix. Arguments center : List of two integer values. degrees : Float indicating the angle in degrees. Returns Numpy array [source]","title":"get_rotation_matrix"},{"location":"backend/image/#calculate_image_center","text":"paz . backend . image . image . calculate_image_center ( image ) Return image center. Arguments image : Numpy array. Returns image center. [source]","title":"calculate_image_center"},{"location":"backend/image/#get_affine_transform","text":"paz . backend . image . opencv_image . get_affine_transform ( source_points , destination_points ) Return the transformation matrix. Arguments source_points : Numpy array. destination_points : Numpy array. Returns Transformation matrix.","title":"get_affine_transform"},{"location":"backend/keypoints/","text":"Backend functionality for 2D keypoints [source] build_cube_points3D paz . backend . keypoints . build_cube_points3D ( width , height , depth ) Build the 3D points of a cube in the openCV coordinate system: 4--------1 /| /| / | / | 3--------2 | | 8 _ | 5 | / | / |/ |/ 7--------6 Z (depth) / /_____X (width) | | Y (height) Arguments height : float, height of the 3D box. width : float, width of the 3D box. depth : float, width of the 3D box. Returns Numpy array of shape ``(8, 3)'' corresponding to 3D keypoints of a cube [source] normalize_keypoints2D paz . backend . keypoints . normalize_keypoints2D ( points2D , height , width ) Transform points2D in image coordinates to normalized coordinates i.e. [U, V] -> [-1, 1]. UV have maximum values of [W, H] respectively. Image plane width (0,0)--------> (U) | height | | v (V) Arguments points2D : Numpy array of shape (num_keypoints, 2). height : Int. Height of the image width : Int. Width of the image Returns Numpy array of shape (num_keypoints, 2). [source] denormalize_keypoints2D paz . backend . keypoints . denormalize_keypoints2D ( points2D , height , width ) Transform nomralized points2D to image UV coordinates i.e. [-1, 1] -> [U, V]. UV have maximum values of [W, H] respectively. Image plane (0,0)--------> (U) | | | v (V) Arguments points2D : Numpy array of shape (num_keypoints, 2). height : Int. Height of the image width : Int. Width of the image Returns Numpy array of shape (num_keypoints, 2). [source] project_to_image paz . backend . keypoints . project_to_image ( rotation , translation , points3D , camera_intrinsics ) Project points3D to image plane using a perspective transformation. Image plane (0,0)--------> (U) | | | v (V) Arguments rotation : Array (3, 3). Rotation matrix (Rco). translation : Array (3). Translation (Tco). points3D : Array (num_points, 3). Points 3D in object frame. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. Returns Array (num_points, 2) in UV image space. [source] solve_PnP_RANSAC paz . backend . keypoints . solve_PnP_RANSAC ( object_points3D , image_points2D , camera_intrinsics , inlier_threshold = 5 , num_iterations = 100 ) Returns rotation (Roc) and translation (Toc) vectors that transform 3D points in object frame to camera frame. O------------O /| /| / | / | O------------O | | | z | | | O _| _|__O | / |___y| / object | / / | / coordinates |/ x |/ O------------O Z | / | Rco, Tco /_____X <------| | | camera Y coordinates Arguments object_points3D : Array (num_points, 3). Points 3D in object reference frame. Represented as (0) in image above. image_points2D : Array (num_points, 2). Points in 2D in camera UV space. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. inlier_threshold : Number of inliers for RANSAC method. num_iterations : Maximum number of iterations. Returns Rotation vector in axis-angle form (3) and translation vector (3). [source] arguments_to_image_points2D paz . backend . keypoints . arguments_to_image_points2D ( row_args , col_args ) Convert array arguments into UV coordinates. Image plane (0,0)--------> (U) | | | v (V) Arguments row_args : Array (num_rows). col_args : Array (num_cols). Returns Array (num_cols, num_rows) representing points2D in UV space. Notes Arguments are row args (V) and col args (U). Image points are in UV coordinates; thus, we concatenate them in that order i.e. [col_args, row_args] [source] points3D_to_RGB paz . backend . keypoints . points3D_to_RGB ( points3D , object_sizes ) Transforms points3D in object frame to RGB color space. Arguments points3D : Array (num_points, 3). Points3D a object_sizes : Array (3) indicating the (width, height, depth) of object. Returns Array of ints (num_points, 3) in RGB space. [source] cascade_classifier paz . backend . keypoints . cascade_classifier ( path ) OpenCV Cascade classifier. Arguments path : String. Path to default openCV XML format. Returns OpenCV classifier with detectMultiScale for inference.. [source] project_points3D paz . backend . keypoints . project_points3D ( points3D , pose6D , camera ) Projects 3D points into a specific pose. Arguments points3D : Numpy array of shape (num_points, 3) . pose6D : An instance of paz.abstract.Pose6D . camera : An instance of paz.backend.Camera object. Returns Numpy array of shape (num_points, 2) [source] solve_PNP paz . backend . keypoints . solve_PNP ( points3D , points2D , camera , solver ) Calculates 6D pose from 3D points and 2D keypoints correspondences. Arguments points3D : Numpy array of shape (num_points, 3) . 3D points known in advance. points2D : Numpy array of shape (num_points, 2) . Predicted 2D keypoints of object. camera : Instance of ''paz.backend.Camera'' containing as properties the ''camera_intrinsics'' a Numpy array of shape ''(3, 3)'' usually calculated from the openCV ''calibrateCamera'' function, and the ''distortion'' a Numpy array of shape ''(5)'' in which the elements are usually obtained from the openCV ''calibrateCamera'' function. solver : Flag from e.g openCV.SOLVEPNP_UPNP. distortion : Numpy array of shape of 5 elements calculated from the openCV calibrateCamera function. Returns A list containing success flag, rotation and translation components of the 6D pose. [source] translate_keypoints paz . backend . keypoints . translate_keypoints ( keypoints , translation ) Translate keypoints. Arguments kepoints : Numpy array of shape (num_keypoints, 2) . translation : A list of length two indicating the x,y translation values Returns Numpy array [source] rotate_keypoint paz . backend . keypoints . rotate_keypoint ( point2D , rotation_angle ) Rotate keypoint. Arguments point2D : keypoint [x, y] rotation angle: Int. Angle of rotation. Returns List of x and y rotated points [source] transform_keypoint paz . backend . keypoints . transform_keypoint ( keypoint , transform ) Transform keypoint. Arguments keypoint2D : keypoint [x, y] transform : Numpy array. Transformation matrix [source] add_offset_to_point paz . backend . keypoints . add_offset_to_point ( keypoint_location , offset = 0 ) Add offset to keypoint location Arguments keypoint_location : keypoint [y, x] offset : Float.","title":"Keypoints"},{"location":"backend/keypoints/#build_cube_points3d","text":"paz . backend . keypoints . build_cube_points3D ( width , height , depth ) Build the 3D points of a cube in the openCV coordinate system: 4--------1 /| /| / | / | 3--------2 | | 8 _ | 5 | / | / |/ |/ 7--------6 Z (depth) / /_____X (width) | | Y (height) Arguments height : float, height of the 3D box. width : float, width of the 3D box. depth : float, width of the 3D box. Returns Numpy array of shape ``(8, 3)'' corresponding to 3D keypoints of a cube [source]","title":"build_cube_points3D"},{"location":"backend/keypoints/#normalize_keypoints2d","text":"paz . backend . keypoints . normalize_keypoints2D ( points2D , height , width ) Transform points2D in image coordinates to normalized coordinates i.e. [U, V] -> [-1, 1]. UV have maximum values of [W, H] respectively. Image plane width (0,0)--------> (U) | height | | v (V) Arguments points2D : Numpy array of shape (num_keypoints, 2). height : Int. Height of the image width : Int. Width of the image Returns Numpy array of shape (num_keypoints, 2). [source]","title":"normalize_keypoints2D"},{"location":"backend/keypoints/#denormalize_keypoints2d","text":"paz . backend . keypoints . denormalize_keypoints2D ( points2D , height , width ) Transform nomralized points2D to image UV coordinates i.e. [-1, 1] -> [U, V]. UV have maximum values of [W, H] respectively. Image plane (0,0)--------> (U) | | | v (V) Arguments points2D : Numpy array of shape (num_keypoints, 2). height : Int. Height of the image width : Int. Width of the image Returns Numpy array of shape (num_keypoints, 2). [source]","title":"denormalize_keypoints2D"},{"location":"backend/keypoints/#project_to_image","text":"paz . backend . keypoints . project_to_image ( rotation , translation , points3D , camera_intrinsics ) Project points3D to image plane using a perspective transformation. Image plane (0,0)--------> (U) | | | v (V) Arguments rotation : Array (3, 3). Rotation matrix (Rco). translation : Array (3). Translation (Tco). points3D : Array (num_points, 3). Points 3D in object frame. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. Returns Array (num_points, 2) in UV image space. [source]","title":"project_to_image"},{"location":"backend/keypoints/#solve_pnp_ransac","text":"paz . backend . keypoints . solve_PnP_RANSAC ( object_points3D , image_points2D , camera_intrinsics , inlier_threshold = 5 , num_iterations = 100 ) Returns rotation (Roc) and translation (Toc) vectors that transform 3D points in object frame to camera frame. O------------O /| /| / | / | O------------O | | | z | | | O _| _|__O | / |___y| / object | / / | / coordinates |/ x |/ O------------O Z | / | Rco, Tco /_____X <------| | | camera Y coordinates Arguments object_points3D : Array (num_points, 3). Points 3D in object reference frame. Represented as (0) in image above. image_points2D : Array (num_points, 2). Points in 2D in camera UV space. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. inlier_threshold : Number of inliers for RANSAC method. num_iterations : Maximum number of iterations. Returns Rotation vector in axis-angle form (3) and translation vector (3). [source]","title":"solve_PnP_RANSAC"},{"location":"backend/keypoints/#arguments_to_image_points2d","text":"paz . backend . keypoints . arguments_to_image_points2D ( row_args , col_args ) Convert array arguments into UV coordinates. Image plane (0,0)--------> (U) | | | v (V) Arguments row_args : Array (num_rows). col_args : Array (num_cols). Returns Array (num_cols, num_rows) representing points2D in UV space. Notes Arguments are row args (V) and col args (U). Image points are in UV coordinates; thus, we concatenate them in that order i.e. [col_args, row_args] [source]","title":"arguments_to_image_points2D"},{"location":"backend/keypoints/#points3d_to_rgb","text":"paz . backend . keypoints . points3D_to_RGB ( points3D , object_sizes ) Transforms points3D in object frame to RGB color space. Arguments points3D : Array (num_points, 3). Points3D a object_sizes : Array (3) indicating the (width, height, depth) of object. Returns Array of ints (num_points, 3) in RGB space. [source]","title":"points3D_to_RGB"},{"location":"backend/keypoints/#cascade_classifier","text":"paz . backend . keypoints . cascade_classifier ( path ) OpenCV Cascade classifier. Arguments path : String. Path to default openCV XML format. Returns OpenCV classifier with detectMultiScale for inference.. [source]","title":"cascade_classifier"},{"location":"backend/keypoints/#project_points3d","text":"paz . backend . keypoints . project_points3D ( points3D , pose6D , camera ) Projects 3D points into a specific pose. Arguments points3D : Numpy array of shape (num_points, 3) . pose6D : An instance of paz.abstract.Pose6D . camera : An instance of paz.backend.Camera object. Returns Numpy array of shape (num_points, 2) [source]","title":"project_points3D"},{"location":"backend/keypoints/#solve_pnp","text":"paz . backend . keypoints . solve_PNP ( points3D , points2D , camera , solver ) Calculates 6D pose from 3D points and 2D keypoints correspondences. Arguments points3D : Numpy array of shape (num_points, 3) . 3D points known in advance. points2D : Numpy array of shape (num_points, 2) . Predicted 2D keypoints of object. camera : Instance of ''paz.backend.Camera'' containing as properties the ''camera_intrinsics'' a Numpy array of shape ''(3, 3)'' usually calculated from the openCV ''calibrateCamera'' function, and the ''distortion'' a Numpy array of shape ''(5)'' in which the elements are usually obtained from the openCV ''calibrateCamera'' function. solver : Flag from e.g openCV.SOLVEPNP_UPNP. distortion : Numpy array of shape of 5 elements calculated from the openCV calibrateCamera function. Returns A list containing success flag, rotation and translation components of the 6D pose. [source]","title":"solve_PNP"},{"location":"backend/keypoints/#translate_keypoints","text":"paz . backend . keypoints . translate_keypoints ( keypoints , translation ) Translate keypoints. Arguments kepoints : Numpy array of shape (num_keypoints, 2) . translation : A list of length two indicating the x,y translation values Returns Numpy array [source]","title":"translate_keypoints"},{"location":"backend/keypoints/#rotate_keypoint","text":"paz . backend . keypoints . rotate_keypoint ( point2D , rotation_angle ) Rotate keypoint. Arguments point2D : keypoint [x, y] rotation angle: Int. Angle of rotation. Returns List of x and y rotated points [source]","title":"rotate_keypoint"},{"location":"backend/keypoints/#transform_keypoint","text":"paz . backend . keypoints . transform_keypoint ( keypoint , transform ) Transform keypoint. Arguments keypoint2D : keypoint [x, y] transform : Numpy array. Transformation matrix [source]","title":"transform_keypoint"},{"location":"backend/keypoints/#add_offset_to_point","text":"paz . backend . keypoints . add_offset_to_point ( keypoint_location , offset = 0 ) Add offset to keypoint location Arguments keypoint_location : keypoint [y, x] offset : Float.","title":"add_offset_to_point"},{"location":"backend/quaternion/","text":"Backend functionality for quaternions {{autogenerated}}","title":"Quaternions"},{"location":"backend/render/","text":"[source] compute_modelview_matrices paz . backend . render . compute_modelview_matrices ( camera_origin , world_origin , roll = None , translate = None ) Compute model-view matrices from camera to origin and origin to camera. Arguments camera_origin : Numpy-array of length 3 determining the camera origin world_origin : Numpy-array of length 3 determining the world origin roll : None or float. If None no roll is performed. If float value should be between [0, 2*pi) Returns Transformation from camera to world and world to camera. [source] get_look_at_transform paz . backend . render . get_look_at_transform ( camera_position , target_position ) Make transformation from target position to camera position with orientation looking at the target position. Arguments camera_position : Numpy-array of length 3. Camera position. target_position : Numpy-array of length 3. Target position. [source] random_perturbation paz . backend . render . random_perturbation ( localization , shift ) Adds noise to 'localization' vector coordinates. Arguments localization : List of 3 floats. shift : Float indicating a uniform distribution [-shift, shift]. Returns perturbed localization [source] random_translation paz . backend . render . random_translation ( localization , shift ) Adds noise to 'localization' vector coordinates. Arguments localization : List of 3 floats. shift : Float indicating a uniform distribution [-shift, shift]. Returns perturbed localization [source] roll_camera paz . backend . render . roll_camera ( world_to_camera , angle ) Roll camera coordinate system. Arguments: world_to_camera : Numpy array containing the affine transformation. max_roll : 'None' or float. If None, the camera is not rolled. If float it should be a value between [0, 2*pi) [source] sample_point_in_full_sphere paz . backend . render . sample_point_in_full_sphere ( distance = 1.0 ) Get a point of the top of the unit sphere. Arguments distance : Float, indicating distance to origin. Returns sphere_point : List of spatial coordinates of a sphere. [source] sample_point_in_sphere paz . backend . render . sample_point_in_sphere ( distance , top_only = False ) Samples random points from a sphere Arguments distance : Float, indicating distance to origin. Returns: List of spatial coordinates of a sphere. [source] sample_point_in_top_sphere paz . backend . render . sample_point_in_top_sphere ( distance = 1.0 ) Get a point of the top of the unit sphere. Arguments distance : Float, indicating distance to origin. Returns sphere_point : List of spatial coordinates of a sphere. [source] sample_uniformly paz . backend . render . sample_uniformly ( value ) Samples from a uniform distribution. Arguments values : List or float. If list it must have [min_value, max_value]. Returns Float [source] scale_translation paz . backend . render . scale_translation ( matrix , scale = 10.0 ) Changes the scale of the translation vector. Used for changing the regression problem to a bigger scale. Arguments: matrix : Numpy array of shape [4, 4] scale : Float used to multiple all the translation component. Returns: Numpy array of shape [4, 4] [source] split_alpha_channel paz . backend . render . split_alpha_channel ( image ) Splits alpha channel from an RGBD image. Arguments image : Numpy array of shape [H, W, 4] Returns List of two numpy arrays of shape [H, W, 3] and [H, W] [source] translate_camera paz . backend . render . translate_camera ( world_to_camera , translation ) Translate camera coordinate system in its XY plane. Arguments: world_to_camera : Numpy array containing the affine transformation. translation : List or array with two inputs.","title":"Render"},{"location":"backend/render/#compute_modelview_matrices","text":"paz . backend . render . compute_modelview_matrices ( camera_origin , world_origin , roll = None , translate = None ) Compute model-view matrices from camera to origin and origin to camera. Arguments camera_origin : Numpy-array of length 3 determining the camera origin world_origin : Numpy-array of length 3 determining the world origin roll : None or float. If None no roll is performed. If float value should be between [0, 2*pi) Returns Transformation from camera to world and world to camera. [source]","title":"compute_modelview_matrices"},{"location":"backend/render/#get_look_at_transform","text":"paz . backend . render . get_look_at_transform ( camera_position , target_position ) Make transformation from target position to camera position with orientation looking at the target position. Arguments camera_position : Numpy-array of length 3. Camera position. target_position : Numpy-array of length 3. Target position. [source]","title":"get_look_at_transform"},{"location":"backend/render/#random_perturbation","text":"paz . backend . render . random_perturbation ( localization , shift ) Adds noise to 'localization' vector coordinates. Arguments localization : List of 3 floats. shift : Float indicating a uniform distribution [-shift, shift]. Returns perturbed localization [source]","title":"random_perturbation"},{"location":"backend/render/#random_translation","text":"paz . backend . render . random_translation ( localization , shift ) Adds noise to 'localization' vector coordinates. Arguments localization : List of 3 floats. shift : Float indicating a uniform distribution [-shift, shift]. Returns perturbed localization [source]","title":"random_translation"},{"location":"backend/render/#roll_camera","text":"paz . backend . render . roll_camera ( world_to_camera , angle ) Roll camera coordinate system. Arguments: world_to_camera : Numpy array containing the affine transformation. max_roll : 'None' or float. If None, the camera is not rolled. If float it should be a value between [0, 2*pi) [source]","title":"roll_camera"},{"location":"backend/render/#sample_point_in_full_sphere","text":"paz . backend . render . sample_point_in_full_sphere ( distance = 1.0 ) Get a point of the top of the unit sphere. Arguments distance : Float, indicating distance to origin. Returns sphere_point : List of spatial coordinates of a sphere. [source]","title":"sample_point_in_full_sphere"},{"location":"backend/render/#sample_point_in_sphere","text":"paz . backend . render . sample_point_in_sphere ( distance , top_only = False ) Samples random points from a sphere Arguments distance : Float, indicating distance to origin. Returns: List of spatial coordinates of a sphere. [source]","title":"sample_point_in_sphere"},{"location":"backend/render/#sample_point_in_top_sphere","text":"paz . backend . render . sample_point_in_top_sphere ( distance = 1.0 ) Get a point of the top of the unit sphere. Arguments distance : Float, indicating distance to origin. Returns sphere_point : List of spatial coordinates of a sphere. [source]","title":"sample_point_in_top_sphere"},{"location":"backend/render/#sample_uniformly","text":"paz . backend . render . sample_uniformly ( value ) Samples from a uniform distribution. Arguments values : List or float. If list it must have [min_value, max_value]. Returns Float [source]","title":"sample_uniformly"},{"location":"backend/render/#scale_translation","text":"paz . backend . render . scale_translation ( matrix , scale = 10.0 ) Changes the scale of the translation vector. Used for changing the regression problem to a bigger scale. Arguments: matrix : Numpy array of shape [4, 4] scale : Float used to multiple all the translation component. Returns: Numpy array of shape [4, 4] [source]","title":"scale_translation"},{"location":"backend/render/#split_alpha_channel","text":"paz . backend . render . split_alpha_channel ( image ) Splits alpha channel from an RGBD image. Arguments image : Numpy array of shape [H, W, 4] Returns List of two numpy arrays of shape [H, W, 3] and [H, W] [source]","title":"split_alpha_channel"},{"location":"backend/render/#translate_camera","text":"paz . backend . render . translate_camera ( world_to_camera , translation ) Translate camera coordinate system in its XY plane. Arguments: world_to_camera : Numpy array containing the affine transformation. translation : List or array with two inputs.","title":"translate_camera"},{"location":"backend/standard/","text":"[source] get_upper_multiple paz . backend . standard . get_upper_multiple ( x , multiple = 64 ) Returns the upper multiple of 'multiple' to the x. Arguments x : Int. multiple : Int. Returns upper multiple. Int. [source] resize_with_same_aspect_ratio paz . backend . standard . resize_with_same_aspect_ratio ( image , input_size , multiple = 64 ) Resize the sort side of the input image to input_size and keep the aspect ratio. Arguments input_size : Dimension to be resized. Int. H : Int. W : Int. Returns resized H and W. [source] get_transformation_scale paz . backend . standard . get_transformation_scale ( image , size , scaling_factor ) Caluclte scale of resized H and W. Arguments H : Int. H_resized : Int. H_resized : Int. scaling_factor : Int. Returns scaled H and W [source] compare_vertical_neighbours paz . backend . standard . compare_vertical_neighbours ( x , y , image , offset = 0.25 ) Compare two vertical neighbors and add an offset to the smaller one. Arguments x : Int. x coordinate of pixel to be compared. y : Int. y coordinate of pixel to be compared. image : Numpy array. offset : Float. [source] compare_horizontal_neighbours paz . backend . standard . compare_horizontal_neighbours ( x , y , image , offset = 0.25 ) Compare two horizontal neighbors and add an offset to the smaller one. Arguments x : Int. x coordinate of pixel to be compared. y : Int. y coordinate of pixel to be compared. image : Numpy array. offset : Float. [source] get_all_indices_of_array paz . backend . standard . get_all_indices_of_array ( array ) Get all the indices of an array. Arguments array : Numpy array Returns Numpy array. Array with the indices of the input array [source] gather_nd paz . backend . standard . gather_nd ( array , indices , axis ) Take the value from the input array on the given indices along the given axis. Arguments array : Numpy array indices : list/numpy array. values to be gathered from axis : Int. Axis along which to gather values. Returns Numpy array. Gathered values from the input array [source] calculate_norm paz . backend . standard . calculate_norm ( vector , order = None , axis = None ) Calculates the norm of vector. Arguments x : List of spatial coordinates (x, y, z) [source] tensor_to_numpy paz . backend . standard . tensor_to_numpy ( tensor ) Convert a tensor to a numpy array. Arguments tensor : multidimensional array of type tensor [source] pad_matrix paz . backend . standard . pad_matrix ( matrix , pool_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = 'valid' , value = 0 ) Pad an array Arguments matrix : Numpy array. padding : String. Type of padding value : Int. Value to be added in the padded area. poolsize : Int. How many rows and colums to be padded for 'same' padding [source] max_pooling_2d paz . backend . standard . max_pooling_2d ( image , pool_size = 3 , strides = 1 , padding = 'same' ) Returns the maximum pooled value of an image. Arguments image : Numpy array. poolsize : Int or list of len 2. Window size for each pool padding : String. Type of padding","title":"Standard"},{"location":"backend/standard/#get_upper_multiple","text":"paz . backend . standard . get_upper_multiple ( x , multiple = 64 ) Returns the upper multiple of 'multiple' to the x. Arguments x : Int. multiple : Int. Returns upper multiple. Int. [source]","title":"get_upper_multiple"},{"location":"backend/standard/#resize_with_same_aspect_ratio","text":"paz . backend . standard . resize_with_same_aspect_ratio ( image , input_size , multiple = 64 ) Resize the sort side of the input image to input_size and keep the aspect ratio. Arguments input_size : Dimension to be resized. Int. H : Int. W : Int. Returns resized H and W. [source]","title":"resize_with_same_aspect_ratio"},{"location":"backend/standard/#get_transformation_scale","text":"paz . backend . standard . get_transformation_scale ( image , size , scaling_factor ) Caluclte scale of resized H and W. Arguments H : Int. H_resized : Int. H_resized : Int. scaling_factor : Int. Returns scaled H and W [source]","title":"get_transformation_scale"},{"location":"backend/standard/#compare_vertical_neighbours","text":"paz . backend . standard . compare_vertical_neighbours ( x , y , image , offset = 0.25 ) Compare two vertical neighbors and add an offset to the smaller one. Arguments x : Int. x coordinate of pixel to be compared. y : Int. y coordinate of pixel to be compared. image : Numpy array. offset : Float. [source]","title":"compare_vertical_neighbours"},{"location":"backend/standard/#compare_horizontal_neighbours","text":"paz . backend . standard . compare_horizontal_neighbours ( x , y , image , offset = 0.25 ) Compare two horizontal neighbors and add an offset to the smaller one. Arguments x : Int. x coordinate of pixel to be compared. y : Int. y coordinate of pixel to be compared. image : Numpy array. offset : Float. [source]","title":"compare_horizontal_neighbours"},{"location":"backend/standard/#get_all_indices_of_array","text":"paz . backend . standard . get_all_indices_of_array ( array ) Get all the indices of an array. Arguments array : Numpy array Returns Numpy array. Array with the indices of the input array [source]","title":"get_all_indices_of_array"},{"location":"backend/standard/#gather_nd","text":"paz . backend . standard . gather_nd ( array , indices , axis ) Take the value from the input array on the given indices along the given axis. Arguments array : Numpy array indices : list/numpy array. values to be gathered from axis : Int. Axis along which to gather values. Returns Numpy array. Gathered values from the input array [source]","title":"gather_nd"},{"location":"backend/standard/#calculate_norm","text":"paz . backend . standard . calculate_norm ( vector , order = None , axis = None ) Calculates the norm of vector. Arguments x : List of spatial coordinates (x, y, z) [source]","title":"calculate_norm"},{"location":"backend/standard/#tensor_to_numpy","text":"paz . backend . standard . tensor_to_numpy ( tensor ) Convert a tensor to a numpy array. Arguments tensor : multidimensional array of type tensor [source]","title":"tensor_to_numpy"},{"location":"backend/standard/#pad_matrix","text":"paz . backend . standard . pad_matrix ( matrix , pool_size = ( 3 , 3 ), strides = ( 1 , 1 ), padding = 'valid' , value = 0 ) Pad an array Arguments matrix : Numpy array. padding : String. Type of padding value : Int. Value to be added in the padded area. poolsize : Int. How many rows and colums to be padded for 'same' padding [source]","title":"pad_matrix"},{"location":"backend/standard/#max_pooling_2d","text":"paz . backend . standard . max_pooling_2d ( image , pool_size = 3 , strides = 1 , padding = 'same' ) Returns the maximum pooled value of an image. Arguments image : Numpy array. poolsize : Int or list of len 2. Window size for each pool padding : String. Type of padding","title":"max_pooling_2d"},{"location":"getting-started/bounding_boxes/","text":"Bounding boxes In this tutorial we show you how to build bounding boxes for your own training pipeline: You can find the complete script of this tutorial here Let's do our basic imports import os import numpy as np from tensorflow.keras.utils import get_file import paz.processors as pr from paz.abstract import SequentialProcessor from paz.backend.image import load_image First we will download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/object_detection_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' ) Let's first build our labels: Keep in mind that the origin of our images is located at the top-left . The x_min, y_min are the normalized coordinates of top-left bounding-box corner. height , width = load_image ( image_fullpath ) . shape [: 2 ] x_min_human , y_min_human = 200 / width , 60 / height x_min_horse , y_min_horse = 100 / width , 90 / height The x_max, y_max are the normalized coordinates of bottom-right bounding-box corner. x_max_human , y_max_human = 300 / width , 200 / height x_max_horse , y_max_horse = 400 / width , 300 / height Our image has 1 + 2 classes. The first class is the background-class. The other 2 classes correspond to each object i.e. human, horse. num_classes = 3 background_class , human_class , horse_class = 0 , 1 , 2 class_names = [ 'background' , 'human' , 'horse' ] box_data = np . array ( [[ x_min_human , y_min_human , x_max_human , y_max_human , human_class ], [ x_min_horse , y_min_horse , x_max_horse , y_max_horse , horse_class ]]) Let's create a simple visualization pipeline. For an explanation of what control-map is doing please check our tutorial at: paz/examples/tutorials/controlmap_processor.py draw_boxes = SequentialProcessor () draw_boxes . add ( pr . ControlMap ( pr . ToBoxes2D ( class_names ), [ 1 ], [ 1 ])) draw_boxes . add ( pr . ControlMap ( pr . LoadImage (), [ 0 ], [ 0 ])) draw_boxes . add ( pr . ControlMap ( pr . DenormalizeBoxes2D (), [ 0 , 1 ], [ 1 ], { 0 : 0 })) draw_boxes . add ( pr . DrawBoxes2D ( class_names )) draw_boxes . add ( pr . ShowImage ()) We can now look at our boxes! draw_boxes ( image_fullpath , box_data )","title":"Bounding boxes"},{"location":"getting-started/bounding_boxes/#bounding-boxes","text":"In this tutorial we show you how to build bounding boxes for your own training pipeline: You can find the complete script of this tutorial here Let's do our basic imports import os import numpy as np from tensorflow.keras.utils import get_file import paz.processors as pr from paz.abstract import SequentialProcessor from paz.backend.image import load_image First we will download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/object_detection_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' ) Let's first build our labels: Keep in mind that the origin of our images is located at the top-left . The x_min, y_min are the normalized coordinates of top-left bounding-box corner. height , width = load_image ( image_fullpath ) . shape [: 2 ] x_min_human , y_min_human = 200 / width , 60 / height x_min_horse , y_min_horse = 100 / width , 90 / height The x_max, y_max are the normalized coordinates of bottom-right bounding-box corner. x_max_human , y_max_human = 300 / width , 200 / height x_max_horse , y_max_horse = 400 / width , 300 / height Our image has 1 + 2 classes. The first class is the background-class. The other 2 classes correspond to each object i.e. human, horse. num_classes = 3 background_class , human_class , horse_class = 0 , 1 , 2 class_names = [ 'background' , 'human' , 'horse' ] box_data = np . array ( [[ x_min_human , y_min_human , x_max_human , y_max_human , human_class ], [ x_min_horse , y_min_horse , x_max_horse , y_max_horse , horse_class ]]) Let's create a simple visualization pipeline. For an explanation of what control-map is doing please check our tutorial at: paz/examples/tutorials/controlmap_processor.py draw_boxes = SequentialProcessor () draw_boxes . add ( pr . ControlMap ( pr . ToBoxes2D ( class_names ), [ 1 ], [ 1 ])) draw_boxes . add ( pr . ControlMap ( pr . LoadImage (), [ 0 ], [ 0 ])) draw_boxes . add ( pr . ControlMap ( pr . DenormalizeBoxes2D (), [ 0 , 1 ], [ 1 ], { 0 : 0 })) draw_boxes . add ( pr . DrawBoxes2D ( class_names )) draw_boxes . add ( pr . ShowImage ()) We can now look at our boxes! draw_boxes ( image_fullpath , box_data )","title":"Bounding boxes"},{"location":"getting-started/controlmap/","text":"Control-map In this tutorial we show you how to use one of our most useful processors ControlMap . You can find the complete script of this tutorial here import os import numpy as np from paz.abstract import SequentialProcessor from paz.backend.image import show_image , load_image import paz.processors as pr from tensorflow.keras.utils import get_file Let's download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/object_detection_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' ) The x_min, y_min are the normalized coordinates of top-left bounding-box corner. H , W = load_image ( image_fullpath ) . shape [: 2 ] The x_max, y_max are the normalized coordinates class_names = [ 'background' , 'human' , 'horse' ] box_data = np . array ([[ 200 / W , 60 / H , 300 / W , 200 / H , 1 ], [ 100 / W , 90 / H , 400 / W , 300 / H , 2 ]]) Let's visualize our boxes! First we transform our numpy array into our built-in Box2D messages to_boxes2D = pr . ToBoxes2D ( class_names ) denormalize = pr . DenormalizeBoxes2D () boxes2D = to_boxes2D ( box_data ) image = load_image ( image_fullpath ) boxes2D = denormalize ( image , boxes2D ) draw_boxes2D = pr . DrawBoxes2D ( class_names ) show_image ( draw_boxes2D ( image , boxes2D )) As you can see, we were not able to put everything as a SequentialProcessor . This is because we are dealing with 2 inputs: box_data and image . We can join them into a single processor using pr.ControlMap wrap. pr.ControlMap allows you to select which arguments ( intro_indices ) are passed to your processor, and also where you should put the output of your processor ( outro_indices ). draw_boxes = SequentialProcessor () draw_boxes . add ( pr . ControlMap ( to_boxes2D , intro_indices = [ 1 ], outro_indices = [ 1 ])) draw_boxes . add ( pr . ControlMap ( pr . LoadImage (), [ 0 ], [ 0 ])) draw_boxes . add ( pr . ControlMap ( denormalize , [ 0 , 1 ], [ 1 ], keep = { 0 : 0 })) draw_boxes . add ( pr . DrawBoxes2D ( class_names )) draw_boxes . add ( pr . ShowImage ()) Now you have everything in a single packed function that loads and draws! draw_boxes ( image_fullpath , box_data ) Also note if one of your function is eating away one input that you wish to keep in your pipeline, you can use the keep dictionary to explicitly say which of your inputs you wish to keep and where it should be located. This is represented respectively by the key and the value of the keep dictionary.","title":"Control-map"},{"location":"getting-started/controlmap/#control-map","text":"In this tutorial we show you how to use one of our most useful processors ControlMap . You can find the complete script of this tutorial here import os import numpy as np from paz.abstract import SequentialProcessor from paz.backend.image import show_image , load_image import paz.processors as pr from tensorflow.keras.utils import get_file Let's download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/object_detection_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' ) The x_min, y_min are the normalized coordinates of top-left bounding-box corner. H , W = load_image ( image_fullpath ) . shape [: 2 ] The x_max, y_max are the normalized coordinates class_names = [ 'background' , 'human' , 'horse' ] box_data = np . array ([[ 200 / W , 60 / H , 300 / W , 200 / H , 1 ], [ 100 / W , 90 / H , 400 / W , 300 / H , 2 ]]) Let's visualize our boxes! First we transform our numpy array into our built-in Box2D messages to_boxes2D = pr . ToBoxes2D ( class_names ) denormalize = pr . DenormalizeBoxes2D () boxes2D = to_boxes2D ( box_data ) image = load_image ( image_fullpath ) boxes2D = denormalize ( image , boxes2D ) draw_boxes2D = pr . DrawBoxes2D ( class_names ) show_image ( draw_boxes2D ( image , boxes2D )) As you can see, we were not able to put everything as a SequentialProcessor . This is because we are dealing with 2 inputs: box_data and image . We can join them into a single processor using pr.ControlMap wrap. pr.ControlMap allows you to select which arguments ( intro_indices ) are passed to your processor, and also where you should put the output of your processor ( outro_indices ). draw_boxes = SequentialProcessor () draw_boxes . add ( pr . ControlMap ( to_boxes2D , intro_indices = [ 1 ], outro_indices = [ 1 ])) draw_boxes . add ( pr . ControlMap ( pr . LoadImage (), [ 0 ], [ 0 ])) draw_boxes . add ( pr . ControlMap ( denormalize , [ 0 , 1 ], [ 1 ], keep = { 0 : 0 })) draw_boxes . add ( pr . DrawBoxes2D ( class_names )) draw_boxes . add ( pr . ShowImage ()) Now you have everything in a single packed function that loads and draws! draw_boxes ( image_fullpath , box_data ) Also note if one of your function is eating away one input that you wish to keep in your pipeline, you can use the keep dictionary to explicitly say which of your inputs you wish to keep and where it should be located. This is represented respectively by the key and the value of the keep dictionary.","title":"Control-map"},{"location":"getting-started/image_augmentation/","text":"Image augmentation This tutorial explains the basic functionality of SequentialProcessors for data augmentation in a classification task. You can find the complete script of this tutorial here import os from paz.abstract import SequentialProcessor from paz.backend.image import show_image , load_image import paz.processors as pr from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.utils import get_file Let's download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/image_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' ) We construct a data augmentation pipeline using the built-in PAZ processors: augment = SequentialProcessor () augment . add ( pr . RandomContrast ()) augment . add ( pr . RandomBrightness ()) augment . add ( pr . RandomSaturation ()) We can now apply our pipeline as a normal function: for _ in range ( 5 ): image = load_image ( image_fullpath ) # use it as a normal function image = augment ( image ) show_image ( image ) We can add to our sequential pipeline other function anywhere i.e. arg 0: augment . insert ( 0 , pr . LoadImage ()) for _ in range ( 5 ): # now we don't load the image every time. image = augment ( image_fullpath ) show_image ( image ) Adding new processor at the end to have a single function. augment . add ( pr . ShowImage ()) for _ in range ( 5 ): # everything compressed into a single function image = augment ( image_fullpath ) We can also pop the last processor added. augment . pop () We now create another processor for geometric augmentation. NOTE : We can instantiate a new SequentialProcessor using a list of processors transform = SequentialProcessor ([ pr . RandomRotation (), pr . RandomTranslation ()]) You should start getting now transformations similar to these ones: We can call both of our processors separately: for _ in range ( 5 ): image = transform ( augment ( image_fullpath )) show_image ( image ) But since processors are just functions we can simply add it as a processor: augment . add ( transform ) for _ in range ( 5 ): image = augment ( image_fullpath ) show_image ( image ) We can also use the Keras ImageDataGenerator : generator = ImageDataGenerator ( rotation_range = 30 , width_shift_range = 0.1 , height_shift_range = 0.1 , zoom_range = 0.1 , horizontal_flip = True ) We can add it by using our processor/wrapper ImageDataProcessor : augment = SequentialProcessor () augment . add ( pr . LoadImage ()) augment . add ( pr . ImageDataProcessor ( generator )) augment . add ( pr . ShowImage ()) for _ in range ( 5 ): image = augment ( image_fullpath )","title":"Image augmentation"},{"location":"getting-started/image_augmentation/#image-augmentation","text":"This tutorial explains the basic functionality of SequentialProcessors for data augmentation in a classification task. You can find the complete script of this tutorial here import os from paz.abstract import SequentialProcessor from paz.backend.image import show_image , load_image import paz.processors as pr from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.utils import get_file Let's download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/image_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' ) We construct a data augmentation pipeline using the built-in PAZ processors: augment = SequentialProcessor () augment . add ( pr . RandomContrast ()) augment . add ( pr . RandomBrightness ()) augment . add ( pr . RandomSaturation ()) We can now apply our pipeline as a normal function: for _ in range ( 5 ): image = load_image ( image_fullpath ) # use it as a normal function image = augment ( image ) show_image ( image ) We can add to our sequential pipeline other function anywhere i.e. arg 0: augment . insert ( 0 , pr . LoadImage ()) for _ in range ( 5 ): # now we don't load the image every time. image = augment ( image_fullpath ) show_image ( image ) Adding new processor at the end to have a single function. augment . add ( pr . ShowImage ()) for _ in range ( 5 ): # everything compressed into a single function image = augment ( image_fullpath ) We can also pop the last processor added. augment . pop () We now create another processor for geometric augmentation. NOTE : We can instantiate a new SequentialProcessor using a list of processors transform = SequentialProcessor ([ pr . RandomRotation (), pr . RandomTranslation ()]) You should start getting now transformations similar to these ones: We can call both of our processors separately: for _ in range ( 5 ): image = transform ( augment ( image_fullpath )) show_image ( image ) But since processors are just functions we can simply add it as a processor: augment . add ( transform ) for _ in range ( 5 ): image = augment ( image_fullpath ) show_image ( image ) We can also use the Keras ImageDataGenerator : generator = ImageDataGenerator ( rotation_range = 30 , width_shift_range = 0.1 , height_shift_range = 0.1 , zoom_range = 0.1 , horizontal_flip = True ) We can add it by using our processor/wrapper ImageDataProcessor : augment = SequentialProcessor () augment . add ( pr . LoadImage ()) augment . add ( pr . ImageDataProcessor ( generator )) augment . add ( pr . ShowImage ()) for _ in range ( 5 ): image = augment ( image_fullpath )","title":"Image augmentation"},{"location":"getting-started/introduction_to_processors/","text":"Introduction to processors PAZ allows us to easily create preprocessing, data-augmentation and post-processing pipelines. In the example below we show how to create a simple data-augmentation pipeline: from paz.abstract import SequentialProcessor from paz import processors as pr augment_image = SequentialProcessor () augment_image . add ( pr . RandomContrast ()) augment_image . add ( pr . RandomBrightness ()) augment_image . add ( pr . RandomSaturation ()) augment_image . add ( pr . RandomHue ()) The final pipeline ( augment_image ) behaves as a Python function: new_image = augment_image ( image ) There exists plenty default pipelines already built in PAZ. For more information please consult paz.pipelines . Pipelines are built from paz.processors . There are plenty of processors implemented in PAZ; however, one can easily build a custom processor by inheriting from paz.abstract.Processor . In the example below we show how to build a processor for normalizing an image to a range from 0 to 1. from paz.abstract import Processor class NormalizeImage ( Processor ): \"\"\"Normalize image by diving all values by 255.0. \"\"\" def __init__ ( self ): super ( NormalizeImage , self ) . __init__ () def call ( self , image ): return image / 255.0 We can now use our processor to create a pipeline for loading an image and normalizing it: from paz.abstract import SequentialProcessor from paz.processors import LoadImage preprocess_image = SequentialProcessor () preprocess_image . add ( LoadImage ()) preprocess_image . add ( NormalizeImage ()) We can now use our new function/pipeline to load and normalize an image: image = preprocess_image ( 'images/cat.jpg' ) Why the name Processor ? Originally PAZ was only meant for pre-processing pipelines that included data-augmentation, normalization, etc. However, I found out that we could use the same API for post-processing; therefore, I thought at the time that Processor would be adequate to describe the capacity of both pre-processing and post-processing. Names that I also thought could have worked were: Function , Functor but I didn't want to use those since I thought they would be more confusing. Similarly, in Keras this abstraction is interpreted as a Layer but here I don't think that abstraction is adequate. A layer of computation maybe? So after having this thoughts swirling around I decided to go with Processor and be explicit about my mental jugglery hoping that this name doesn't cause much mental overhead in the future.","title":"Intro to processors"},{"location":"getting-started/introduction_to_processors/#introduction-to-processors","text":"PAZ allows us to easily create preprocessing, data-augmentation and post-processing pipelines. In the example below we show how to create a simple data-augmentation pipeline: from paz.abstract import SequentialProcessor from paz import processors as pr augment_image = SequentialProcessor () augment_image . add ( pr . RandomContrast ()) augment_image . add ( pr . RandomBrightness ()) augment_image . add ( pr . RandomSaturation ()) augment_image . add ( pr . RandomHue ()) The final pipeline ( augment_image ) behaves as a Python function: new_image = augment_image ( image ) There exists plenty default pipelines already built in PAZ. For more information please consult paz.pipelines . Pipelines are built from paz.processors . There are plenty of processors implemented in PAZ; however, one can easily build a custom processor by inheriting from paz.abstract.Processor . In the example below we show how to build a processor for normalizing an image to a range from 0 to 1. from paz.abstract import Processor class NormalizeImage ( Processor ): \"\"\"Normalize image by diving all values by 255.0. \"\"\" def __init__ ( self ): super ( NormalizeImage , self ) . __init__ () def call ( self , image ): return image / 255.0 We can now use our processor to create a pipeline for loading an image and normalizing it: from paz.abstract import SequentialProcessor from paz.processors import LoadImage preprocess_image = SequentialProcessor () preprocess_image . add ( LoadImage ()) preprocess_image . add ( NormalizeImage ()) We can now use our new function/pipeline to load and normalize an image: image = preprocess_image ( 'images/cat.jpg' )","title":"Introduction to processors"},{"location":"getting-started/introduction_to_processors/#why-the-name-processor","text":"Originally PAZ was only meant for pre-processing pipelines that included data-augmentation, normalization, etc. However, I found out that we could use the same API for post-processing; therefore, I thought at the time that Processor would be adequate to describe the capacity of both pre-processing and post-processing. Names that I also thought could have worked were: Function , Functor but I didn't want to use those since I thought they would be more confusing. Similarly, in Keras this abstraction is interpreted as a Layer but here I don't think that abstraction is adequate. A layer of computation maybe? So after having this thoughts swirling around I decided to go with Processor and be explicit about my mental jugglery hoping that this name doesn't cause much mental overhead in the future.","title":"Why the name Processor?"},{"location":"getting-started/object_detection_pipeline/","text":"Data augmentation for object detection This tutorial explains the basic functionality of SequentialProcessors for data augmentation in an object detection task. You can find the complete script of this tutorial here This script explains the basic functionality of SequentialProcessors for data augmentation in an object-detection task. import os import numpy as np from tensorflow.keras.utils import get_file from paz.abstract import SequentialProcessor , ProcessingSequence from paz.models.detection.utils import create_prior_boxes import paz.processors as pr import paz.backend as P Let's download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/object_detection_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' ) Image augmentation and preprocessing part We can also create sequential pipelines by inheriting SequentialProcessor class AugmentImage ( SequentialProcessor ): def __init__ ( self ): super ( AugmentImage , self ) . __init__ () self . add ( pr . RandomContrast ()) self . add ( pr . RandomBrightness ()) self . add ( pr . RandomSaturation ()) self . add ( pr . RandomHue ()) class PreprocessImage ( SequentialProcessor ): def __init__ ( self , shape , mean = pr . BGR_IMAGENET_MEAN ): super ( PreprocessImage , self ) . __init__ () self . add ( pr . ResizeImage ( shape )) self . add ( pr . CastImage ( float )) if mean is None : self . add ( pr . NormalizeImage ()) else : self . add ( pr . SubtractMeanImage ( mean )) Let's see who it works: preprocess_image , augment_image = PreprocessImage (( 300 , 300 )), AugmentImage () print ( 'Image pre-processing examples' ) for _ in range ( 10 ): image = P . image . load_image ( image_fullpath ) image = preprocess_image ( augment_image ( image )) P . image . show_image ( image . astype ( 'uint8' )) Box augmentation and preprocessing part Let's first build our box labels: For a tutorial on how to build your box labels check here H , W = P . image . load_image ( image_fullpath ) . shape [: 2 ] class_names = [ 'background' , 'human' , 'horse' ] box_data = np . array ([[ 200 / W , 60 / H , 300 / W , 200 / H , 1 ], [ 100 / W , 90 / H , 400 / W , 300 / H , 2 ]]) Data augmentation for boxes class AugmentBoxes ( SequentialProcessor ): def __init__ ( self , mean = pr . BGR_IMAGENET_MEAN ): super ( AugmentBoxes , self ) . __init__ () self . add ( pr . ToImageBoxCoordinates ()) self . add ( pr . Expand ( mean = mean )) self . add ( pr . RandomSampleCrop ()) self . add ( pr . RandomFlipBoxesLeftRight ()) self . add ( pr . ToNormalizedBoxCoordinates ()) To visualize our current box augmentation we build a quick pipeline for drawing our boxes draw_boxes = SequentialProcessor ([ pr . ControlMap ( pr . ToBoxes2D ( class_names , False ), [ 1 ], [ 1 ]), pr . ControlMap ( pr . DenormalizeBoxes2D (), [ 0 , 1 ], [ 1 ], { 0 : 0 }), pr . DrawBoxes2D ( class_names ), pr . ShowImage ()]) Let's test our box data augmentation pipeline! augment_boxes = AugmentBoxes () print ( 'Box augmentation examples' ) for _ in range ( 10 ): image = P . image . load_image ( image_fullpath ) image , boxes = augment_boxes ( image , box_data . copy ()) draw_boxes ( P . image . resize_image ( image , ( 300 , 300 )), boxes ) Data preprocessing for boxes There is also some box-preprocessing that is required. Mostly we must match our boxes to a set of default (prior) boxes. Then we must encode them and expand the class label to a one-hot vector. class PreprocessBoxes ( SequentialProcessor ): def __init__ ( self , num_classes , prior_boxes , IOU , variances ): super ( PreprocessBoxes , self ) . __init__ () self . add ( pr . MatchBoxes ( prior_boxes , IOU ),) self . add ( pr . EncodeBoxes ( prior_boxes , variances )) self . add ( pr . BoxClassToOneHotVector ( num_classes )) Putting everything together in a single processor: Note that these is the same processor we have internally in PAZ: paz.pipelines.AugmentDetection . class AugmentDetection ( SequentialProcessor ): def __init__ ( self , prior_boxes , split = pr . TRAIN , num_classes = 21 , size = 300 , mean = pr . BGR_IMAGENET_MEAN , IOU = .5 , variances = [ .1 , .2 ]): super ( AugmentDetection , self ) . __init__ () # image processors self . augment_image = AugmentImage () self . augment_image . add ( pr . ConvertColorSpace ( pr . RGB2BGR )) self . preprocess_image = PreprocessImage (( size , size ), mean ) # box processors self . augment_boxes = AugmentBoxes () args = ( num_classes , prior_boxes , IOU , variances ) self . preprocess_boxes = PreprocessBoxes ( * args ) # pipeline self . add ( pr . UnpackDictionary ([ 'image' , 'boxes' ])) self . add ( pr . ControlMap ( pr . LoadImage (), [ 0 ], [ 0 ])) if split == pr . TRAIN : self . add ( pr . ControlMap ( self . augment_image , [ 0 ], [ 0 ])) self . add ( pr . ControlMap ( self . augment_boxes , [ 0 , 1 ], [ 0 , 1 ])) self . add ( pr . ControlMap ( self . preprocess_image , [ 0 ], [ 0 ])) self . add ( pr . ControlMap ( self . preprocess_boxes , [ 1 ], [ 1 ])) self . add ( pr . SequenceWrapper ( { 0 : { 'image' : [ size , size , 3 ]}}, { 1 : { 'boxes' : [ len ( prior_boxes ), 4 + num_classes ]}})) Here we just made some small modifications to our original drawing pipeline prior_boxes = create_prior_boxes () draw_boxes . processors [ 0 ] . processor . one_hot_encoded = True draw_boxes . insert ( 0 , pr . ControlMap ( pr . DecodeBoxes ( prior_boxes ), [ 1 ], [ 1 ])) draw_boxes . insert ( 2 , pr . ControlMap ( pr . FilterClassBoxes2D ( class_names [ 1 :]), [ 1 ], [ 1 ])) def deprocess_image ( image ): image = ( image + pr . BGR_IMAGENET_MEAN ) . astype ( 'uint8' ) return P . image . convert_color_space ( image , pr . BGR2RGB ) Finally we can apply our complete pipeline that preprocess and augments the images as well the bounding boxes! augmentator = AugmentDetection ( prior_boxes , num_classes = len ( class_names )) print ( 'Image and boxes augmentations' ) for _ in range ( 10 ): sample = { 'image' : image_fullpath , 'boxes' : box_data . copy ()} data = augmentator ( sample ) image , boxes = data [ 'inputs' ][ 'image' ], data [ 'labels' ][ 'boxes' ] image = deprocess_image ( image ) draw_boxes ( image , boxes ) Note that we change the input and output format from lists to a dictionaries. The input changed by adding the pr.UnpackDictionary processor, and the output changed by the pr.SequenceWrapper processor. The pr.SequenceWrapper method allows us to easily connect the complete pipeline to a Sequence Generator. Here we show you the final step such that you can wrap it in our custom generator and pass it directly to your model to do model.fit . data = [{ 'image' : image_fullpath , 'boxes' : box_data }] print ( 'Image and boxes augmentations with generator' ) batch_size = 1 sequence = ProcessingSequence ( augmentator , batch_size , data ) for _ in range ( 10 ): batch = sequence . __getitem__ ( 0 ) batch_images , batch_boxes = batch [ 0 ][ 'image' ], batch [ 1 ][ 'boxes' ] image , boxes = batch_images [ 0 ], batch_boxes [ 0 ] image = deprocess_image ( image ) draw_boxes ( image , boxes ) You should now be able to see transformations similar to these ones:","title":"Object detection pipeline"},{"location":"getting-started/object_detection_pipeline/#data-augmentation-for-object-detection","text":"This tutorial explains the basic functionality of SequentialProcessors for data augmentation in an object detection task. You can find the complete script of this tutorial here This script explains the basic functionality of SequentialProcessors for data augmentation in an object-detection task. import os import numpy as np from tensorflow.keras.utils import get_file from paz.abstract import SequentialProcessor , ProcessingSequence from paz.models.detection.utils import create_prior_boxes import paz.processors as pr import paz.backend as P Let's download a test image and put it inside our PAZ directory IMAGE_URL = ( 'https://github.com/oarriaga/altamira-data/releases/download' '/v0.9/object_detection_augmentation.png' ) image_filename = os . path . basename ( IMAGE_URL ) image_fullpath = get_file ( image_filename , IMAGE_URL , cache_subdir = 'paz/data' )","title":"Data augmentation for object detection"},{"location":"getting-started/object_detection_pipeline/#image-augmentation-and-preprocessing-part","text":"We can also create sequential pipelines by inheriting SequentialProcessor class AugmentImage ( SequentialProcessor ): def __init__ ( self ): super ( AugmentImage , self ) . __init__ () self . add ( pr . RandomContrast ()) self . add ( pr . RandomBrightness ()) self . add ( pr . RandomSaturation ()) self . add ( pr . RandomHue ()) class PreprocessImage ( SequentialProcessor ): def __init__ ( self , shape , mean = pr . BGR_IMAGENET_MEAN ): super ( PreprocessImage , self ) . __init__ () self . add ( pr . ResizeImage ( shape )) self . add ( pr . CastImage ( float )) if mean is None : self . add ( pr . NormalizeImage ()) else : self . add ( pr . SubtractMeanImage ( mean )) Let's see who it works: preprocess_image , augment_image = PreprocessImage (( 300 , 300 )), AugmentImage () print ( 'Image pre-processing examples' ) for _ in range ( 10 ): image = P . image . load_image ( image_fullpath ) image = preprocess_image ( augment_image ( image )) P . image . show_image ( image . astype ( 'uint8' ))","title":"Image augmentation and preprocessing part"},{"location":"getting-started/object_detection_pipeline/#box-augmentation-and-preprocessing-part","text":"Let's first build our box labels: For a tutorial on how to build your box labels check here H , W = P . image . load_image ( image_fullpath ) . shape [: 2 ] class_names = [ 'background' , 'human' , 'horse' ] box_data = np . array ([[ 200 / W , 60 / H , 300 / W , 200 / H , 1 ], [ 100 / W , 90 / H , 400 / W , 300 / H , 2 ]])","title":"Box augmentation and preprocessing part"},{"location":"getting-started/object_detection_pipeline/#data-augmentation-for-boxes","text":"class AugmentBoxes ( SequentialProcessor ): def __init__ ( self , mean = pr . BGR_IMAGENET_MEAN ): super ( AugmentBoxes , self ) . __init__ () self . add ( pr . ToImageBoxCoordinates ()) self . add ( pr . Expand ( mean = mean )) self . add ( pr . RandomSampleCrop ()) self . add ( pr . RandomFlipBoxesLeftRight ()) self . add ( pr . ToNormalizedBoxCoordinates ()) To visualize our current box augmentation we build a quick pipeline for drawing our boxes draw_boxes = SequentialProcessor ([ pr . ControlMap ( pr . ToBoxes2D ( class_names , False ), [ 1 ], [ 1 ]), pr . ControlMap ( pr . DenormalizeBoxes2D (), [ 0 , 1 ], [ 1 ], { 0 : 0 }), pr . DrawBoxes2D ( class_names ), pr . ShowImage ()]) Let's test our box data augmentation pipeline! augment_boxes = AugmentBoxes () print ( 'Box augmentation examples' ) for _ in range ( 10 ): image = P . image . load_image ( image_fullpath ) image , boxes = augment_boxes ( image , box_data . copy ()) draw_boxes ( P . image . resize_image ( image , ( 300 , 300 )), boxes )","title":"Data augmentation for boxes"},{"location":"getting-started/object_detection_pipeline/#data-preprocessing-for-boxes","text":"There is also some box-preprocessing that is required. Mostly we must match our boxes to a set of default (prior) boxes. Then we must encode them and expand the class label to a one-hot vector. class PreprocessBoxes ( SequentialProcessor ): def __init__ ( self , num_classes , prior_boxes , IOU , variances ): super ( PreprocessBoxes , self ) . __init__ () self . add ( pr . MatchBoxes ( prior_boxes , IOU ),) self . add ( pr . EncodeBoxes ( prior_boxes , variances )) self . add ( pr . BoxClassToOneHotVector ( num_classes )) Putting everything together in a single processor: Note that these is the same processor we have internally in PAZ: paz.pipelines.AugmentDetection . class AugmentDetection ( SequentialProcessor ): def __init__ ( self , prior_boxes , split = pr . TRAIN , num_classes = 21 , size = 300 , mean = pr . BGR_IMAGENET_MEAN , IOU = .5 , variances = [ .1 , .2 ]): super ( AugmentDetection , self ) . __init__ () # image processors self . augment_image = AugmentImage () self . augment_image . add ( pr . ConvertColorSpace ( pr . RGB2BGR )) self . preprocess_image = PreprocessImage (( size , size ), mean ) # box processors self . augment_boxes = AugmentBoxes () args = ( num_classes , prior_boxes , IOU , variances ) self . preprocess_boxes = PreprocessBoxes ( * args ) # pipeline self . add ( pr . UnpackDictionary ([ 'image' , 'boxes' ])) self . add ( pr . ControlMap ( pr . LoadImage (), [ 0 ], [ 0 ])) if split == pr . TRAIN : self . add ( pr . ControlMap ( self . augment_image , [ 0 ], [ 0 ])) self . add ( pr . ControlMap ( self . augment_boxes , [ 0 , 1 ], [ 0 , 1 ])) self . add ( pr . ControlMap ( self . preprocess_image , [ 0 ], [ 0 ])) self . add ( pr . ControlMap ( self . preprocess_boxes , [ 1 ], [ 1 ])) self . add ( pr . SequenceWrapper ( { 0 : { 'image' : [ size , size , 3 ]}}, { 1 : { 'boxes' : [ len ( prior_boxes ), 4 + num_classes ]}})) Here we just made some small modifications to our original drawing pipeline prior_boxes = create_prior_boxes () draw_boxes . processors [ 0 ] . processor . one_hot_encoded = True draw_boxes . insert ( 0 , pr . ControlMap ( pr . DecodeBoxes ( prior_boxes ), [ 1 ], [ 1 ])) draw_boxes . insert ( 2 , pr . ControlMap ( pr . FilterClassBoxes2D ( class_names [ 1 :]), [ 1 ], [ 1 ])) def deprocess_image ( image ): image = ( image + pr . BGR_IMAGENET_MEAN ) . astype ( 'uint8' ) return P . image . convert_color_space ( image , pr . BGR2RGB ) Finally we can apply our complete pipeline that preprocess and augments the images as well the bounding boxes! augmentator = AugmentDetection ( prior_boxes , num_classes = len ( class_names )) print ( 'Image and boxes augmentations' ) for _ in range ( 10 ): sample = { 'image' : image_fullpath , 'boxes' : box_data . copy ()} data = augmentator ( sample ) image , boxes = data [ 'inputs' ][ 'image' ], data [ 'labels' ][ 'boxes' ] image = deprocess_image ( image ) draw_boxes ( image , boxes ) Note that we change the input and output format from lists to a dictionaries. The input changed by adding the pr.UnpackDictionary processor, and the output changed by the pr.SequenceWrapper processor. The pr.SequenceWrapper method allows us to easily connect the complete pipeline to a Sequence Generator. Here we show you the final step such that you can wrap it in our custom generator and pass it directly to your model to do model.fit . data = [{ 'image' : image_fullpath , 'boxes' : box_data }] print ( 'Image and boxes augmentations with generator' ) batch_size = 1 sequence = ProcessingSequence ( augmentator , batch_size , data ) for _ in range ( 10 ): batch = sequence . __getitem__ ( 0 ) batch_images , batch_boxes = batch [ 0 ][ 'image' ], batch [ 1 ][ 'boxes' ] image , boxes = batch_images [ 0 ], batch_boxes [ 0 ] image = deprocess_image ( image ) draw_boxes ( image , boxes ) You should now be able to see transformations similar to these ones:","title":"Data preprocessing for boxes"},{"location":"models/classification/","text":"Models for object classification [source] MiniXception paz . models . classification . xception . MiniXception ( input_shape , num_classes , weights = None ) Build MiniXception (see references). Arguments input_shape : List of three integers e.g. [H, W, 3] num_classes : Int. weights : None or string with pre-trained dataset. Valid datasets include only FER . Returns Tensorflow-Keras model. References Real-time Convolutional Neural Networks for Emotion and Gender Classification","title":"Classification"},{"location":"models/classification/#minixception","text":"paz . models . classification . xception . MiniXception ( input_shape , num_classes , weights = None ) Build MiniXception (see references). Arguments input_shape : List of three integers e.g. [H, W, 3] num_classes : Int. weights : None or string with pre-trained dataset. Valid datasets include only FER . Returns Tensorflow-Keras model. References Real-time Convolutional Neural Networks for Emotion and Gender Classification","title":"MiniXception"},{"location":"models/detection/","text":"Models for 2D object detection [source] SSD300 paz . models . detection . ssd300 . SSD300 ( num_classes = 21 , base_weights = 'VOC' , head_weights = 'VOC' , input_shape = ( 300 , 300 , 3 ), num_priors = [ 4 , 6 , 6 , 6 , 4 , 4 ], l2_loss = 0.0005 , return_base = False , trainable_base = True ) Single-shot-multibox detector for 300x300x3 BGR input images. Arguments num_classes : Integer. Specifies the number of class labels. base_weights : String or None. If string should be a valid dataset name. Current valid datasets include VOC FAT and VGG . head_weights : String or None. If string should be a valid dataset name. Current valid datasets include VOC and FAT . input_shape : List of integers. Input shape to the model including only spatial and channel resolution e.g. (300, 300, 3). num_priors : List of integers. Number of default box shapes used in each detection layer. l2_loss : Float. l2 regularization loss for convolutional layers. return_base : Boolean. If True the model returned is just the original base. trainable_base : Boolean. If True the base model weights are also trained. Reference SSD: Single Shot MultiBox Detector [source] SSD512 paz . models . detection . ssd512 . SSD512 ( num_classes = 81 , weights = 'COCO' , input_shape = ( 512 , 512 , 3 ), num_priors = [ 4 , 6 , 6 , 6 , 6 , 4 , 4 ], l2_loss = 0.0005 , return_base = False , trainable_base = True ) Single-shot-multibox detector for 512x512x3 BGR input images. Arguments num_classes : Integer. Specifies the number of class labels. weights : String or None. If string should be a valid dataset name. Current valid datasets include COCO and YCBVideo . input_shape : List of integers. Input shape to the model including only spatial and channel resolution e.g. (512, 512, 3). num_priors : List of integers. Number of default box shapes used in each detection layer. l2_loss : Float. l2 regularization loss for convolutional layers. return_base : Boolean. If True the model returned is just the original base. trainable_base : Boolean. If True the base model weights are also trained. Reference SSD: Single Shot MultiBox Detector [source] HaarCascadeDetector paz . models . detection . haar_cascade . HaarCascadeDetector ( self , weights = 'frontalface_default' , class_arg = None , scale = 1.3 , neighbors = 5 ) Haar cascade face detector. Arguments path : String. Postfix to default openCV haarcascades XML files, see [1] e.g. eye , frontalface_alt2 , fullbody class_arg : Int. Class label argument. scale = Float. Scale for image reduction neighbors : Int. Minimum neighbors Reference Haar Cascades","title":"Detection"},{"location":"models/detection/#ssd300","text":"paz . models . detection . ssd300 . SSD300 ( num_classes = 21 , base_weights = 'VOC' , head_weights = 'VOC' , input_shape = ( 300 , 300 , 3 ), num_priors = [ 4 , 6 , 6 , 6 , 4 , 4 ], l2_loss = 0.0005 , return_base = False , trainable_base = True ) Single-shot-multibox detector for 300x300x3 BGR input images. Arguments num_classes : Integer. Specifies the number of class labels. base_weights : String or None. If string should be a valid dataset name. Current valid datasets include VOC FAT and VGG . head_weights : String or None. If string should be a valid dataset name. Current valid datasets include VOC and FAT . input_shape : List of integers. Input shape to the model including only spatial and channel resolution e.g. (300, 300, 3). num_priors : List of integers. Number of default box shapes used in each detection layer. l2_loss : Float. l2 regularization loss for convolutional layers. return_base : Boolean. If True the model returned is just the original base. trainable_base : Boolean. If True the base model weights are also trained. Reference SSD: Single Shot MultiBox Detector [source]","title":"SSD300"},{"location":"models/detection/#ssd512","text":"paz . models . detection . ssd512 . SSD512 ( num_classes = 81 , weights = 'COCO' , input_shape = ( 512 , 512 , 3 ), num_priors = [ 4 , 6 , 6 , 6 , 6 , 4 , 4 ], l2_loss = 0.0005 , return_base = False , trainable_base = True ) Single-shot-multibox detector for 512x512x3 BGR input images. Arguments num_classes : Integer. Specifies the number of class labels. weights : String or None. If string should be a valid dataset name. Current valid datasets include COCO and YCBVideo . input_shape : List of integers. Input shape to the model including only spatial and channel resolution e.g. (512, 512, 3). num_priors : List of integers. Number of default box shapes used in each detection layer. l2_loss : Float. l2 regularization loss for convolutional layers. return_base : Boolean. If True the model returned is just the original base. trainable_base : Boolean. If True the base model weights are also trained. Reference SSD: Single Shot MultiBox Detector [source]","title":"SSD512"},{"location":"models/detection/#haarcascadedetector","text":"paz . models . detection . haar_cascade . HaarCascadeDetector ( self , weights = 'frontalface_default' , class_arg = None , scale = 1.3 , neighbors = 5 ) Haar cascade face detector. Arguments path : String. Postfix to default openCV haarcascades XML files, see [1] e.g. eye , frontalface_alt2 , fullbody class_arg : Int. Class label argument. scale = Float. Scale for image reduction neighbors : Int. Minimum neighbors Reference Haar Cascades","title":"HaarCascadeDetector"},{"location":"models/keypoint/","text":"Models for 2D keypoint estimation [source] KeypointNet paz . models . keypoint . keypointnet . KeypointNet ( input_shape , num_keypoints , depth = 0.2 , filters = 64 , alpha = 0.1 ) Keypointnet model for discovering keypoint locations in 3D space Arguments input_shape : List of integers indicating [H, W, num_channels) . num_keypoints : Int. Number of keypoints to discover. depth : Float. Prior depth (centimeters) of keypoints. filters : Int. Number of filters used in convolutional layers. alpha : Float. Alpha parameter of leaky relu. Returns Keras/tensorflow model References Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning [source] KeypointNet2D paz . models . keypoint . keypointnet . KeypointNet2D ( input_shape , num_keypoints , filters = 64 , alpha = 0.1 ) Model for discovering keypoint locations in 2D space, modified from Arguments input_shape : List of integers indicating [H, W, num_channels] . num_keypoints : Int. Number of keypoints to discover. filters : Int. Number of filters used in convolutional layers. alpha : Float. Alpha parameter of leaky relu. Returns Keras/tensorflow model References Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning [source] Projector paz . models . keypoint . projector . Projector ( self , focal_length , use_numpy = False ) Projects keypoints from image coordinates to 3D space and viceversa. This model uses the camera focal length and the depth estimation of a point to project it to image coordinates. It works with numpy matrices or tensorflow values. See use_numpy . Arguments focal_length : Float. Focal length of camera used to generate keypoints. use_numpy : Boolean. If True both unproject and project functions take numpy arrays as inputs. If False takes tf.tensors as inputs.","title":"Keypoints"},{"location":"models/keypoint/#keypointnet","text":"paz . models . keypoint . keypointnet . KeypointNet ( input_shape , num_keypoints , depth = 0.2 , filters = 64 , alpha = 0.1 ) Keypointnet model for discovering keypoint locations in 3D space Arguments input_shape : List of integers indicating [H, W, num_channels) . num_keypoints : Int. Number of keypoints to discover. depth : Float. Prior depth (centimeters) of keypoints. filters : Int. Number of filters used in convolutional layers. alpha : Float. Alpha parameter of leaky relu. Returns Keras/tensorflow model References Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning [source]","title":"KeypointNet"},{"location":"models/keypoint/#keypointnet2d","text":"paz . models . keypoint . keypointnet . KeypointNet2D ( input_shape , num_keypoints , filters = 64 , alpha = 0.1 ) Model for discovering keypoint locations in 2D space, modified from Arguments input_shape : List of integers indicating [H, W, num_channels] . num_keypoints : Int. Number of keypoints to discover. filters : Int. Number of filters used in convolutional layers. alpha : Float. Alpha parameter of leaky relu. Returns Keras/tensorflow model References Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning [source]","title":"KeypointNet2D"},{"location":"models/keypoint/#projector","text":"paz . models . keypoint . projector . Projector ( self , focal_length , use_numpy = False ) Projects keypoints from image coordinates to 3D space and viceversa. This model uses the camera focal length and the depth estimation of a point to project it to image coordinates. It works with numpy matrices or tensorflow values. See use_numpy . Arguments focal_length : Float. Focal length of camera used to generate keypoints. use_numpy : Boolean. If True both unproject and project functions take numpy arrays as inputs. If False takes tf.tensors as inputs.","title":"Projector"},{"location":"models/layers/","text":"Custom layers used in our models [source] Conv2DNormalization paz . models . layers . Conv2DNormalization ( scale , axis = 3 ) Normalization layer as described in ParseNet paper. Arguments scale : Float determining how much to scale the features. axis : Integer specifying axis of image channels. Returns Feature map tensor normalized with an L2 norm and then scaled. References ParseNet: Looking Wider to See Better [source] SubtractScalar paz . models . layers . SubtractScalar ( constant ) Subtracts scalar value to tensor. Arguments constant : Float. Value to be subtracted to all tensor values. [source] ExpectedValue2D paz . models . layers . ExpectedValue2D ( axes = [ 2 , 3 ]) Calculates the expected value along ''axes''. Arguments axes : List of integers. Axes for which the expected value will be calculated. [source] ExpectedDepth paz . models . layers . ExpectedDepth ( axes = [ 2 , 3 ]) Calculates the expected depth along ''axes''. This layer takes two inputs. First input is a depth estimation tensor. Second input is a probability map of the keypoints. It multiplies both values and calculates the expected depth. Arguments axes : List of integers. Axes for which the expected value will be calculated.","title":"Layers"},{"location":"models/layers/#conv2dnormalization","text":"paz . models . layers . Conv2DNormalization ( scale , axis = 3 ) Normalization layer as described in ParseNet paper. Arguments scale : Float determining how much to scale the features. axis : Integer specifying axis of image channels. Returns Feature map tensor normalized with an L2 norm and then scaled. References ParseNet: Looking Wider to See Better [source]","title":"Conv2DNormalization"},{"location":"models/layers/#subtractscalar","text":"paz . models . layers . SubtractScalar ( constant ) Subtracts scalar value to tensor. Arguments constant : Float. Value to be subtracted to all tensor values. [source]","title":"SubtractScalar"},{"location":"models/layers/#expectedvalue2d","text":"paz . models . layers . ExpectedValue2D ( axes = [ 2 , 3 ]) Calculates the expected value along ''axes''. Arguments axes : List of integers. Axes for which the expected value will be calculated. [source]","title":"ExpectedValue2D"},{"location":"models/layers/#expecteddepth","text":"paz . models . layers . ExpectedDepth ( axes = [ 2 , 3 ]) Calculates the expected depth along ''axes''. This layer takes two inputs. First input is a depth estimation tensor. Second input is a probability map of the keypoints. It multiplies both values and calculates the expected depth. Arguments axes : List of integers. Axes for which the expected value will be calculated.","title":"ExpectedDepth"},{"location":"models/pose_estimation/","text":"[source] HigherHRNet paz . models . pose_estimation . higher_hrnet . HigherHRNet ( weights = 'COCO' , input_shape = ( None , None , 3 ), num_keypoints = 17 , with_AE_loss = [ True , False ]) Human pose estimation detector for any input size of images. Arguments weights : String or None. If string should be a valid dataset name. Current valid datasets include COCO . input_shape : List of integers. Input shape to the model including only spatial and channel resolution e.g. (512, 512, 3). num_keypoints : Int. Number of joints. with_AE_loss : List of boolean. Reference HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation","title":"Pose estimation"},{"location":"models/pose_estimation/#higherhrnet","text":"paz . models . pose_estimation . higher_hrnet . HigherHRNet ( weights = 'COCO' , input_shape = ( None , None , 3 ), num_keypoints = 17 , with_AE_loss = [ True , False ]) Human pose estimation detector for any input size of images. Arguments weights : String or None. If string should be a valid dataset name. Current valid datasets include COCO . input_shape : List of integers. Input shape to the model including only spatial and channel resolution e.g. (512, 512, 3). num_keypoints : Int. Number of joints. with_AE_loss : List of boolean. Reference HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation","title":"HigherHRNet"},{"location":"models/segmentation/","text":"[source] UNET_VGG16 paz . models . segmentation . unet . UNET_VGG16 ( num_classes = 1 , input_shape = ( 224 , 224 , 3 ), weights = 'imagenet' , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decode_filters = [ 256 , 128 , 64 , 32 , 16 ]) Build a UNET model with a VGG16 backbone. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET-VGG16 Keras/tensorflow model. [source] UNET_VGG19 paz . models . segmentation . unet . UNET_VGG19 ( num_classes = 1 , input_shape = ( 224 , 224 , 3 ), weights = 'imagenet' , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decode_filters = [ 256 , 128 , 64 , 32 , 16 ]) Build a UNET model with a VGG19 backbone. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET-VGG19 Keras/tensorflow model. [source] UNET_RESNET50 paz . models . segmentation . unet . UNET_RESNET50 ( num_classes = 1 , input_shape = ( 224 , 224 , 3 ), weights = 'imagenet' , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decode_filters = [ 256 , 128 , 64 , 32 , 16 ]) Build a UNET model with a RESNET50V2 backbone. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET-RESNET50V2 Keras/tensorflow model. [source] UNET paz . models . segmentation . unet . UNET ( input_shape , num_classes , branch_names , BACKBONE , weights , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decoder_filters = [ 256 , 128 , 64 , 32 , 16 ], input_tensor = None , name = 'UNET' ) Build a generic UNET model with a given BACKBONE class. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET Keras/tensorflow model.","title":"Segmentation"},{"location":"models/segmentation/#unet_vgg16","text":"paz . models . segmentation . unet . UNET_VGG16 ( num_classes = 1 , input_shape = ( 224 , 224 , 3 ), weights = 'imagenet' , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decode_filters = [ 256 , 128 , 64 , 32 , 16 ]) Build a UNET model with a VGG16 backbone. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET-VGG16 Keras/tensorflow model. [source]","title":"UNET_VGG16"},{"location":"models/segmentation/#unet_vgg19","text":"paz . models . segmentation . unet . UNET_VGG19 ( num_classes = 1 , input_shape = ( 224 , 224 , 3 ), weights = 'imagenet' , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decode_filters = [ 256 , 128 , 64 , 32 , 16 ]) Build a UNET model with a VGG19 backbone. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET-VGG19 Keras/tensorflow model. [source]","title":"UNET_VGG19"},{"location":"models/segmentation/#unet_resnet50","text":"paz . models . segmentation . unet . UNET_RESNET50 ( num_classes = 1 , input_shape = ( 224 , 224 , 3 ), weights = 'imagenet' , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decode_filters = [ 256 , 128 , 64 , 32 , 16 ]) Build a UNET model with a RESNET50V2 backbone. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET-RESNET50V2 Keras/tensorflow model. [source]","title":"UNET_RESNET50"},{"location":"models/segmentation/#unet","text":"paz . models . segmentation . unet . UNET ( input_shape , num_classes , branch_names , BACKBONE , weights , freeze_backbone = False , activation = 'sigmoid' , decoder_type = 'upsample' , decoder_filters = [ 256 , 128 , 64 , 32 , 16 ], input_tensor = None , name = 'UNET' ) Build a generic UNET model with a given BACKBONE class. Arguments input_shape : List of integers: (H, W, num_channels) . num_classes : Integer used for output number of channels. branch_names : List of strings containing layer names of BACKBONE() . BACKBONE : Class for instantiating a backbone model weights : String indicating backbone weights e.g. ''imagenet'', None . freeze_backbone : Boolean. If True BACKBONE() updates are frozen. decoder_type : String indicating decoding function e.g. ''upsample ''transpose''. decoder_filters : List of integers used in each application of decoder. activation : Output activation of the model. input_tensor : Input tensor. If given shape is overwritten and this tensor is used instead as input. name : String. indicating the name of the model. Returns A UNET Keras/tensorflow model.","title":"UNET"},{"location":"optimization/callbacks/","text":"[source] DrawInferences paz . optimization . callbacks . DrawInferences ( save_path , images , pipeline , topic = 'image' , verbose = 1 ) Saves an image with its corresponding inferences Arguments save_path : String. Path in which the images will be saved. images : List of numpy arrays of shape. pipeline : Function that takes as input an element of ''images'' and outputs a ''Dict'' with inferences. topic : Key to the ''inferences'' dictionary containing as value the drawn inferences. verbose : Integer. If is bigger than 1 messages would be displayed. [source] LearningRateScheduler paz . optimization . callbacks . LearningRateScheduler ( learning_rate , gamma_decay , scheduled_epochs , verbose = 1 ) Callback for reducing learning rate at specific epochs. Arguments learning_rate : float. Indicates the starting learning rate. gamma_decay : float. In an scheduled epoch the learning rate is multiplied by this factor. scheduled_epochs : List of integers. Indicates in which epochs the learning rate will be multiplied by the gamma decay factor. verbose : Integer. If is bigger than 1 messages would be displayed. [source] EvaluateMAP paz . optimization . callbacks . EvaluateMAP ( data_manager , detector , period , save_path , iou_thresh = 0.5 ) Evaluates mean average precision (MAP) of an object detector. Arguments data_manager : Data manager and loader class. See ''paz.datasets'' for examples. detector : Tensorflow-Keras model. period : Int. Indicates how often the evaluation is performed. save_path : Str. iou_thresh : Float.","title":"Callbacks"},{"location":"optimization/callbacks/#drawinferences","text":"paz . optimization . callbacks . DrawInferences ( save_path , images , pipeline , topic = 'image' , verbose = 1 ) Saves an image with its corresponding inferences Arguments save_path : String. Path in which the images will be saved. images : List of numpy arrays of shape. pipeline : Function that takes as input an element of ''images'' and outputs a ''Dict'' with inferences. topic : Key to the ''inferences'' dictionary containing as value the drawn inferences. verbose : Integer. If is bigger than 1 messages would be displayed. [source]","title":"DrawInferences"},{"location":"optimization/callbacks/#learningratescheduler","text":"paz . optimization . callbacks . LearningRateScheduler ( learning_rate , gamma_decay , scheduled_epochs , verbose = 1 ) Callback for reducing learning rate at specific epochs. Arguments learning_rate : float. Indicates the starting learning rate. gamma_decay : float. In an scheduled epoch the learning rate is multiplied by this factor. scheduled_epochs : List of integers. Indicates in which epochs the learning rate will be multiplied by the gamma decay factor. verbose : Integer. If is bigger than 1 messages would be displayed. [source]","title":"LearningRateScheduler"},{"location":"optimization/callbacks/#evaluatemap","text":"paz . optimization . callbacks . EvaluateMAP ( data_manager , detector , period , save_path , iou_thresh = 0.5 ) Evaluates mean average precision (MAP) of an object detector. Arguments data_manager : Data manager and loader class. See ''paz.datasets'' for examples. detector : Tensorflow-Keras model. period : Int. Indicates how often the evaluation is performed. save_path : Str. iou_thresh : Float.","title":"EvaluateMAP"},{"location":"optimization/losses/","text":"[source] MultiBoxLoss paz . optimization . losses . multi_box_loss . MultiBoxLoss ( neg_pos_ratio = 3 , alpha = 1.0 , max_num_negatives = 300 ) Multi-box loss for a single-shot detection architecture. Arguments neg_pos_ratio : Int. Number of negatives used per positive box. alpha : Float. Weight parameter for localization loss. max_num_negatives : Int. Maximum number of negatives per batch. References SSD: Single Shot MultiBox Detector [source] KeypointNetLoss paz . optimization . losses . keypointnet_loss . KeypointNetLoss ( num_keypoints , focal_length , rotation_noise = 0.1 , separation_delta = 0.05 , loss_weights = { 'consistency' : 1.0 , 'silhouette' : 1.0 , 'separation' : 1.0 , 'relative_pose' : 0.2 , 'variance' : 0.5 }) KeypointNet loss for discovering latent keypoints. Arguments num_keypints : Int. Number of keypoints to discover. focal_length : Float. Focal length of camera rotation_noise : Float. Noise added to the estimation of the rotation. separation_delta : Float. Delta used for the ''separation'' loss. loss_weights : Dict. having as keys strings with the different losses names e.g. ''consistency'' and as value the weight used for that loss. References Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning [source] DiceLoss paz . optimization . losses . segmentation . dice_loss . DiceLoss ( beta = 1.0 , class_weights = 1.0 ) Computes the F beta loss. The F beta score is the geometric mean of the precision and recall, where the recall is B times more important than the precision. Arguments beta : Float. class_weights : Float or list of floats of shape (num_classes) . [source] FocalLoss paz . optimization . losses . segmentation . focal_loss . FocalLoss ( gamma = 2.0 , alpha = 0.25 ) Computes the Focal loss. The Focal loss down weights properly classified examples. Arguments gamma : Float. alpha : Float. class_weights : Float or list of floats of shape (num_classes) . [source] JaccardLoss paz . optimization . losses . segmentation . jaccard_loss . JaccardLoss ( class_weights = 1.0 ) Computes the Jaccard loss. The Jaccard score is the intersection over union of the predicted with respect to real masks. Arguments class_weights : Float or list of floats of shape (num_classes) . [source] WeightedReconstruction paz . optimization . losses . segmentation . weighted_reconstruction . WeightedReconstruction ( beta = 3.0 ) Computes L1 reconstruction loss by multiplying positive alpha mask by beta. Arguments beta : Float. Value used to multiple positive alpha mask values. RGBA_true : Tensor [batch, H, W, 4]. Color with alpha mask label values. RGB_pred : Tensor [batch, H, W, 3]. Predicted RGB values. Returns Tensor [batch, H, W] with weighted reconstruction loss values. [source] WeightedReconstructionWithError paz . optimization . losses . segmentation . weighted_reconstruction . WeightedReconstructionWithError ( beta = 3.0 ) Computes L1 reconstruction loss by multiplying positive alpha mask by beta. Arguments RGBA_true : Tensor [batch, H, W, 4]. Color with alpha mask label values. RGBE_pred : Tensor [batch, H, W, 4]. Predicted RGB and error mask. beta : Float. Value used to multiple positive alpha mask values. Returns Tensor [batch, H, W] with weighted reconstruction loss values.","title":"Losses"},{"location":"optimization/losses/#multiboxloss","text":"paz . optimization . losses . multi_box_loss . MultiBoxLoss ( neg_pos_ratio = 3 , alpha = 1.0 , max_num_negatives = 300 ) Multi-box loss for a single-shot detection architecture. Arguments neg_pos_ratio : Int. Number of negatives used per positive box. alpha : Float. Weight parameter for localization loss. max_num_negatives : Int. Maximum number of negatives per batch. References SSD: Single Shot MultiBox Detector [source]","title":"MultiBoxLoss"},{"location":"optimization/losses/#keypointnetloss","text":"paz . optimization . losses . keypointnet_loss . KeypointNetLoss ( num_keypoints , focal_length , rotation_noise = 0.1 , separation_delta = 0.05 , loss_weights = { 'consistency' : 1.0 , 'silhouette' : 1.0 , 'separation' : 1.0 , 'relative_pose' : 0.2 , 'variance' : 0.5 }) KeypointNet loss for discovering latent keypoints. Arguments num_keypints : Int. Number of keypoints to discover. focal_length : Float. Focal length of camera rotation_noise : Float. Noise added to the estimation of the rotation. separation_delta : Float. Delta used for the ''separation'' loss. loss_weights : Dict. having as keys strings with the different losses names e.g. ''consistency'' and as value the weight used for that loss. References Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning [source]","title":"KeypointNetLoss"},{"location":"optimization/losses/#diceloss","text":"paz . optimization . losses . segmentation . dice_loss . DiceLoss ( beta = 1.0 , class_weights = 1.0 ) Computes the F beta loss. The F beta score is the geometric mean of the precision and recall, where the recall is B times more important than the precision. Arguments beta : Float. class_weights : Float or list of floats of shape (num_classes) . [source]","title":"DiceLoss"},{"location":"optimization/losses/#focalloss","text":"paz . optimization . losses . segmentation . focal_loss . FocalLoss ( gamma = 2.0 , alpha = 0.25 ) Computes the Focal loss. The Focal loss down weights properly classified examples. Arguments gamma : Float. alpha : Float. class_weights : Float or list of floats of shape (num_classes) . [source]","title":"FocalLoss"},{"location":"optimization/losses/#jaccardloss","text":"paz . optimization . losses . segmentation . jaccard_loss . JaccardLoss ( class_weights = 1.0 ) Computes the Jaccard loss. The Jaccard score is the intersection over union of the predicted with respect to real masks. Arguments class_weights : Float or list of floats of shape (num_classes) . [source]","title":"JaccardLoss"},{"location":"optimization/losses/#weightedreconstruction","text":"paz . optimization . losses . segmentation . weighted_reconstruction . WeightedReconstruction ( beta = 3.0 ) Computes L1 reconstruction loss by multiplying positive alpha mask by beta. Arguments beta : Float. Value used to multiple positive alpha mask values. RGBA_true : Tensor [batch, H, W, 4]. Color with alpha mask label values. RGB_pred : Tensor [batch, H, W, 3]. Predicted RGB values. Returns Tensor [batch, H, W] with weighted reconstruction loss values. [source]","title":"WeightedReconstruction"},{"location":"optimization/losses/#weightedreconstructionwitherror","text":"paz . optimization . losses . segmentation . weighted_reconstruction . WeightedReconstructionWithError ( beta = 3.0 ) Computes L1 reconstruction loss by multiplying positive alpha mask by beta. Arguments RGBA_true : Tensor [batch, H, W, 4]. Color with alpha mask label values. RGBE_pred : Tensor [batch, H, W, 4]. Predicted RGB and error mask. beta : Float. Value used to multiple positive alpha mask values. Returns Tensor [batch, H, W] with weighted reconstruction loss values.","title":"WeightedReconstructionWithError"},{"location":"pipelines/applications/","text":"Out-of-the-box high-level pipelines for inference. All of these pipelines can be imported too from: paz.pipelines [source] SSD512COCO paz . pipelines . detection . SSD512COCO ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD512 trained on COCO. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD512COCO detect = SSD512COCO () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . Reference SSD: Single Shot MultiBox Detector [source] SSD300VOC paz . pipelines . detection . SSD300VOC ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD300 trained on VOC. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD300VOC detect = SSD300VOC () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . Reference SSD: Single Shot MultiBox Detector [source] SSD512YCBVideo paz . pipelines . detection . SSD512YCBVideo ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD512 trained on YCBVideo. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD512YCBVideo detect = SSD512YCBVideo () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . [source] SSD300FAT paz . pipelines . detection . SSD300FAT ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD300 trained on FAT. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD300FAT detect = SSD300FAT () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . [source] DetectMiniXceptionFER paz . pipelines . detection . DetectMiniXceptionFER ( offsets = [ 0 , 0 ], colors = [[ 255 , 0 , 0 ], [ 45 , 90 , 45 ], [ 255 , 0 , 255 ], [ 255 , 255 , 0 ], [ 0 , 0 , 255 ], [ 0 , 255 , 255 ], [ 0 , 255 , 0 ]]) Emotion classification and detection pipeline. Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . Example from paz.pipelines import DetectMiniXceptionFER detect = DetectMiniXceptionFER () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . References Real-time Convolutional Neural Networks for Emotion and Gender Classification [source] MiniXceptionFER paz . pipelines . classification . MiniXceptionFER () Mini Xception pipeline for classifying emotions from RGB faces. Example from paz.pipelines import MiniXceptionFER classify = MiniXceptionFER () # apply directly to an image (numpy-array) inference = classify ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : class_names and scores . References Real-time Convolutional Neural Networks for Emotion and Gender Classification [source] FaceKeypointNet2D32 paz . pipelines . keypoints . FaceKeypointNet2D32 ( draw = True , radius = 3 ) KeypointNet2D model trained with Kaggle Facial Detection challenge. Arguments draw : Boolean indicating if inferences should be drawn. radius : Int. used for drawing the predicted keypoints. Example from paz.pipelines import FaceKeypointNet2D32 estimate_keypoints = FaceKeypointNet2D32 () # apply directly to an image (numpy-array) inference = estimate_keypoints ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and keypoints . The corresponding values of these keys contain the image with the drawn inferences and a numpy array representing the keypoints. [source] HeadPoseKeypointNet2D32 paz . pipelines . pose . HeadPoseKeypointNet2D32 ( camera , offsets = [ 0 , 0 ], radius = 5 , thickness = 2 ) Head pose estimation pipeline using a HaarCascade face detector and a pre-trained KeypointNet2D estimation model. Arguments camera : Instance of paz.backend.camera.Camera with camera intrinsics. offsets : List of floats indicating the scaled offset to be added to the Box2D coordinates. radius : Int. radius of keypoint to be drawn. Example from paz.pipelines import HeadPoseKeypointNet2D32 estimate_pose = HeadPoseKeypointNet2D32 () # apply directly to an image (numpy-array) inferences = estimate_pose ( image ) Returns A function that takes an RGB image and outputs the following inferences as keys of a dictionary: image , boxes2D , keypoints and poses6D . [source] HaarCascadeFrontalFace paz . pipelines . detection . HaarCascadeFrontalFace ( class_name = 'Face' , color = [ 0 , 255 , 0 ], draw = True ) HaarCascade pipeline for detecting frontal faces Arguments class_name : String indicating the class name. color : List indicating the RGB color e.g. [0, 255, 0] . draw : Boolean. If False the bounding boxes are not drawn. Example from paz.pipelines import HaarCascadeFrontalFace detect = HaarCascadeFrontalFace () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D .","title":"Applications"},{"location":"pipelines/applications/#ssd512coco","text":"paz . pipelines . detection . SSD512COCO ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD512 trained on COCO. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD512COCO detect = SSD512COCO () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . Reference SSD: Single Shot MultiBox Detector [source]","title":"SSD512COCO"},{"location":"pipelines/applications/#ssd300voc","text":"paz . pipelines . detection . SSD300VOC ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD300 trained on VOC. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD300VOC detect = SSD300VOC () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . Reference SSD: Single Shot MultiBox Detector [source]","title":"SSD300VOC"},{"location":"pipelines/applications/#ssd512ycbvideo","text":"paz . pipelines . detection . SSD512YCBVideo ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD512 trained on YCBVideo. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD512YCBVideo detect = SSD512YCBVideo () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . [source]","title":"SSD512YCBVideo"},{"location":"pipelines/applications/#ssd300fat","text":"paz . pipelines . detection . SSD300FAT ( score_thresh = 0.6 , nms_thresh = 0.45 , draw = True ) Single-shot inference pipeline with SSD300 trained on FAT. Arguments score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. draw : Boolean. If True prediction are drawn in the returned image. Example from paz.pipelines import SSD300FAT detect = SSD300FAT () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . [source]","title":"SSD300FAT"},{"location":"pipelines/applications/#detectminixceptionfer","text":"paz . pipelines . detection . DetectMiniXceptionFER ( offsets = [ 0 , 0 ], colors = [[ 255 , 0 , 0 ], [ 45 , 90 , 45 ], [ 255 , 0 , 255 ], [ 255 , 255 , 0 ], [ 0 , 0 , 255 ], [ 0 , 255 , 255 ], [ 0 , 255 , 0 ]]) Emotion classification and detection pipeline. Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . Example from paz.pipelines import DetectMiniXceptionFER detect = DetectMiniXceptionFER () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D . References Real-time Convolutional Neural Networks for Emotion and Gender Classification [source]","title":"DetectMiniXceptionFER"},{"location":"pipelines/applications/#minixceptionfer","text":"paz . pipelines . classification . MiniXceptionFER () Mini Xception pipeline for classifying emotions from RGB faces. Example from paz.pipelines import MiniXceptionFER classify = MiniXceptionFER () # apply directly to an image (numpy-array) inference = classify ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : class_names and scores . References Real-time Convolutional Neural Networks for Emotion and Gender Classification [source]","title":"MiniXceptionFER"},{"location":"pipelines/applications/#facekeypointnet2d32","text":"paz . pipelines . keypoints . FaceKeypointNet2D32 ( draw = True , radius = 3 ) KeypointNet2D model trained with Kaggle Facial Detection challenge. Arguments draw : Boolean indicating if inferences should be drawn. radius : Int. used for drawing the predicted keypoints. Example from paz.pipelines import FaceKeypointNet2D32 estimate_keypoints = FaceKeypointNet2D32 () # apply directly to an image (numpy-array) inference = estimate_keypoints ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and keypoints . The corresponding values of these keys contain the image with the drawn inferences and a numpy array representing the keypoints. [source]","title":"FaceKeypointNet2D32"},{"location":"pipelines/applications/#headposekeypointnet2d32","text":"paz . pipelines . pose . HeadPoseKeypointNet2D32 ( camera , offsets = [ 0 , 0 ], radius = 5 , thickness = 2 ) Head pose estimation pipeline using a HaarCascade face detector and a pre-trained KeypointNet2D estimation model. Arguments camera : Instance of paz.backend.camera.Camera with camera intrinsics. offsets : List of floats indicating the scaled offset to be added to the Box2D coordinates. radius : Int. radius of keypoint to be drawn. Example from paz.pipelines import HeadPoseKeypointNet2D32 estimate_pose = HeadPoseKeypointNet2D32 () # apply directly to an image (numpy-array) inferences = estimate_pose ( image ) Returns A function that takes an RGB image and outputs the following inferences as keys of a dictionary: image , boxes2D , keypoints and poses6D . [source]","title":"HeadPoseKeypointNet2D32"},{"location":"pipelines/applications/#haarcascadefrontalface","text":"paz . pipelines . detection . HaarCascadeFrontalFace ( class_name = 'Face' , color = [ 0 , 255 , 0 ], draw = True ) HaarCascade pipeline for detecting frontal faces Arguments class_name : String indicating the class name. color : List indicating the RGB color e.g. [0, 255, 0] . draw : Boolean. If False the bounding boxes are not drawn. Example from paz.pipelines import HaarCascadeFrontalFace detect = HaarCascadeFrontalFace () # apply directly to an image (numpy-array) inferences = detect ( image ) Returns A function that takes an RGB image and outputs the predictions as a dictionary with keys : image and boxes2D . The corresponding values of these keys contain the image with the drawn inferences and a list of paz.abstract.messages.Boxes2D .","title":"HaarCascadeFrontalFace"},{"location":"pipelines/detection/","text":"Built-in pipelines for preprocessing, agumentating and predicting. [source] AugmentBoxes paz . pipelines . detection . AugmentBoxes ( mean = ( 104 , 117 , 123 )) Perform data augmentation with bounding boxes. Arguments mean : List of three elements used to fill empty image spaces. [source] AugmentDetection paz . pipelines . detection . AugmentDetection ( prior_boxes , split = 0 , num_classes = 21 , size = 300 , mean = ( 104 , 117 , 123 ), IOU = 0.5 , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Augment boxes and images for object detection. Arguments prior_boxes : Numpy array of shape [num_boxes, 4] containing prior/default bounding boxes. split : Flag from paz.processors.TRAIN , paz.processors.VAL or paz.processors.TEST . Certain transformations would take place depending on the flag. num_classes : Int. size : Int. Image size. mean : List of three elements indicating the per channel mean. IOU : Float. Intersection over union used to match boxes. variances : List of two floats indicating variances to be encoded for encoding bounding boxes. [source] PreprocessBoxes paz . pipelines . detection . PreprocessBoxes ( num_classes , prior_boxes , IOU , variances ) Preprocess bounding boxes Arguments num_classes : Int. prior_boxes : Numpy array of shape [num_boxes, 4] containing prior/default bounding boxes. IOU : Float. Intersection over union used to match boxes. variances : List of two floats indicating variances to be encoded for encoding bounding boxes. [source] DetectSingleShot paz . pipelines . detection . DetectSingleShot ( model , class_names , score_thresh , nms_thresh , mean = ( 104 , 117 , 123 ), variances = [ 0.1 , 0.1 , 0.2 , 0.2 ], draw = True ) Single-shot object detection prediction. Arguments model : Keras model. class_names : List of strings indicating the class names. score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. mean : List of three elements indicating the per channel mean. draw : Boolean. If True prediction are drawn in the returned image. [source] DetectHaarCascade paz . pipelines . detection . DetectHaarCascade ( detector , class_names = None , colors = None , draw = True ) HaarCascade prediction pipeline/function from RGB-image. Arguments detector : An instantiated HaarCascadeDetector model. offsets : List of two elements. Each element must be between [0, 1]. class_names : List of strings. draw : Boolean. If True prediction are drawn in the returned image. Returns A function for predicting bounding box detections. [source] DetectHumanPose2D paz . pipelines . detection . DetectHumanPose2D ( dataset = 'COCO' , data_with_center = False , max_num_people = 30 , with_flip = True , draw = True ) Detectect human keypoints in a image and draw a skeleton over the image. Arguments model : Modle weights trained on HigherHRNet model. keypoint_order : List of length 17 (number of keypoints). where the keypoints are listed order wise. flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. dataset : String. Name of the dataset used for training the model. data_with_center : Boolean. True is the model is trained using the center. image : Numpy array. Input image Returns dictonary with the following keys: image: contains the image with skeleton drawn on it. keypoints: location of keypoints score: score of detection","title":"Detection"},{"location":"pipelines/detection/#augmentboxes","text":"paz . pipelines . detection . AugmentBoxes ( mean = ( 104 , 117 , 123 )) Perform data augmentation with bounding boxes. Arguments mean : List of three elements used to fill empty image spaces. [source]","title":"AugmentBoxes"},{"location":"pipelines/detection/#augmentdetection","text":"paz . pipelines . detection . AugmentDetection ( prior_boxes , split = 0 , num_classes = 21 , size = 300 , mean = ( 104 , 117 , 123 ), IOU = 0.5 , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Augment boxes and images for object detection. Arguments prior_boxes : Numpy array of shape [num_boxes, 4] containing prior/default bounding boxes. split : Flag from paz.processors.TRAIN , paz.processors.VAL or paz.processors.TEST . Certain transformations would take place depending on the flag. num_classes : Int. size : Int. Image size. mean : List of three elements indicating the per channel mean. IOU : Float. Intersection over union used to match boxes. variances : List of two floats indicating variances to be encoded for encoding bounding boxes. [source]","title":"AugmentDetection"},{"location":"pipelines/detection/#preprocessboxes","text":"paz . pipelines . detection . PreprocessBoxes ( num_classes , prior_boxes , IOU , variances ) Preprocess bounding boxes Arguments num_classes : Int. prior_boxes : Numpy array of shape [num_boxes, 4] containing prior/default bounding boxes. IOU : Float. Intersection over union used to match boxes. variances : List of two floats indicating variances to be encoded for encoding bounding boxes. [source]","title":"PreprocessBoxes"},{"location":"pipelines/detection/#detectsingleshot","text":"paz . pipelines . detection . DetectSingleShot ( model , class_names , score_thresh , nms_thresh , mean = ( 104 , 117 , 123 ), variances = [ 0.1 , 0.1 , 0.2 , 0.2 ], draw = True ) Single-shot object detection prediction. Arguments model : Keras model. class_names : List of strings indicating the class names. score_thresh : Float between [0, 1] nms_thresh : Float between [0, 1]. mean : List of three elements indicating the per channel mean. draw : Boolean. If True prediction are drawn in the returned image. [source]","title":"DetectSingleShot"},{"location":"pipelines/detection/#detecthaarcascade","text":"paz . pipelines . detection . DetectHaarCascade ( detector , class_names = None , colors = None , draw = True ) HaarCascade prediction pipeline/function from RGB-image. Arguments detector : An instantiated HaarCascadeDetector model. offsets : List of two elements. Each element must be between [0, 1]. class_names : List of strings. draw : Boolean. If True prediction are drawn in the returned image. Returns A function for predicting bounding box detections. [source]","title":"DetectHaarCascade"},{"location":"pipelines/detection/#detecthumanpose2d","text":"paz . pipelines . detection . DetectHumanPose2D ( dataset = 'COCO' , data_with_center = False , max_num_people = 30 , with_flip = True , draw = True ) Detectect human keypoints in a image and draw a skeleton over the image. Arguments model : Modle weights trained on HigherHRNet model. keypoint_order : List of length 17 (number of keypoints). where the keypoints are listed order wise. flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. dataset : String. Name of the dataset used for training the model. data_with_center : Boolean. True is the model is trained using the center. image : Numpy array. Input image Returns dictonary with the following keys: image: contains the image with skeleton drawn on it. keypoints: location of keypoints score: score of detection","title":"DetectHumanPose2D"},{"location":"pipelines/heatmaps/","text":"[source] GetHeatmapsAndTags paz . pipelines . heatmaps . GetHeatmapsAndTags ( model , flipped_keypoint_order , with_flip , data_with_center , scale_output = True , axes = [ 0 , 3 , 1 , 2 ]) Get Heatmaps and Tags from the model output. Arguments model : Model weights trained on HigherHRNet model. flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. data_with_center : Boolean. True is the model is trained using the center. image : Numpy array. Input image of shape (H, W) Returns heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W)","title":"Heatmaps"},{"location":"pipelines/heatmaps/#getheatmapsandtags","text":"paz . pipelines . heatmaps . GetHeatmapsAndTags ( model , flipped_keypoint_order , with_flip , data_with_center , scale_output = True , axes = [ 0 , 3 , 1 , 2 ]) Get Heatmaps and Tags from the model output. Arguments model : Model weights trained on HigherHRNet model. flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. data_with_center : Boolean. True is the model is trained using the center. image : Numpy array. Input image of shape (H, W) Returns heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W)","title":"GetHeatmapsAndTags"},{"location":"pipelines/image/","text":"Built-in pipelines for preprocessing, agumentating and predicting. [source] AugmentImage paz . pipelines . image . AugmentImage () Augments an RGB image by randomly changing contrast, brightness saturation and hue. [source] PreprocessImage paz . pipelines . image . PreprocessImage ( shape , mean = ( 104 , 117 , 123 )) Preprocess RGB image by resizing it to the given shape . If a mean is given it is substracted from image and it not the image gets normalized. Arguments shape : List of two Ints. mean : List of three Ints indicating the per-channel mean to be subtracted. [source] DecoderPredictor paz . pipelines . image . DecoderPredictor ( decoder ) Pipeline for predicting decoded image from a latent vector. Arguments model : Keras model. [source] EncoderPredictor paz . pipelines . image . EncoderPredictor ( encoder ) Pipeline for predicting latent vector of an encoder. Arguments model : Keras model. [source] PreprocessImageHigherHRNet paz . pipelines . image . PreprocessImageHigherHRNet ( scaling_factor = 200 , input_size = 512 , multiple = 64 ) Transform the image according to the HigherHRNet model requirement. Arguments scaling_factor : Int. scale factor for image dimensions. input_size : Int. resize the first dimension of image to input size. inverse : Boolean. Reverse the affine transform input. image : Numpy array. Input image Returns image : resized and transformed image center : center of the image scale : scaled image dimensions","title":"Image"},{"location":"pipelines/image/#augmentimage","text":"paz . pipelines . image . AugmentImage () Augments an RGB image by randomly changing contrast, brightness saturation and hue. [source]","title":"AugmentImage"},{"location":"pipelines/image/#preprocessimage","text":"paz . pipelines . image . PreprocessImage ( shape , mean = ( 104 , 117 , 123 )) Preprocess RGB image by resizing it to the given shape . If a mean is given it is substracted from image and it not the image gets normalized. Arguments shape : List of two Ints. mean : List of three Ints indicating the per-channel mean to be subtracted. [source]","title":"PreprocessImage"},{"location":"pipelines/image/#decoderpredictor","text":"paz . pipelines . image . DecoderPredictor ( decoder ) Pipeline for predicting decoded image from a latent vector. Arguments model : Keras model. [source]","title":"DecoderPredictor"},{"location":"pipelines/image/#encoderpredictor","text":"paz . pipelines . image . EncoderPredictor ( encoder ) Pipeline for predicting latent vector of an encoder. Arguments model : Keras model. [source]","title":"EncoderPredictor"},{"location":"pipelines/image/#preprocessimagehigherhrnet","text":"paz . pipelines . image . PreprocessImageHigherHRNet ( scaling_factor = 200 , input_size = 512 , multiple = 64 ) Transform the image according to the HigherHRNet model requirement. Arguments scaling_factor : Int. scale factor for image dimensions. input_size : Int. resize the first dimension of image to input size. inverse : Boolean. Reverse the affine transform input. image : Numpy array. Input image Returns image : resized and transformed image center : center of the image scale : scaled image dimensions","title":"PreprocessImageHigherHRNet"},{"location":"pipelines/keypoints/","text":"Built-in pipelines for preprocessing, agumentating and predicting. [source] KeypointNetInference paz . pipelines . keypoints . KeypointNetInference ( model , num_keypoints = None , radius = 5 ) Performs inference from a KeypointNetShared model. Arguments model : Keras model for predicting keypoints. num_keypoints : Int or None. If None num_keypoints is tried to be inferred from model.output_shape radius : Int. used for drawing the predicted keypoints. [source] KeypointNetSharedAugmentation paz . pipelines . keypoints . KeypointNetSharedAugmentation ( renderer , size ) Wraps RenderTwoViews as a sequential processor for using it directly with a paz.GeneratingSequence . Arguments renderer : RenderTwoViews processor. size : Image size. [source] EstimateKeypoints2D paz . pipelines . keypoints . EstimateKeypoints2D ( model , num_keypoints , draw = True , radius = 3 , color = 4 ) Basic 2D keypoint prediction pipeline. Arguments model : Keras model for predicting keypoints. num_keypoints : Int or None. If None num_keypoints is tried to be inferred from model.output_shape draw : Boolean indicating if inferences should be drawn. radius : Int. used for drawing the predicted keypoints. [source] DetectKeypoints2D paz . pipelines . detection . DetectKeypoints2D ( detect , estimate_keypoints , offsets = [ 0 , 0 ], radius = 3 ) [source] GetKeypoints paz . pipelines . keypoints . GetKeypoints ( max_num_instance , keypoint_order , detection_thresh = 0.2 , tag_thresh = 1 ) Extract out the top k keypoints heatmaps and group the keypoints with their respective tags value. Adjust and refine the keypoint locations by removing the margins. Arguments max_num_instance : Int. Maximum number of instances to be detected. keypoint_order : List of length 17 (number of keypoints). heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W, 2) Returns grouped_keypoints : numpy array. keypoints grouped by tag scores : int: score for the keypoint [source] TransformKeypoints paz . pipelines . keypoints . TransformKeypoints ( inverse = False ) Transform the keypoint coordinates. Arguments grouped_keypoints : Numpy array. keypoints grouped by tag center : Tuple. center of the imput image scale : Float. scaled imput image dimension shape : Tuple/List Returns transformed_keypoints : keypoint location with respect to the input image","title":"Keypoints"},{"location":"pipelines/keypoints/#keypointnetinference","text":"paz . pipelines . keypoints . KeypointNetInference ( model , num_keypoints = None , radius = 5 ) Performs inference from a KeypointNetShared model. Arguments model : Keras model for predicting keypoints. num_keypoints : Int or None. If None num_keypoints is tried to be inferred from model.output_shape radius : Int. used for drawing the predicted keypoints. [source]","title":"KeypointNetInference"},{"location":"pipelines/keypoints/#keypointnetsharedaugmentation","text":"paz . pipelines . keypoints . KeypointNetSharedAugmentation ( renderer , size ) Wraps RenderTwoViews as a sequential processor for using it directly with a paz.GeneratingSequence . Arguments renderer : RenderTwoViews processor. size : Image size. [source]","title":"KeypointNetSharedAugmentation"},{"location":"pipelines/keypoints/#estimatekeypoints2d","text":"paz . pipelines . keypoints . EstimateKeypoints2D ( model , num_keypoints , draw = True , radius = 3 , color = 4 ) Basic 2D keypoint prediction pipeline. Arguments model : Keras model for predicting keypoints. num_keypoints : Int or None. If None num_keypoints is tried to be inferred from model.output_shape draw : Boolean indicating if inferences should be drawn. radius : Int. used for drawing the predicted keypoints. [source]","title":"EstimateKeypoints2D"},{"location":"pipelines/keypoints/#detectkeypoints2d","text":"paz . pipelines . detection . DetectKeypoints2D ( detect , estimate_keypoints , offsets = [ 0 , 0 ], radius = 3 ) [source]","title":"DetectKeypoints2D"},{"location":"pipelines/keypoints/#getkeypoints","text":"paz . pipelines . keypoints . GetKeypoints ( max_num_instance , keypoint_order , detection_thresh = 0.2 , tag_thresh = 1 ) Extract out the top k keypoints heatmaps and group the keypoints with their respective tags value. Adjust and refine the keypoint locations by removing the margins. Arguments max_num_instance : Int. Maximum number of instances to be detected. keypoint_order : List of length 17 (number of keypoints). heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W, 2) Returns grouped_keypoints : numpy array. keypoints grouped by tag scores : int: score for the keypoint [source]","title":"GetKeypoints"},{"location":"pipelines/keypoints/#transformkeypoints","text":"paz . pipelines . keypoints . TransformKeypoints ( inverse = False ) Transform the keypoint coordinates. Arguments grouped_keypoints : Numpy array. keypoints grouped by tag center : Tuple. center of the imput image scale : Float. scaled imput image dimension shape : Tuple/List Returns transformed_keypoints : keypoint location with respect to the input image","title":"TransformKeypoints"},{"location":"pipelines/masks/","text":"[source] RGBMaskToImagePoints2D paz . pipelines . masks . RGBMaskToImagePoints2D () Predicts 2D image keypoints from an RGB mask. [source] RGBMaskToObjectPoints3D paz . pipelines . masks . RGBMaskToObjectPoints3D ( object_sizes ) Predicts 3D keypoints from an RGB mask. Arguments object_sizes : Array (3) determining the (width, height, depth) [source] PredictRGBMask paz . pipelines . masks . PredictRGBMask ( model , epsilon = 0.15 ) Predicts RGB mask from a segmentation model Arguments model : Keras segmentation model. epsilon : Float. Values below this value would be replaced by 0. [source] Pix2Points paz . pipelines . masks . Pix2Points ( model , object_sizes , epsilon = 0.15 , resize = False , method = 1 ) Predicts RGB_mask and corresponding points2D and points3D. Arguments model : Keras segmentation model. object_sizes : Array (3) determining the (width, height, depth) epsilon : Float. Values below this value would be replaced by 0. resize : Boolean. If True RGB mask is resized to original shape. method : Interpolation method to use if resize is True. Note Compare with and without RGB interpolation.","title":"Masks"},{"location":"pipelines/masks/#rgbmasktoimagepoints2d","text":"paz . pipelines . masks . RGBMaskToImagePoints2D () Predicts 2D image keypoints from an RGB mask. [source]","title":"RGBMaskToImagePoints2D"},{"location":"pipelines/masks/#rgbmasktoobjectpoints3d","text":"paz . pipelines . masks . RGBMaskToObjectPoints3D ( object_sizes ) Predicts 3D keypoints from an RGB mask. Arguments object_sizes : Array (3) determining the (width, height, depth) [source]","title":"RGBMaskToObjectPoints3D"},{"location":"pipelines/masks/#predictrgbmask","text":"paz . pipelines . masks . PredictRGBMask ( model , epsilon = 0.15 ) Predicts RGB mask from a segmentation model Arguments model : Keras segmentation model. epsilon : Float. Values below this value would be replaced by 0. [source]","title":"PredictRGBMask"},{"location":"pipelines/masks/#pix2points","text":"paz . pipelines . masks . Pix2Points ( model , object_sizes , epsilon = 0.15 , resize = False , method = 1 ) Predicts RGB_mask and corresponding points2D and points3D. Arguments model : Keras segmentation model. object_sizes : Array (3) determining the (width, height, depth) epsilon : Float. Values below this value would be replaced by 0. resize : Boolean. If True RGB mask is resized to original shape. method : Interpolation method to use if resize is True. Note Compare with and without RGB interpolation.","title":"Pix2Points"},{"location":"pipelines/pose/","text":"Built-in pipelines for preprocessing, agumentating and predicting. [source] EstimatePoseKeypoints paz . pipelines . pose . EstimatePoseKeypoints ( detect , estimate_keypoints , camera , offsets , model_points , class_to_dimensions , radius = 3 , thickness = 1 ) [source] HeadPoseKeypointNet2D32 paz . pipelines . pose . HeadPoseKeypointNet2D32 ( camera , offsets = [ 0 , 0 ], radius = 5 , thickness = 2 ) Head pose estimation pipeline using a HaarCascade face detector and a pre-trained KeypointNet2D estimation model. Arguments camera : Instance of paz.backend.camera.Camera with camera intrinsics. offsets : List of floats indicating the scaled offset to be added to the Box2D coordinates. radius : Int. radius of keypoint to be drawn. Example from paz.pipelines import HeadPoseKeypointNet2D32 estimate_pose = HeadPoseKeypointNet2D32 () # apply directly to an image (numpy-array) inferences = estimate_pose ( image ) Returns A function that takes an RGB image and outputs the following inferences as keys of a dictionary: image , boxes2D , keypoints and poses6D .","title":"Pose"},{"location":"pipelines/pose/#estimateposekeypoints","text":"paz . pipelines . pose . EstimatePoseKeypoints ( detect , estimate_keypoints , camera , offsets , model_points , class_to_dimensions , radius = 3 , thickness = 1 ) [source]","title":"EstimatePoseKeypoints"},{"location":"pipelines/pose/#headposekeypointnet2d32","text":"paz . pipelines . pose . HeadPoseKeypointNet2D32 ( camera , offsets = [ 0 , 0 ], radius = 5 , thickness = 2 ) Head pose estimation pipeline using a HaarCascade face detector and a pre-trained KeypointNet2D estimation model. Arguments camera : Instance of paz.backend.camera.Camera with camera intrinsics. offsets : List of floats indicating the scaled offset to be added to the Box2D coordinates. radius : Int. radius of keypoint to be drawn. Example from paz.pipelines import HeadPoseKeypointNet2D32 estimate_pose = HeadPoseKeypointNet2D32 () # apply directly to an image (numpy-array) inferences = estimate_pose ( image ) Returns A function that takes an RGB image and outputs the following inferences as keys of a dictionary: image , boxes2D , keypoints and poses6D .","title":"HeadPoseKeypointNet2D32"},{"location":"pipelines/renderer/","text":"[source] RandomizeRenderedImage paz . pipelines . renderer . RandomizeRenderedImage ( image_paths , num_occlusions = 1 , max_radius_scale = 0.5 ) Performs alpha blending and data-augmentation to an image and it's alpha channel. image_paths: List of strings indicating the paths to the images used for the background. num_occlusions: Int. number of occlusions to be added to the image. max_radius_scale: Float between [0, 1] indicating the maximum radius in scale of the image size. [source] RenderTwoViews paz . pipelines . renderer . RenderTwoViews ( renderer ) Renders two views along with their transformations. Arguments renderer : A class with a method render that outputs two lists. The first list contains two numpy arrays representing the images e.g. (image_A, image_B) each of shape [H, W, 3] . The other list contains three numpy arrays representing the transformations from the origin to the cameras and the two alpha channels of both images e.g. [matrices, alpha_channel_A, alpha_channel_B] . matrices is a numpy array of shape (4, 4 * 4) . Each row is a matrix of 4 x 4 representing the following transformations respectively: world_to_A , world_to_B , A_to_world and B_to_world . The shape of each alpha_channel should be [H, W] .","title":"Renderer"},{"location":"pipelines/renderer/#randomizerenderedimage","text":"paz . pipelines . renderer . RandomizeRenderedImage ( image_paths , num_occlusions = 1 , max_radius_scale = 0.5 ) Performs alpha blending and data-augmentation to an image and it's alpha channel. image_paths: List of strings indicating the paths to the images used for the background. num_occlusions: Int. number of occlusions to be added to the image. max_radius_scale: Float between [0, 1] indicating the maximum radius in scale of the image size. [source]","title":"RandomizeRenderedImage"},{"location":"pipelines/renderer/#rendertwoviews","text":"paz . pipelines . renderer . RenderTwoViews ( renderer ) Renders two views along with their transformations. Arguments renderer : A class with a method render that outputs two lists. The first list contains two numpy arrays representing the images e.g. (image_A, image_B) each of shape [H, W, 3] . The other list contains three numpy arrays representing the transformations from the origin to the cameras and the two alpha channels of both images e.g. [matrices, alpha_channel_A, alpha_channel_B] . matrices is a numpy array of shape (4, 4 * 4) . Each row is a matrix of 4 x 4 representing the following transformations respectively: world_to_A , world_to_B , A_to_world and B_to_world . The shape of each alpha_channel should be [H, W] .","title":"RenderTwoViews"},{"location":"processors/detection/","text":"Processors for object detection [source] SquareBoxes2D paz . processors . detection . SquareBoxes2D () Transforms bounding rectangular boxes into square bounding boxes. [source] DenormalizeBoxes2D paz . processors . detection . DenormalizeBoxes2D () Denormalizes boxes shapes to be in accordance to the original image size. Arguments: image_size : List containing height and width of an image. [source] RoundBoxes2D paz . processors . detection . RoundBoxes2D () Round to integer box coordinates. [source] ClipBoxes2D paz . processors . detection . ClipBoxes2D () Clips boxes coordinates into the image dimensions [source] FilterClassBoxes2D paz . processors . detection . FilterClassBoxes2D ( valid_class_names ) Filters boxes with valid class names. Arguments valid_class_names : List of strings indicating class names to be kept. [source] CropBoxes2D paz . processors . detection . CropBoxes2D () Creates a list of images cropped from the bounding boxes. Arguments offset_scales : List of floats having x and y scales respectively. [source] ToBoxes2D paz . processors . detection . ToBoxes2D ( class_names = None , one_hot_encoded = False ) Transforms boxes from dataset into Boxes2D messages. Arguments class_names : List of class names ordered with respect to the class indices from the dataset boxes . [source] MatchBoxes paz . processors . detection . MatchBoxes ( prior_boxes , iou = 0.5 ) Match prior boxes with ground truth boxes. Arguments prior_boxes : Numpy array of shape (num_boxes, 4). iou : Float in [0, 1]. Intersection over union in which prior boxes will be considered positive. A positive box is box with a class different than background . variance : List of two floats. [source] EncodeBoxes paz . processors . detection . EncodeBoxes ( prior_boxes , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Encodes bounding boxes. Arguments prior_boxes : Numpy array of shape (num_boxes, 4). variances : List of two float values. [source] DecodeBoxes paz . processors . detection . DecodeBoxes ( prior_boxes , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Decodes bounding boxes. Arguments prior_boxes : Numpy array of shape (num_boxes, 4). variances : List of two float values. [source] NonMaximumSuppressionPerClass paz . processors . detection . NonMaximumSuppressionPerClass ( nms_thresh = 0.45 , conf_thresh = 0.01 ) Applies non maximum suppression per class. Arguments nms_thresh : Float between [0, 1]. conf_thresh : Float between [0, 1]. [source] FilterBoxes paz . processors . detection . FilterBoxes ( class_names , conf_thresh = 0.5 ) Filters boxes outputted from function detect as Box2D messages. Arguments class_names : List of class names. conf_thresh : Float between [0, 1]. [source] OffsetBoxes2D paz . processors . detection . OffsetBoxes2D ( offsets ) Offsets the height and widht of a list of Boxes2D . Arguments offsets : Float between [0, 1]. [source] CropImage paz . processors . detection . CropImage () Crop images using a list of box2D .","title":"Detection"},{"location":"processors/detection/#squareboxes2d","text":"paz . processors . detection . SquareBoxes2D () Transforms bounding rectangular boxes into square bounding boxes. [source]","title":"SquareBoxes2D"},{"location":"processors/detection/#denormalizeboxes2d","text":"paz . processors . detection . DenormalizeBoxes2D () Denormalizes boxes shapes to be in accordance to the original image size. Arguments: image_size : List containing height and width of an image. [source]","title":"DenormalizeBoxes2D"},{"location":"processors/detection/#roundboxes2d","text":"paz . processors . detection . RoundBoxes2D () Round to integer box coordinates. [source]","title":"RoundBoxes2D"},{"location":"processors/detection/#clipboxes2d","text":"paz . processors . detection . ClipBoxes2D ()","title":"ClipBoxes2D"},{"location":"processors/detection/#clips-boxes-coordinates-into-the-image-dimensions","text":"[source]","title":"Clips boxes coordinates into the image dimensions"},{"location":"processors/detection/#filterclassboxes2d","text":"paz . processors . detection . FilterClassBoxes2D ( valid_class_names ) Filters boxes with valid class names. Arguments valid_class_names : List of strings indicating class names to be kept. [source]","title":"FilterClassBoxes2D"},{"location":"processors/detection/#cropboxes2d","text":"paz . processors . detection . CropBoxes2D () Creates a list of images cropped from the bounding boxes. Arguments offset_scales : List of floats having x and y scales respectively. [source]","title":"CropBoxes2D"},{"location":"processors/detection/#toboxes2d","text":"paz . processors . detection . ToBoxes2D ( class_names = None , one_hot_encoded = False ) Transforms boxes from dataset into Boxes2D messages. Arguments class_names : List of class names ordered with respect to the class indices from the dataset boxes . [source]","title":"ToBoxes2D"},{"location":"processors/detection/#matchboxes","text":"paz . processors . detection . MatchBoxes ( prior_boxes , iou = 0.5 ) Match prior boxes with ground truth boxes. Arguments prior_boxes : Numpy array of shape (num_boxes, 4). iou : Float in [0, 1]. Intersection over union in which prior boxes will be considered positive. A positive box is box with a class different than background . variance : List of two floats. [source]","title":"MatchBoxes"},{"location":"processors/detection/#encodeboxes","text":"paz . processors . detection . EncodeBoxes ( prior_boxes , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Encodes bounding boxes. Arguments prior_boxes : Numpy array of shape (num_boxes, 4). variances : List of two float values. [source]","title":"EncodeBoxes"},{"location":"processors/detection/#decodeboxes","text":"paz . processors . detection . DecodeBoxes ( prior_boxes , variances = [ 0.1 , 0.1 , 0.2 , 0.2 ]) Decodes bounding boxes. Arguments prior_boxes : Numpy array of shape (num_boxes, 4). variances : List of two float values. [source]","title":"DecodeBoxes"},{"location":"processors/detection/#nonmaximumsuppressionperclass","text":"paz . processors . detection . NonMaximumSuppressionPerClass ( nms_thresh = 0.45 , conf_thresh = 0.01 ) Applies non maximum suppression per class. Arguments nms_thresh : Float between [0, 1]. conf_thresh : Float between [0, 1]. [source]","title":"NonMaximumSuppressionPerClass"},{"location":"processors/detection/#filterboxes","text":"paz . processors . detection . FilterBoxes ( class_names , conf_thresh = 0.5 ) Filters boxes outputted from function detect as Box2D messages. Arguments class_names : List of class names. conf_thresh : Float between [0, 1]. [source]","title":"FilterBoxes"},{"location":"processors/detection/#offsetboxes2d","text":"paz . processors . detection . OffsetBoxes2D ( offsets ) Offsets the height and widht of a list of Boxes2D . Arguments offsets : Float between [0, 1]. [source]","title":"OffsetBoxes2D"},{"location":"processors/detection/#cropimage","text":"paz . processors . detection . CropImage () Crop images using a list of box2D .","title":"CropImage"},{"location":"processors/draw/","text":"Processors for drawing [source] DrawBoxes2D paz . processors . draw . DrawBoxes2D ( class_names = None , colors = None , weighted = False , scale = 0.7 , with_score = True ) Draws bounding boxes from Boxes2D messages. Arguments class_names : List of strings. colors : List of lists containing the color values weighted : Boolean. If True the colors are weighted with the score of the bounding box. scale : Float. Scale of drawn text. [source] DrawKeypoints2D paz . processors . draw . DrawKeypoints2D ( num_keypoints , radius = 3 , normalized = False ) Draws keypoints into image. Arguments num_keypoints : Int. Used initialize colors for each keypoint radius : Float. Approximate radius of the circle in pixel coordinates. [source] DrawBoxes3D paz . processors . draw . DrawBoxes3D ( camera , class_to_dimensions , color = ( 0 , 255 , 0 ), thickness = 5 , radius = 2 ) [source] DrawRandomPolygon paz . processors . draw . DrawRandomPolygon ( max_radius_scale = 0.5 ) Adds occlusion to image Arguments max_radius_scale : Maximum radius in scale with respect to image i.e. each vertex radius from the polygon is sampled from [0, max_radius_scale] . This radius is later multiplied by the image dimensions. [source] DrawPose6D paz . processors . draw . DrawPose6D ( cube_points3D , camera_intrinsics , thickness = 2 ) Draws pose6D by projecting cube3D to image space with camera intrinsics. Arguments image : Array (H, W, 3) pose6D : paz message Pose6D with quaternion and translation values. cube3D : Array (8, 3). Cube 3D points in object frame. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. Returns Original image array (H, W, 3) with drawn cube points. Note This function uses a numpy project function instead of openCV. [source] DrawHumanSkeleton paz . processors . draw . DrawHumanSkeleton ( dataset , check_scores ) Draw human pose skeleton on image. Arguments images : Numpy array. grouped_joints : Joint locations of all the person model detected in the image. List of numpy array. dataset : String. check_scores : Boolean. Flag to check score before drawing. Returns A numpy array containing pose skeleton.","title":"Draw"},{"location":"processors/draw/#drawboxes2d","text":"paz . processors . draw . DrawBoxes2D ( class_names = None , colors = None , weighted = False , scale = 0.7 , with_score = True ) Draws bounding boxes from Boxes2D messages. Arguments class_names : List of strings. colors : List of lists containing the color values weighted : Boolean. If True the colors are weighted with the score of the bounding box. scale : Float. Scale of drawn text. [source]","title":"DrawBoxes2D"},{"location":"processors/draw/#drawkeypoints2d","text":"paz . processors . draw . DrawKeypoints2D ( num_keypoints , radius = 3 , normalized = False ) Draws keypoints into image. Arguments num_keypoints : Int. Used initialize colors for each keypoint radius : Float. Approximate radius of the circle in pixel coordinates. [source]","title":"DrawKeypoints2D"},{"location":"processors/draw/#drawboxes3d","text":"paz . processors . draw . DrawBoxes3D ( camera , class_to_dimensions , color = ( 0 , 255 , 0 ), thickness = 5 , radius = 2 ) [source]","title":"DrawBoxes3D"},{"location":"processors/draw/#drawrandompolygon","text":"paz . processors . draw . DrawRandomPolygon ( max_radius_scale = 0.5 ) Adds occlusion to image Arguments max_radius_scale : Maximum radius in scale with respect to image i.e. each vertex radius from the polygon is sampled from [0, max_radius_scale] . This radius is later multiplied by the image dimensions. [source]","title":"DrawRandomPolygon"},{"location":"processors/draw/#drawpose6d","text":"paz . processors . draw . DrawPose6D ( cube_points3D , camera_intrinsics , thickness = 2 ) Draws pose6D by projecting cube3D to image space with camera intrinsics. Arguments image : Array (H, W, 3) pose6D : paz message Pose6D with quaternion and translation values. cube3D : Array (8, 3). Cube 3D points in object frame. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. Returns Original image array (H, W, 3) with drawn cube points. Note This function uses a numpy project function instead of openCV. [source]","title":"DrawPose6D"},{"location":"processors/draw/#drawhumanskeleton","text":"paz . processors . draw . DrawHumanSkeleton ( dataset , check_scores ) Draw human pose skeleton on image. Arguments images : Numpy array. grouped_joints : Joint locations of all the person model detected in the image. List of numpy array. dataset : String. check_scores : Boolean. Flag to check score before drawing. Returns A numpy array containing pose skeleton.","title":"DrawHumanSkeleton"},{"location":"processors/geometric/","text":"Processors for geometric image transformations [source] RandomFlipBoxesLeftRight paz . processors . geometric . RandomFlipBoxesLeftRight () Flips image and implemented labels horizontally. [source] ToImageBoxCoordinates paz . processors . geometric . ToImageBoxCoordinates () Convert normalized box coordinates to image-size box coordinates. [source] ToNormalizedBoxCoordinates paz . processors . geometric . ToNormalizedBoxCoordinates () Convert image-size box coordinates to normalized box coordinates. [source] RandomSampleCrop paz . processors . geometric . RandomSampleCrop ( probability = 0.5 ) Crops and image while adjusting the bounding boxes. Boxes should be in point form. Arguments probability : Float between ''[0, 1]''. [source] RandomTranslation paz . processors . geometric . RandomTranslation ( delta_scale = [ 0.25 , 0.25 ], fill_color = None ) Applies a random translation to image and labels Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. fill_color: List of three integers indicating the color values e.g. ''[0, 0, 0]''. [source] RandomRotation paz . processors . geometric . RandomRotation ( rotation_range = 30 , fill_color = None , probability = 0.5 ) Randomly rotate an images Arguments rotation_range : Int. indicating the max and min values in degrees of the uniform distribution [-range, range] from which the angles are sampled. fill_color : ''None'' or List of three integers indicating the color values e.g. [0, 0, 0] . If None mean channel values of the image will be calculated as fill values. probability : Float between 0 and 1. [source] RandomKeypointTranslation paz . processors . geometric . RandomKeypointTranslation ( delta_scale = [ 0.2 , 0.2 ], fill_color = None , probability = 0.5 ) Applies a random translation to image and keypoints. Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. fill_color : ''None'' or List of three integers indicating the color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of the image will be calculated as fill values. probability : Float between ''[0, 1]''. [source] RandomKeypointRotation paz . processors . geometric . RandomKeypointRotation ( rotation_range = 30 , fill_color = None , probability = 0.5 ) Randomly rotate an images with its corresponding keypoints. Arguments rotation_range : Int. indicating the max and min values in degrees of the uniform distribution ''[-range, range]'' from which the angles are sampled. fill_color : ''None'' or List of three integers indicating the color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of the image will be calculated as fill values. [source] GetTransformationSize paz . processors . geometric . GetTransformationSize ( input_size , multiple ) Calculate the transformation size for the imgae. The size is tuple of length two indicating the x, y values. Arguments image : Numpy array [source] GetTransformationScale paz . processors . geometric . GetTransformationScale ( scaling_factor ) Calculate the transformation scale for the imgae. The scale is a numpy array of size two indicating the width and height scale. Arguments image : Numpy array size : Numpy array of length 2 [source] GetSourceDestinationPoints paz . processors . geometric . GetSourceDestinationPoints ( scaling_factor ) Returns the source and destination points for affine transformation. Arguments center : Numpy array of shape (2,). Center coordinates of image scale : Numpy array of shape (2,). Scale of width and height of image size : List of length 2. Size of image [source] GetImageCenter paz . processors . geometric . GetImageCenter ( offset = 0.5 ) Calculate the center of the image and add an offset to the center. Arguments image : Numpy array offset : Float [source] WarpAffine paz . processors . geometric . WarpAffine () Applies an affine transformation to an image Arguments image : Numpy array transform : Numpy array. Transformation matrix size : Numpy array. Transformation size","title":"Geometric"},{"location":"processors/geometric/#randomflipboxesleftright","text":"paz . processors . geometric . RandomFlipBoxesLeftRight () Flips image and implemented labels horizontally. [source]","title":"RandomFlipBoxesLeftRight"},{"location":"processors/geometric/#toimageboxcoordinates","text":"paz . processors . geometric . ToImageBoxCoordinates () Convert normalized box coordinates to image-size box coordinates. [source]","title":"ToImageBoxCoordinates"},{"location":"processors/geometric/#tonormalizedboxcoordinates","text":"paz . processors . geometric . ToNormalizedBoxCoordinates () Convert image-size box coordinates to normalized box coordinates. [source]","title":"ToNormalizedBoxCoordinates"},{"location":"processors/geometric/#randomsamplecrop","text":"paz . processors . geometric . RandomSampleCrop ( probability = 0.5 ) Crops and image while adjusting the bounding boxes. Boxes should be in point form. Arguments probability : Float between ''[0, 1]''. [source]","title":"RandomSampleCrop"},{"location":"processors/geometric/#randomtranslation","text":"paz . processors . geometric . RandomTranslation ( delta_scale = [ 0.25 , 0.25 ], fill_color = None ) Applies a random translation to image and labels Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. fill_color: List of three integers indicating the color values e.g. ''[0, 0, 0]''. [source]","title":"RandomTranslation"},{"location":"processors/geometric/#randomrotation","text":"paz . processors . geometric . RandomRotation ( rotation_range = 30 , fill_color = None , probability = 0.5 ) Randomly rotate an images Arguments rotation_range : Int. indicating the max and min values in degrees of the uniform distribution [-range, range] from which the angles are sampled. fill_color : ''None'' or List of three integers indicating the color values e.g. [0, 0, 0] . If None mean channel values of the image will be calculated as fill values. probability : Float between 0 and 1. [source]","title":"RandomRotation"},{"location":"processors/geometric/#randomkeypointtranslation","text":"paz . processors . geometric . RandomKeypointTranslation ( delta_scale = [ 0.2 , 0.2 ], fill_color = None , probability = 0.5 ) Applies a random translation to image and keypoints. Arguments delta_scale : List with two elements having the normalized deltas. e.g. ''[.25, .25]''. fill_color : ''None'' or List of three integers indicating the color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of the image will be calculated as fill values. probability : Float between ''[0, 1]''. [source]","title":"RandomKeypointTranslation"},{"location":"processors/geometric/#randomkeypointrotation","text":"paz . processors . geometric . RandomKeypointRotation ( rotation_range = 30 , fill_color = None , probability = 0.5 ) Randomly rotate an images with its corresponding keypoints. Arguments rotation_range : Int. indicating the max and min values in degrees of the uniform distribution ''[-range, range]'' from which the angles are sampled. fill_color : ''None'' or List of three integers indicating the color values e.g. ''[0, 0, 0]''. If ''None'' mean channel values of the image will be calculated as fill values. [source]","title":"RandomKeypointRotation"},{"location":"processors/geometric/#gettransformationsize","text":"paz . processors . geometric . GetTransformationSize ( input_size , multiple ) Calculate the transformation size for the imgae. The size is tuple of length two indicating the x, y values. Arguments image : Numpy array [source]","title":"GetTransformationSize"},{"location":"processors/geometric/#gettransformationscale","text":"paz . processors . geometric . GetTransformationScale ( scaling_factor ) Calculate the transformation scale for the imgae. The scale is a numpy array of size two indicating the width and height scale. Arguments image : Numpy array size : Numpy array of length 2 [source]","title":"GetTransformationScale"},{"location":"processors/geometric/#getsourcedestinationpoints","text":"paz . processors . geometric . GetSourceDestinationPoints ( scaling_factor ) Returns the source and destination points for affine transformation. Arguments center : Numpy array of shape (2,). Center coordinates of image scale : Numpy array of shape (2,). Scale of width and height of image size : List of length 2. Size of image [source]","title":"GetSourceDestinationPoints"},{"location":"processors/geometric/#getimagecenter","text":"paz . processors . geometric . GetImageCenter ( offset = 0.5 ) Calculate the center of the image and add an offset to the center. Arguments image : Numpy array offset : Float [source]","title":"GetImageCenter"},{"location":"processors/geometric/#warpaffine","text":"paz . processors . geometric . WarpAffine () Applies an affine transformation to an image Arguments image : Numpy array transform : Numpy array. Transformation matrix size : Numpy array. Transformation size","title":"WarpAffine"},{"location":"processors/groups/","text":"[source] ToAffineMatrix paz . processors . groups . ToAffineMatrix () Builds affine matrix from a rotation matrix and a translation vector. [source] RotationVectorToQuaternion paz . processors . groups . RotationVectorToQuaternion () Transforms rotation vector into quaternion. [source] RotationVectorToRotationMatrix paz . processors . groups . RotationVectorToRotationMatrix () Transforms rotation vector into a rotation matrix.","title":"Groups"},{"location":"processors/groups/#toaffinematrix","text":"paz . processors . groups . ToAffineMatrix () Builds affine matrix from a rotation matrix and a translation vector. [source]","title":"ToAffineMatrix"},{"location":"processors/groups/#rotationvectortoquaternion","text":"paz . processors . groups . RotationVectorToQuaternion () Transforms rotation vector into quaternion. [source]","title":"RotationVectorToQuaternion"},{"location":"processors/groups/#rotationvectortorotationmatrix","text":"paz . processors . groups . RotationVectorToRotationMatrix () Transforms rotation vector into a rotation matrix.","title":"RotationVectorToRotationMatrix"},{"location":"processors/heatmaps/","text":"[source] TransposeOutput paz . processors . heatmaps . TransposeOutput ( axes ) Transpose the output of the HigherHRNet model Arguments axes : List or tuple Output : List of numpy array [source] ScaleOutput paz . processors . heatmaps . ScaleOutput ( scale_factor , full_scaling = False ) Scale the output of the HigherHRNet model Arguments scaling_factor : Int. full_scaling : Boolean. If all the array of array are to be scaled. Output : List of numpy array [source] GetHeatmaps paz . processors . heatmaps . GetHeatmaps ( flipped_keypoint_order ) Get Heatmaps from the model output. Arguments flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. outputs : List of numpy arrays. Output of HigherHRNet model with_flip : Boolean. indicates whether to flip the output Returns heatmaps : Numpy array of shape (1, num_keypoints, H, W) [source] GetTags paz . processors . heatmaps . GetTags ( flipped_keypoint_order ) Get Tags from the model output. Arguments flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. outputs : List of numpy arrays. Output of HigherHRNet model with_flip : Boolean. indicates whether to flip the output Returns Tags : Numpy array of shape (1, num_keypoints, H, W) [source] RemoveLastElement paz . processors . heatmaps . RemoveLastElement () Remove last element of array Arguments x : array or list of arrays [source] AggregateResults paz . processors . heatmaps . AggregateResults ( with_flip = False ) Aggregate heatmaps and tags to get final heatmaps and tags for processing. Arguments heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W) Returns heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W, 2) [source] TopKDetections paz . processors . heatmaps . TopKDetections ( k , use_numpy = False ) Extract out the top k detections Arguments k : Int. Maximum number of instances to be detected. use_numpy : Boolean. Whether to use numpy functions or tf functions. heatmaps : Numpy array of shape (1, num_joints, H, W) Tags : Numpy array of shape (1, num_joints, H, W, 2) Returns top_k_detections : Numpy array. Contains the top k keypoints locations of the detection with their value and tags. [source] GroupKeypointsByTag paz . processors . heatmaps . GroupKeypointsByTag ( keypoint_order , tag_thresh , detection_thresh ) Group the keypoints with their respective tags value. Arguments keypoint_order : List of length 17 (number of keypoints). tag_thresh : Float. detection_thresh : Float. Detection : Numpy array containing the location, value and tags of top k keypoints Returns grouped_keypoints : Numpy array. keypoints grouped by tag [source] AdjustKeypointsLocations paz . processors . heatmaps . AdjustKeypointsLocations () Adjust the keypoint locations by removing the margins. Arguments heatmaps : Numpy array. grouped_keypoints : numpy array. keypoints grouped by tag [source] GetScores paz . processors . heatmaps . GetScores () Calculate the score of the detection results. Arguments grouped_keypoints : numpy array. keypoints grouped by tag [source] RefineKeypointsLocations paz . processors . heatmaps . RefineKeypointsLocations () Refine the keypoint locations by removing the margins. Arguments heatmaps : Numpy array. Tgas : Numpy array. grouped_keypoints : numpy array. keypoints grouped by tag [source] TransformKeypoints paz . processors . heatmaps . TransformKeypoints () Transform keypoint. Arguments grouped_keypoints : numpy array. keypoints grouped by tag transform : Numpy array. Transformation matrix [source] ExtractKeypointsLocations paz . processors . heatmaps . ExtractKeypointsLocations () Extract keypoint location. Arguments keypoints : numpy array","title":"Heatmaps"},{"location":"processors/heatmaps/#transposeoutput","text":"paz . processors . heatmaps . TransposeOutput ( axes ) Transpose the output of the HigherHRNet model Arguments axes : List or tuple Output : List of numpy array [source]","title":"TransposeOutput"},{"location":"processors/heatmaps/#scaleoutput","text":"paz . processors . heatmaps . ScaleOutput ( scale_factor , full_scaling = False ) Scale the output of the HigherHRNet model Arguments scaling_factor : Int. full_scaling : Boolean. If all the array of array are to be scaled. Output : List of numpy array [source]","title":"ScaleOutput"},{"location":"processors/heatmaps/#getheatmaps","text":"paz . processors . heatmaps . GetHeatmaps ( flipped_keypoint_order ) Get Heatmaps from the model output. Arguments flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. outputs : List of numpy arrays. Output of HigherHRNet model with_flip : Boolean. indicates whether to flip the output Returns heatmaps : Numpy array of shape (1, num_keypoints, H, W) [source]","title":"GetHeatmaps"},{"location":"processors/heatmaps/#gettags","text":"paz . processors . heatmaps . GetTags ( flipped_keypoint_order ) Get Tags from the model output. Arguments flipped_keypoint_order : List of length 17 (number of keypoints). Flipped list of keypoint order. outputs : List of numpy arrays. Output of HigherHRNet model with_flip : Boolean. indicates whether to flip the output Returns Tags : Numpy array of shape (1, num_keypoints, H, W) [source]","title":"GetTags"},{"location":"processors/heatmaps/#removelastelement","text":"paz . processors . heatmaps . RemoveLastElement () Remove last element of array Arguments x : array or list of arrays [source]","title":"RemoveLastElement"},{"location":"processors/heatmaps/#aggregateresults","text":"paz . processors . heatmaps . AggregateResults ( with_flip = False ) Aggregate heatmaps and tags to get final heatmaps and tags for processing. Arguments heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W) Returns heatmaps : Numpy array of shape (1, num_keypoints, H, W) Tags : Numpy array of shape (1, num_keypoints, H, W, 2) [source]","title":"AggregateResults"},{"location":"processors/heatmaps/#topkdetections","text":"paz . processors . heatmaps . TopKDetections ( k , use_numpy = False ) Extract out the top k detections Arguments k : Int. Maximum number of instances to be detected. use_numpy : Boolean. Whether to use numpy functions or tf functions. heatmaps : Numpy array of shape (1, num_joints, H, W) Tags : Numpy array of shape (1, num_joints, H, W, 2) Returns top_k_detections : Numpy array. Contains the top k keypoints locations of the detection with their value and tags. [source]","title":"TopKDetections"},{"location":"processors/heatmaps/#groupkeypointsbytag","text":"paz . processors . heatmaps . GroupKeypointsByTag ( keypoint_order , tag_thresh , detection_thresh ) Group the keypoints with their respective tags value. Arguments keypoint_order : List of length 17 (number of keypoints). tag_thresh : Float. detection_thresh : Float. Detection : Numpy array containing the location, value and tags of top k keypoints Returns grouped_keypoints : Numpy array. keypoints grouped by tag [source]","title":"GroupKeypointsByTag"},{"location":"processors/heatmaps/#adjustkeypointslocations","text":"paz . processors . heatmaps . AdjustKeypointsLocations () Adjust the keypoint locations by removing the margins. Arguments heatmaps : Numpy array. grouped_keypoints : numpy array. keypoints grouped by tag [source]","title":"AdjustKeypointsLocations"},{"location":"processors/heatmaps/#getscores","text":"paz . processors . heatmaps . GetScores () Calculate the score of the detection results. Arguments grouped_keypoints : numpy array. keypoints grouped by tag [source]","title":"GetScores"},{"location":"processors/heatmaps/#refinekeypointslocations","text":"paz . processors . heatmaps . RefineKeypointsLocations () Refine the keypoint locations by removing the margins. Arguments heatmaps : Numpy array. Tgas : Numpy array. grouped_keypoints : numpy array. keypoints grouped by tag [source]","title":"RefineKeypointsLocations"},{"location":"processors/heatmaps/#transformkeypoints","text":"paz . processors . heatmaps . TransformKeypoints () Transform keypoint. Arguments grouped_keypoints : numpy array. keypoints grouped by tag transform : Numpy array. Transformation matrix [source]","title":"TransformKeypoints"},{"location":"processors/heatmaps/#extractkeypointslocations","text":"paz . processors . heatmaps . ExtractKeypointsLocations () Extract keypoint location. Arguments keypoints : numpy array","title":"ExtractKeypointsLocations"},{"location":"processors/image/","text":"Processors for image transformations [source] CastImage paz . processors . image . CastImage ( dtype ) Cast image to given dtype. Arguments dtype : Str or np.dtype [source] SubtractMeanImage paz . processors . image . SubtractMeanImage ( mean ) Subtract channel-wise mean to image. Arguments mean : List of length 3, containing the channel-wise mean. [source] AddMeanImage paz . processors . image . AddMeanImage ( mean ) Adds channel-wise mean to image. Arguments mean : List of length 3, containing the channel-wise mean. [source] NormalizeImage paz . processors . image . NormalizeImage () Normalize image by diving all values by 255.0. [source] DenormalizeImage paz . processors . image . DenormalizeImage () Denormalize image by multiplying all values by 255.0. [source] LoadImage paz . processors . image . LoadImage ( num_channels = 3 ) Loads image. Arguments num_channels : Integer, valid integers are: 1, 3 and 4. [source] RandomSaturation paz . processors . image . RandomSaturation ( lower = 0.3 , upper = 1.5 ) Applies random saturation to an image in RGB space. Arguments lower : Float, lower bound for saturation factor. upper : Float, upper bound for saturation factor. [source] RandomBrightness paz . processors . image . RandomBrightness ( delta = 32 ) Adjust random brightness to an image in RGB space. Arguments max_delta : Float. [source] RandomContrast paz . processors . image . RandomContrast ( lower = 0.5 , upper = 1.5 ) Applies random contrast to an image in RGB Arguments lower : Float, indicating the lower bound of the random number to be multiplied with the BGR/RGB image. upper : Float, indicating the upper bound of the random number to be multiplied with the BGR/RGB image. [source] RandomHue paz . processors . image . RandomHue ( delta = 18 ) Applies random hue to an image in RGB space. Arguments delta : Int, indicating the range (-delta, delta ) of possible hue values. [source] ResizeImages paz . processors . image . ResizeImages ( shape ) Resize list of images. Arguments size : List of two ints. [source] ResizeImages paz . processors . image . ResizeImages ( shape ) Resize list of images. Arguments size : List of two ints. [source] RandomImageBlur paz . processors . image . RandomImageBlur ( probability = 0.5 ) Randomizes image quality Arguments probability : Float between [0, 1]. Assigns probability of how often a random image blur is applied. [source] RandomGaussianBlur paz . processors . image . RandomGaussianBlur ( kernel_size = ( 5 , 5 ), probability = 0.5 ) Randomizes image quality Arguments probability : Float between [0, 1]. Assigns probability of how often a random image blur is applied. [source] RandomFlipImageLeftRight paz . processors . image . RandomFlipImageLeftRight () Randomly flip the image left or right [source] ConvertColorSpace paz . processors . image . ConvertColorSpace ( flag ) Converts image to a different color space. Arguments flag : Flag found in processors indicating transform e.g. pr.BGR2RGB [source] ShowImage paz . processors . image . ShowImage ( window_name = 'image' , wait = True ) Shows image in a separate window. Arguments window_name : String. Window name. wait : Boolean [source] ImageDataProcessor paz . processors . image . ImageDataProcessor ( generator ) Wrapper for Keras ImageDataGenerator Arguments generator : An instantiated Keras ImageDataGenerator [source] AlphaBlending paz . processors . image . AlphaBlending () Blends image to background using the image's alpha channel. [source] RandomImageCrop paz . processors . image . RandomImageCrop ( crop_factor = 0.3 , probability = 0.5 ) Crops randomly a rectangle from an image. Arguments crop_factor : Float between [0, 1] . probability : Float between [0, 1] . [source] RandomShapeCrop paz . processors . image . RandomShapeCrop ( shape ) Randomly crops a part of an image of always the same given shape . Arguments shape : List of two ints [height, width]. Dimensions of image to be cropped. [source] MakeRandomPlainImage paz . processors . image . MakeRandomPlainImage ( shape ) Makes random plain image by randomly sampling an RGB color. Arguments shape : List of two ints [height, width]. Dimensions of plain image to be generated. [source] ConcatenateAlphaMask paz . processors . image . ConcatenateAlphaMask () Concatenates alpha mask to original image. [source] BlendRandomCroppedBackground paz . processors . image . BlendRandomCroppedBackground ( background_paths ) Blends image with a randomly cropped background. Arguments background_paths : List of strings. Each element of the list is a full-path to an image used for cropping a background. [source] AddOcclusion paz . processors . image . AddOcclusion ( max_radius_scale = 0.5 , probability = 0.5 ) Adds a random occlusion to image by generating random vertices and drawing a polygon. Arguments max_radius_scale : Float between [0, 1]. Value multiplied with largest image dimension to obtain the maximum radius possible of a vertex in the occlusion polygon. probability : Float between [0, 1]. Assigns probability of how often an occlusion to an image is generated. [source] TranslateImage paz . processors . geometric . TranslateImage ( fill_color = None ) Applies a translation of image. The translation is a list of length two indicating the x, y values. Arguments fill_color : List of three integers indicating the color values e.g. [0, 0, 0] [source] ImageToNormalizedDeviceCoordinates paz . processors . image . ImageToNormalizedDeviceCoordinates () Map image value from [0, 255] -> [-1, 1]. [source] NormalizedDeviceCoordinatesToImage paz . processors . image . NormalizedDeviceCoordinatesToImage () Map normalized value from [-1, 1] -> [0, 255]. [source] ReplaceLowerThanThreshold paz . processors . image . ReplaceLowerThanThreshold ( threshold = 1e-08 , replacement = 0.0 ) [source] GetNonZeroValues paz . processors . image . GetNonZeroValues () [source] GetNonZeroArguments paz . processors . image . GetNonZeroArguments ()","title":"Image"},{"location":"processors/image/#castimage","text":"paz . processors . image . CastImage ( dtype ) Cast image to given dtype. Arguments dtype : Str or np.dtype [source]","title":"CastImage"},{"location":"processors/image/#subtractmeanimage","text":"paz . processors . image . SubtractMeanImage ( mean ) Subtract channel-wise mean to image. Arguments mean : List of length 3, containing the channel-wise mean. [source]","title":"SubtractMeanImage"},{"location":"processors/image/#addmeanimage","text":"paz . processors . image . AddMeanImage ( mean ) Adds channel-wise mean to image. Arguments mean : List of length 3, containing the channel-wise mean. [source]","title":"AddMeanImage"},{"location":"processors/image/#normalizeimage","text":"paz . processors . image . NormalizeImage () Normalize image by diving all values by 255.0. [source]","title":"NormalizeImage"},{"location":"processors/image/#denormalizeimage","text":"paz . processors . image . DenormalizeImage () Denormalize image by multiplying all values by 255.0. [source]","title":"DenormalizeImage"},{"location":"processors/image/#loadimage","text":"paz . processors . image . LoadImage ( num_channels = 3 ) Loads image. Arguments num_channels : Integer, valid integers are: 1, 3 and 4. [source]","title":"LoadImage"},{"location":"processors/image/#randomsaturation","text":"paz . processors . image . RandomSaturation ( lower = 0.3 , upper = 1.5 ) Applies random saturation to an image in RGB space. Arguments lower : Float, lower bound for saturation factor. upper : Float, upper bound for saturation factor. [source]","title":"RandomSaturation"},{"location":"processors/image/#randombrightness","text":"paz . processors . image . RandomBrightness ( delta = 32 ) Adjust random brightness to an image in RGB space. Arguments max_delta : Float. [source]","title":"RandomBrightness"},{"location":"processors/image/#randomcontrast","text":"paz . processors . image . RandomContrast ( lower = 0.5 , upper = 1.5 ) Applies random contrast to an image in RGB Arguments lower : Float, indicating the lower bound of the random number to be multiplied with the BGR/RGB image. upper : Float, indicating the upper bound of the random number to be multiplied with the BGR/RGB image. [source]","title":"RandomContrast"},{"location":"processors/image/#randomhue","text":"paz . processors . image . RandomHue ( delta = 18 ) Applies random hue to an image in RGB space. Arguments delta : Int, indicating the range (-delta, delta ) of possible hue values. [source]","title":"RandomHue"},{"location":"processors/image/#resizeimages","text":"paz . processors . image . ResizeImages ( shape ) Resize list of images. Arguments size : List of two ints. [source]","title":"ResizeImages"},{"location":"processors/image/#resizeimages_1","text":"paz . processors . image . ResizeImages ( shape ) Resize list of images. Arguments size : List of two ints. [source]","title":"ResizeImages"},{"location":"processors/image/#randomimageblur","text":"paz . processors . image . RandomImageBlur ( probability = 0.5 ) Randomizes image quality Arguments probability : Float between [0, 1]. Assigns probability of how often a random image blur is applied. [source]","title":"RandomImageBlur"},{"location":"processors/image/#randomgaussianblur","text":"paz . processors . image . RandomGaussianBlur ( kernel_size = ( 5 , 5 ), probability = 0.5 ) Randomizes image quality Arguments probability : Float between [0, 1]. Assigns probability of how often a random image blur is applied. [source]","title":"RandomGaussianBlur"},{"location":"processors/image/#randomflipimageleftright","text":"paz . processors . image . RandomFlipImageLeftRight () Randomly flip the image left or right [source]","title":"RandomFlipImageLeftRight"},{"location":"processors/image/#convertcolorspace","text":"paz . processors . image . ConvertColorSpace ( flag ) Converts image to a different color space. Arguments flag : Flag found in processors indicating transform e.g. pr.BGR2RGB [source]","title":"ConvertColorSpace"},{"location":"processors/image/#showimage","text":"paz . processors . image . ShowImage ( window_name = 'image' , wait = True ) Shows image in a separate window. Arguments window_name : String. Window name. wait : Boolean [source]","title":"ShowImage"},{"location":"processors/image/#imagedataprocessor","text":"paz . processors . image . ImageDataProcessor ( generator ) Wrapper for Keras ImageDataGenerator Arguments generator : An instantiated Keras ImageDataGenerator [source]","title":"ImageDataProcessor"},{"location":"processors/image/#alphablending","text":"paz . processors . image . AlphaBlending () Blends image to background using the image's alpha channel. [source]","title":"AlphaBlending"},{"location":"processors/image/#randomimagecrop","text":"paz . processors . image . RandomImageCrop ( crop_factor = 0.3 , probability = 0.5 ) Crops randomly a rectangle from an image. Arguments crop_factor : Float between [0, 1] . probability : Float between [0, 1] . [source]","title":"RandomImageCrop"},{"location":"processors/image/#randomshapecrop","text":"paz . processors . image . RandomShapeCrop ( shape ) Randomly crops a part of an image of always the same given shape . Arguments shape : List of two ints [height, width]. Dimensions of image to be cropped. [source]","title":"RandomShapeCrop"},{"location":"processors/image/#makerandomplainimage","text":"paz . processors . image . MakeRandomPlainImage ( shape ) Makes random plain image by randomly sampling an RGB color. Arguments shape : List of two ints [height, width]. Dimensions of plain image to be generated. [source]","title":"MakeRandomPlainImage"},{"location":"processors/image/#concatenatealphamask","text":"paz . processors . image . ConcatenateAlphaMask () Concatenates alpha mask to original image. [source]","title":"ConcatenateAlphaMask"},{"location":"processors/image/#blendrandomcroppedbackground","text":"paz . processors . image . BlendRandomCroppedBackground ( background_paths ) Blends image with a randomly cropped background. Arguments background_paths : List of strings. Each element of the list is a full-path to an image used for cropping a background. [source]","title":"BlendRandomCroppedBackground"},{"location":"processors/image/#addocclusion","text":"paz . processors . image . AddOcclusion ( max_radius_scale = 0.5 , probability = 0.5 ) Adds a random occlusion to image by generating random vertices and drawing a polygon. Arguments max_radius_scale : Float between [0, 1]. Value multiplied with largest image dimension to obtain the maximum radius possible of a vertex in the occlusion polygon. probability : Float between [0, 1]. Assigns probability of how often an occlusion to an image is generated. [source]","title":"AddOcclusion"},{"location":"processors/image/#translateimage","text":"paz . processors . geometric . TranslateImage ( fill_color = None ) Applies a translation of image. The translation is a list of length two indicating the x, y values. Arguments fill_color : List of three integers indicating the color values e.g. [0, 0, 0] [source]","title":"TranslateImage"},{"location":"processors/image/#imagetonormalizeddevicecoordinates","text":"paz . processors . image . ImageToNormalizedDeviceCoordinates () Map image value from [0, 255] -> [-1, 1]. [source]","title":"ImageToNormalizedDeviceCoordinates"},{"location":"processors/image/#normalizeddevicecoordinatestoimage","text":"paz . processors . image . NormalizedDeviceCoordinatesToImage () Map normalized value from [-1, 1] -> [0, 255]. [source]","title":"NormalizedDeviceCoordinatesToImage"},{"location":"processors/image/#replacelowerthanthreshold","text":"paz . processors . image . ReplaceLowerThanThreshold ( threshold = 1e-08 , replacement = 0.0 ) [source]","title":"ReplaceLowerThanThreshold"},{"location":"processors/image/#getnonzerovalues","text":"paz . processors . image . GetNonZeroValues () [source]","title":"GetNonZeroValues"},{"location":"processors/image/#getnonzeroarguments","text":"paz . processors . image . GetNonZeroArguments ()","title":"GetNonZeroArguments"},{"location":"processors/keypoints/","text":"Processors for keypoints [source] ChangeKeypointsCoordinateSystem paz . processors . keypoints . ChangeKeypointsCoordinateSystem () Changes keypoints 2D coordinate system using box2D coordinates to locate the new origin at the openCV image origin (top-left). [source] DenormalizeKeypoints paz . processors . keypoints . DenormalizeKeypoints () Transform normalized keypoints coordinates into image-size coordinates. Arguments image_size : List of two floats having height and width of image. [source] NormalizeKeypoints paz . processors . keypoints . NormalizeKeypoints ( image_size ) Transform keypoints in image-size coordinates to normalized coordinates. Arguments image_size : List of two ints indicating ''(height, width)'' [source] PartitionKeypoints paz . processors . keypoints . PartitionKeypoints () Partitions keypoints from shape [num_keypoints, 2] into a list of the form ((2), (2), ....) and length equal to num_of_keypoints. [source] ProjectKeypoints paz . processors . keypoints . ProjectKeypoints ( projector , keypoints ) Projects homogenous keypoints (4D) in the camera coordinates system into image coordinates using a projective transformation. Arguments projector : Instance of ''paz.models.Project''. keypoints : Numpy array of shape ''(num_keypoints, 3)'' [source] RemoveKeypointsDepth paz . processors . keypoints . RemoveKeypointsDepth () Removes Z component from keypoints. [source] TranslateKeypoints paz . processors . keypoints . TranslateKeypoints () Applies a translation to keypoints. The translation is a list of length two indicating the x, y values. [source] DenormalizeKeypoints2D paz . processors . keypoints . DenormalizeKeypoints2D () Transform normalized keypoints coordinates into image-size coordinates. Arguments image_size : List of two floats having height and width of image. [source] NormalizeKeypoints2D paz . processors . keypoints . NormalizeKeypoints2D ( image_size ) Transform keypoints in image-size coordinates to normalized coordinates. Arguments image_size : List of two ints indicating ''(height, width)'' [source] ArgumentsToImageKeypoints2D paz . processors . keypoints . ArgumentsToImageKeypoints2D () Convert array arguments into UV coordinates. Image plane (0,0)--------> (U) | | | v (V) Arguments row_args : Array (num_rows). col_args : Array (num_cols). Returns Array (num_cols, num_rows) representing points2D in UV space. Notes Arguments are row args (V) and col args (U). Image points are in UV coordinates; thus, we concatenate them in that order i.e. [col_args, row_args]","title":"Keypoints"},{"location":"processors/keypoints/#changekeypointscoordinatesystem","text":"paz . processors . keypoints . ChangeKeypointsCoordinateSystem () Changes keypoints 2D coordinate system using box2D coordinates to locate the new origin at the openCV image origin (top-left). [source]","title":"ChangeKeypointsCoordinateSystem"},{"location":"processors/keypoints/#denormalizekeypoints","text":"paz . processors . keypoints . DenormalizeKeypoints () Transform normalized keypoints coordinates into image-size coordinates. Arguments image_size : List of two floats having height and width of image. [source]","title":"DenormalizeKeypoints"},{"location":"processors/keypoints/#normalizekeypoints","text":"paz . processors . keypoints . NormalizeKeypoints ( image_size ) Transform keypoints in image-size coordinates to normalized coordinates. Arguments image_size : List of two ints indicating ''(height, width)'' [source]","title":"NormalizeKeypoints"},{"location":"processors/keypoints/#partitionkeypoints","text":"paz . processors . keypoints . PartitionKeypoints () Partitions keypoints from shape [num_keypoints, 2] into a list of the form ((2), (2), ....) and length equal to num_of_keypoints. [source]","title":"PartitionKeypoints"},{"location":"processors/keypoints/#projectkeypoints","text":"paz . processors . keypoints . ProjectKeypoints ( projector , keypoints ) Projects homogenous keypoints (4D) in the camera coordinates system into image coordinates using a projective transformation. Arguments projector : Instance of ''paz.models.Project''. keypoints : Numpy array of shape ''(num_keypoints, 3)'' [source]","title":"ProjectKeypoints"},{"location":"processors/keypoints/#removekeypointsdepth","text":"paz . processors . keypoints . RemoveKeypointsDepth () Removes Z component from keypoints. [source]","title":"RemoveKeypointsDepth"},{"location":"processors/keypoints/#translatekeypoints","text":"paz . processors . keypoints . TranslateKeypoints () Applies a translation to keypoints. The translation is a list of length two indicating the x, y values. [source]","title":"TranslateKeypoints"},{"location":"processors/keypoints/#denormalizekeypoints2d","text":"paz . processors . keypoints . DenormalizeKeypoints2D () Transform normalized keypoints coordinates into image-size coordinates. Arguments image_size : List of two floats having height and width of image. [source]","title":"DenormalizeKeypoints2D"},{"location":"processors/keypoints/#normalizekeypoints2d","text":"paz . processors . keypoints . NormalizeKeypoints2D ( image_size ) Transform keypoints in image-size coordinates to normalized coordinates. Arguments image_size : List of two ints indicating ''(height, width)'' [source]","title":"NormalizeKeypoints2D"},{"location":"processors/keypoints/#argumentstoimagekeypoints2d","text":"paz . processors . keypoints . ArgumentsToImageKeypoints2D () Convert array arguments into UV coordinates. Image plane (0,0)--------> (U) | | | v (V) Arguments row_args : Array (num_rows). col_args : Array (num_cols). Returns Array (num_cols, num_rows) representing points2D in UV space. Notes Arguments are row args (V) and col args (U). Image points are in UV coordinates; thus, we concatenate them in that order i.e. [col_args, row_args]","title":"ArgumentsToImageKeypoints2D"},{"location":"processors/munkres/","text":"[source] Munkres paz . processors . munkres . Munkres () Provides an implementation of the Munkres algorithm. References https://brc2.com/the-algorithm-workshop/ https://software.clapper.org/munkres/ https://github.com/bmc/munkres","title":"Munkres"},{"location":"processors/munkres/#munkres","text":"paz . processors . munkres . Munkres () Provides an implementation of the Munkres algorithm. References https://brc2.com/the-algorithm-workshop/ https://software.clapper.org/munkres/ https://github.com/bmc/munkres","title":"Munkres"},{"location":"processors/pose/","text":"Processors for pose estimation [source] SolvePNP paz . processors . pose . SolvePNP ( points3D , camera , solver = 0 ) Calculates 6D pose from 3D points and 2D keypoints correspondences. Arguments model_points : Numpy array of shape [num_points, 3] . Model 3D points known in advance. camera : Instance of ''paz.backend.Camera'' containing as properties the camera_intrinsics a Numpy array of shape [3, 3] usually calculated from the openCV calibrateCamera function, and the distortion a Numpy array of shape [5] in which the elements are usually obtained from the openCV calibrateCamera function. solver : Flag specifying solvers. Current solvers are: paz.processors.LEVENBERG_MARQUARDT and paz.processors.UPNP . Returns Instance from Pose6D message. [source] SolveChangingObjectPnPRANSAC paz . processors . pose . SolveChangingObjectPnPRANSAC ( camera_intrinsics , inlier_thresh = 5 , num_iterations = 100 ) Returns rotation (Roc) and translation (Toc) vectors that transform 3D points in object frame to camera frame. O------------O /| /| / | / | O------------O | | | z | | | O _| _|__O | / |___y| / object | / / | / coordinates |/ x |/ O------------O Z | / | Rco, Tco /_____X <------| | | camera Y coordinates Arguments object_points3D : Array (num_points, 3). Points 3D in object reference frame. Represented as (0) in image above. image_points2D : Array (num_points, 2). Points in 2D in camera UV space. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. inlier_threshold : Number of inliers for RANSAC method. num_iterations : Maximum number of iterations. Returns Boolean indicating success, rotation vector in axis-angle form (3) and translation vector (3).","title":"Pose"},{"location":"processors/pose/#solvepnp","text":"paz . processors . pose . SolvePNP ( points3D , camera , solver = 0 ) Calculates 6D pose from 3D points and 2D keypoints correspondences. Arguments model_points : Numpy array of shape [num_points, 3] . Model 3D points known in advance. camera : Instance of ''paz.backend.Camera'' containing as properties the camera_intrinsics a Numpy array of shape [3, 3] usually calculated from the openCV calibrateCamera function, and the distortion a Numpy array of shape [5] in which the elements are usually obtained from the openCV calibrateCamera function. solver : Flag specifying solvers. Current solvers are: paz.processors.LEVENBERG_MARQUARDT and paz.processors.UPNP . Returns Instance from Pose6D message. [source]","title":"SolvePNP"},{"location":"processors/pose/#solvechangingobjectpnpransac","text":"paz . processors . pose . SolveChangingObjectPnPRANSAC ( camera_intrinsics , inlier_thresh = 5 , num_iterations = 100 ) Returns rotation (Roc) and translation (Toc) vectors that transform 3D points in object frame to camera frame. O------------O /| /| / | / | O------------O | | | z | | | O _| _|__O | / |___y| / object | / / | / coordinates |/ x |/ O------------O Z | / | Rco, Tco /_____X <------| | | camera Y coordinates Arguments object_points3D : Array (num_points, 3). Points 3D in object reference frame. Represented as (0) in image above. image_points2D : Array (num_points, 2). Points in 2D in camera UV space. camera_intrinsics : Array of shape (3, 3). Diagonal elements represent focal lenghts and last column the image center translation. inlier_threshold : Number of inliers for RANSAC method. num_iterations : Maximum number of iterations. Returns Boolean indicating success, rotation vector in axis-angle form (3) and translation vector (3).","title":"SolveChangingObjectPnPRANSAC"},{"location":"processors/renderer/","text":"Processors used for rendering [source] Render paz . processors . renderer . Render ( renderer ) Render images and labels. Arguments renderer : Object that renders images and labels using a method ''render_sample()''.","title":"Renderer"},{"location":"processors/renderer/#render","text":"paz . processors . renderer . Render ( renderer ) Render images and labels. Arguments renderer : Object that renders images and labels using a method ''render_sample()''.","title":"Render"},{"location":"processors/standard/","text":"Standard processors [source] ControlMap paz . processors . standard . ControlMap ( processor , intro_indices = [ 0 ], outro_indices = [ 0 ], keep = None ) Controls which inputs are passed ''processor'' and the order of its outputs. Arguments processor : Function e.g. a ''paz.processor'' intro_indices : List of Ints. outro_indices : List of Ints. keep : ''None'' or dictionary. If None control maps operates without explicitly retaining an input. If dict it must contain as keys the input args to be kept and as values where they should be located at the end. [source] ExpandDomain paz . processors . standard . ExpandDomain ( processor ) Extends number of inputs a function can take applying the identity function to all new/extended inputs. e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn. Arguments processor : Function e.g. any procesor in ''paz.processors''. [source] CopyDomain paz . processors . standard . CopyDomain ( intro_indices , outro_indices ) Copies ''intro_indices'' and places it ''outro_indices''. Arguments intro_indices : List of Ints. outro_indices : List of Ints. [source] ExtendInputs paz . processors . standard . ExtendInputs ( processor ) Extends number of inputs a function can take applying the identity function to all new/extended inputs. e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn. Arguments processor : Function e.g. any procesor in ''paz.processors''. [source] SequenceWrapper paz . processors . standard . SequenceWrapper ( inputs_info , labels_info ) Wraps arguments to directly use ''paz.abstract.ProcessingSequence'' or ''paz.abstract.GeneratingSequence''. Arguments inputs_info : Dictionary containing an integer per key representing the argument to grab, and as value a dictionary containing the tensor name as key and the tensor shape of a single sample as value e.g. {0: {'input_image': [300, 300, 3]}, 1: {'depth': [300, 300]}}. The values given here are for the inputs of the model. labels_info : Dictionary containing an integer per key representing the argument to grab, and as value a dictionary containing the tensor name as key and the tensor shape of a single sample as value e.g. {2: {'classes': [10]}}. The values given here are for the labels of the model. [source] Predict paz . processors . standard . Predict ( model , preprocess = None , postprocess = None ) Perform input preprocessing, model prediction and output postprocessing. Arguments model : Class with a ''predict'' method e.g. a Keras model. preprocess : Function applied to given inputs. postprocess : Function applied to outputted predictions from model. [source] ToClassName paz . processors . standard . ToClassName ( labels ) [source] ExpandDims paz . processors . standard . ExpandDims ( axis ) Expand dimension of given array. Arguments axis : Int. [source] BoxClassToOneHotVector paz . processors . standard . BoxClassToOneHotVector ( num_classes ) Transform box data with class index to a one-hot encoded vector. Arguments num_classes : Integer. Total number of classes. [source] Squeeze paz . processors . standard . Squeeze ( axis ) Wrap around numpy squeeze due to common use before model predict. Arguments expand_dims : Int or list of Ints. topic : String. [source] Copy paz . processors . standard . Copy () Copies value passed to function. [source] Lambda paz . processors . standard . Lambda ( function ) Applies a lambda function as a processor transformation. Arguments function : Function. [source] UnpackDictionary paz . processors . standard . UnpackDictionary ( order ) Unpacks dictionary into a tuple. Arguments order : List of strings containing the keys of the dictionary. The order of the list is the order in which the tuple would be ordered. [source] WrapOutput paz . processors . standard . WrapOutput ( keys ) Wraps arguments in dictionary Arguments keys : List of strings representing the keys used to wrap the inputs. The order of the list must correspond to the same order of inputs (''args''). [source] Concatenate paz . processors . standard . Concatenate ( axis ) Concatenates a list of arrays in given ''axis''. Arguments axis : Int. [source] SelectElement paz . processors . standard . SelectElement ( index ) Selects element of input value. Arguments index : Int. argument to select from ''inputs''. [source] StochasticProcessor paz . processors . standard . StochasticProcessor ( probability = 0.5 , name = None ) [source] Stochastic paz . processors . standard . Stochastic ( function , probability = 0.5 , name = None ) [source] UnwrapDictionary paz . processors . standard . UnwrapDictionary ( keys ) Unwraps a dictionry into a list given the key order. [source] Scale paz . processors . standard . Scale ( scales ) Scales an input.","title":"Standard"},{"location":"processors/standard/#controlmap","text":"paz . processors . standard . ControlMap ( processor , intro_indices = [ 0 ], outro_indices = [ 0 ], keep = None ) Controls which inputs are passed ''processor'' and the order of its outputs. Arguments processor : Function e.g. a ''paz.processor'' intro_indices : List of Ints. outro_indices : List of Ints. keep : ''None'' or dictionary. If None control maps operates without explicitly retaining an input. If dict it must contain as keys the input args to be kept and as values where they should be located at the end. [source]","title":"ControlMap"},{"location":"processors/standard/#expanddomain","text":"paz . processors . standard . ExpandDomain ( processor ) Extends number of inputs a function can take applying the identity function to all new/extended inputs. e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn. Arguments processor : Function e.g. any procesor in ''paz.processors''. [source]","title":"ExpandDomain"},{"location":"processors/standard/#copydomain","text":"paz . processors . standard . CopyDomain ( intro_indices , outro_indices ) Copies ''intro_indices'' and places it ''outro_indices''. Arguments intro_indices : List of Ints. outro_indices : List of Ints. [source]","title":"CopyDomain"},{"location":"processors/standard/#extendinputs","text":"paz . processors . standard . ExtendInputs ( processor ) Extends number of inputs a function can take applying the identity function to all new/extended inputs. e.g. For a given function f(x) = y. If g = ExtendInputs(f), we can now have g(x, x1, x2, ..., xn) = y, x1, x2, ..., xn. Arguments processor : Function e.g. any procesor in ''paz.processors''. [source]","title":"ExtendInputs"},{"location":"processors/standard/#sequencewrapper","text":"paz . processors . standard . SequenceWrapper ( inputs_info , labels_info ) Wraps arguments to directly use ''paz.abstract.ProcessingSequence'' or ''paz.abstract.GeneratingSequence''. Arguments inputs_info : Dictionary containing an integer per key representing the argument to grab, and as value a dictionary containing the tensor name as key and the tensor shape of a single sample as value e.g. {0: {'input_image': [300, 300, 3]}, 1: {'depth': [300, 300]}}. The values given here are for the inputs of the model. labels_info : Dictionary containing an integer per key representing the argument to grab, and as value a dictionary containing the tensor name as key and the tensor shape of a single sample as value e.g. {2: {'classes': [10]}}. The values given here are for the labels of the model. [source]","title":"SequenceWrapper"},{"location":"processors/standard/#predict","text":"paz . processors . standard . Predict ( model , preprocess = None , postprocess = None ) Perform input preprocessing, model prediction and output postprocessing. Arguments model : Class with a ''predict'' method e.g. a Keras model. preprocess : Function applied to given inputs. postprocess : Function applied to outputted predictions from model. [source]","title":"Predict"},{"location":"processors/standard/#toclassname","text":"paz . processors . standard . ToClassName ( labels ) [source]","title":"ToClassName"},{"location":"processors/standard/#expanddims","text":"paz . processors . standard . ExpandDims ( axis ) Expand dimension of given array. Arguments axis : Int. [source]","title":"ExpandDims"},{"location":"processors/standard/#boxclasstoonehotvector","text":"paz . processors . standard . BoxClassToOneHotVector ( num_classes ) Transform box data with class index to a one-hot encoded vector. Arguments num_classes : Integer. Total number of classes. [source]","title":"BoxClassToOneHotVector"},{"location":"processors/standard/#squeeze","text":"paz . processors . standard . Squeeze ( axis ) Wrap around numpy squeeze due to common use before model predict. Arguments expand_dims : Int or list of Ints. topic : String. [source]","title":"Squeeze"},{"location":"processors/standard/#copy","text":"paz . processors . standard . Copy () Copies value passed to function. [source]","title":"Copy"},{"location":"processors/standard/#lambda","text":"paz . processors . standard . Lambda ( function ) Applies a lambda function as a processor transformation. Arguments function : Function. [source]","title":"Lambda"},{"location":"processors/standard/#unpackdictionary","text":"paz . processors . standard . UnpackDictionary ( order ) Unpacks dictionary into a tuple. Arguments order : List of strings containing the keys of the dictionary. The order of the list is the order in which the tuple would be ordered. [source]","title":"UnpackDictionary"},{"location":"processors/standard/#wrapoutput","text":"paz . processors . standard . WrapOutput ( keys ) Wraps arguments in dictionary Arguments keys : List of strings representing the keys used to wrap the inputs. The order of the list must correspond to the same order of inputs (''args''). [source]","title":"WrapOutput"},{"location":"processors/standard/#concatenate","text":"paz . processors . standard . Concatenate ( axis ) Concatenates a list of arrays in given ''axis''. Arguments axis : Int. [source]","title":"Concatenate"},{"location":"processors/standard/#selectelement","text":"paz . processors . standard . SelectElement ( index ) Selects element of input value. Arguments index : Int. argument to select from ''inputs''. [source]","title":"SelectElement"},{"location":"processors/standard/#stochasticprocessor","text":"paz . processors . standard . StochasticProcessor ( probability = 0.5 , name = None ) [source]","title":"StochasticProcessor"},{"location":"processors/standard/#stochastic","text":"paz . processors . standard . Stochastic ( function , probability = 0.5 , name = None ) [source]","title":"Stochastic"},{"location":"processors/standard/#unwrapdictionary","text":"paz . processors . standard . UnwrapDictionary ( keys ) Unwraps a dictionry into a list given the key order. [source]","title":"UnwrapDictionary"},{"location":"processors/standard/#scale","text":"paz . processors . standard . Scale ( scales ) Scales an input.","title":"Scale"}]}